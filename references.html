<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="icon" type="image/png" href="./assets/images/favicon.ico"/>
    <title>References</title>
    <link rel="stylesheet" href="./assets/styles/styles.css">
</head>
<body>
    <div class="mmca-page">
        <div class="intro">
            <!-- <div class="rep-img">
                <img class="img" src="./assets/images/harvard_lit.png" alt="LIT Labs">
                <img class="img" src="./assets/images/harvard.png" alt="harvard">
            </div> -->
            <div class="intro-txt">
                <a class="mmca-nav-txt" href="./index.html"><h1 class="title">MMCA&nbsp;</h1></a>
                <h3 class="subtitle">Multimodal Collaboration Analytics (MMCA) - A Literature Review</h3>
            </div>
        </div>
        <div class="nav-wrapper">
            <a class="link" href="index.html">Home</a>
            <a class="link" href="purpose.html">Purpose</a>
            <a class="link" href="framework.html">Our Framework</a>
            <a class="link" href="references.html">References</a>
            <a class="link" href="contribute.html">Contribute</a>
        </div>
        <div class="purpose purpose-container reference-container">
            <ul>
            <li>Ainley, M., Corrigan, M., &amp; Richardson, N. (2005). Students, tasks and emotions: Identifying the contribution of emotions to students’ reading of popular culture and popular science texts. <i>Learning and Instruction</i>, <i>15</i>(5), 433–447.</li>
            <li>Andrist, S., Ruis, A. R., &amp; Shaffer, D. W. (2018). A network analytic approach to gaze coordination during a collaborative task. <i>Computers in Human Behavior</i>, <i>89</i>, 339–348. <a href="https://doi.org/10.1016/j.chb.2018.07.017">https://doi.org/10.1016/j.chb.2018.07.017</a></li>
            <li>Bachour, K., Kaplan, F., &amp; Dillenbourg, P. (2010). An Interactive Table for Supporting Participation Balance in Face-to-Face Collaborative Learning. <i>IEEE Transactions on Learning Technologies</i>, <i>3</i>(3), 203–213. <a href="https://doi.org/10.1109/TLT.2010.18">https://doi.org/10.1109/TLT.2010.18</a></li>
            <li>Barry, B., &amp; Stewart, G. (1997a). Composition, process, and performance in self-managed groups: The role of personality. <i>Journal of Applied Psychology</i>, <i>82</i>(1), 62.</li>
            <li>Barry, B., &amp; Stewart, G. L. (1997b). Composition, process and performance in self-managed groups: The role of personality. <i>Journal of Applied Psychology</i>, <i>82</i>(1), 62.</li>
            <li>Bassiou, N., Tsiartas, A., Smith, J., Bratt, H., Richey, C., Shriberg, E., D’Angelo, C., &amp; Alozie, N. (2016). Privacy-Preserving Speech Analytics for Automatic Assessment of Student Collaboration. <i>INTERSPEECH</i>, 888–892.</li>
            <li>Behoora, I., &amp; Tucker, C. S. (2015). Machine learning classification of design team members’ body language patterns for real time emotional state detection. <i>Design Studies</i>, <i>39</i>, 100–127. <a href="https://doi.org/10.1016/j.destud.2015.04.003">https://doi.org/10.1016/j.destud.2015.04.003</a></li>
            <li>Beyan, C., Carissimi, N., Capozzi, F., Vascon, S., Bustreo, M., Pierro, A., Becchio, C., &amp; Murino, V. (2016). Detecting emergent leader in a meeting environment using nonverbal visual features only. <i>Proceedings of the 18th ACM International Conference on Multimodal Interaction - ICMI 2016</i>, 317–324. <a href="https://doi.org/10.1145/2993148.2993175">https://doi.org/10.1145/2993148.2993175</a></li>
            <li>Beyan, C., Katsageorgiou, V.-M., &amp; Murino, V. (2017). Moving as a Leader: Detecting Emergent Leadership in Small Groups using Body Pose. <i>Proceedings of the 2017 ACM on Multimedia Conference - MM ’17</i>, 1425–1433. <a href="https://doi.org/10.1145/3123266.3123404">https://doi.org/10.1145/3123266.3123404</a></li>
            <li>Bhattacharya, I., Foley, M., Zhang, N., Zhang, T., Ku, C., Mine, C., Ji, H., Riedl, C., Welles, B. F., &amp; Radke, R. J. (2018). A multimodal-sensor-enabled room for unobtrusive group meeting analysis. <i>Proceedings of the 20th ACM International Conference on Multimodal Interaction</i>, 347–355.</li>
            <li>Chikersal, P., Tomprou, M., Kim, Y. J., Woolley, A. W., &amp; Dabbish, L. (2017). Deep Structures of Collaboration: Physiological Correlates of Collective Intelligence and Group Satisfaction. <i>Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing</i>, 873–888. <a href="https://doi.org/10.1145/2998181.2998250">https://doi.org/10.1145/2998181.2998250</a></li>
            <li>Chng, E., Seyam, M. R., Yao, W., &amp; Schneider, B. (2020). Using Motion Sensors to Understand Collaborative Interactions in Digital Fabrication Labs. In I. I. Bittencourt, M. Cukurova, K. Muldner, R. Luckin, &amp; E. Millán (Eds.), <i>Artificial Intelligence in Education</i> (Vol. 12163, pp. 118–128). Springer International Publishing. <a href="https://doi.org/10.1007/978-3-030-52237-7_10">https://doi.org/10.1007/978-3-030-52237-7_10</a></li>
            <li>Cukurova, M., Luckin, R., Millán, E., &amp; Mavrikis, M. (2018). The NISPI framework: Analysing collaborative problem-solving from students’ physical interactions. <i>Computers &amp; Education</i>, <i>116</i>, 93–109.</li>
            <li>Dale, R., Bryant, G. A., Manson, J. H., &amp; Gervais, M. M. (n.d.). Body synchrony in triadic interaction. <i>Royal Society Open Science</i>, <i>7</i>(9), 200095. <a href="https://doi.org/10.1098/rsos.200095">https://doi.org/10.1098/rsos.200095</a></li>
            <li>Damon, W., &amp; Phelps, E. (1989). Critical distinctions among three approaches to peer education. <i>International Journal of Educational Research</i>, <i>13</i>(1), 9–19.</li>
            <li>D’Angelo, S., &amp; Begel, A. (2017). Improving communication between pair programmers using shared gaze awareness. <i>Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems</i>, 6245–6290.</li>
            <li>D’Angelo, S., &amp; Gergle, D. (2018). An eye for design: Gaze visualizations for remote collaborative work. <i>Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems</i>, 1–12.</li>
            <li>D’Angelo, S., &amp; Gergle, D. (2016). Gazed and confused: Understanding and designing shared gaze for remote collaboration. <i>Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems</i>, 2492–2496.</li>
            <li>Dich, Y., Reilly, J., &amp; Schneider, B. (2018). Using physiological synchrony as an indicator of collaboration quality, task performance and learning. <i>International Conference on Artificial Intelligence in Education</i>, 98–110.</li>
            <li>Dikker, S., Wan, L., Davidesco, I., Kaggen, L., Oostrik, M., McClintock, J., Rowland, J., Michalareas, G., Van Bavel, J. J., Ding, M., &amp; Poeppel, D. (2017). Brain-to-Brain Synchrony Tracks Real-World Dynamic Group Interactions in the Classroom. <i>Current Biology</i>, <i>27</i>(9), 1375–1380. <a href="https://doi.org/10.1016/j.cub.2017.04.002">https://doi.org/10.1016/j.cub.2017.04.002</a></li>
            <li>Dindar, M., Järvelä, S., &amp; Haataja, E. (2020). What does physiological synchrony reveal about metacognitive experiences and group performance? <i>British Journal of Educational Technology</i>, <i>51</i>(5), 1577–1596. <a href="https://doi.org/10.1111/bjet.12981">https://doi.org/10.1111/bjet.12981</a></li>
            <li>Efklides, A., Papadaki, M., Papantoniou, G., &amp; Kiosseoglou, G. (1998). Individual differences in feelings of difficulty: The case of school mathematics. <i>European Journal of Psychology of Education</i>, <i>13</i>(2), 207–226.</li>
            <li>Eloy, L., Stewart, A. E. B., Amon, M. J., Reinhardt, C., Michaels, A., Chen, S., Shute, V., Duran, N. D., &amp; D’Mello, S. K. (2019). Modeling Team-level Multimodal Dynamics during Multiparty Collaboration. <i>2019 International Conference on Multimodal Interaction</i>, 244–258. <a href="https://doi.org/10.1145/3340555.3353748">https://doi.org/10.1145/3340555.3353748</a></li>
            <li>Erkens, G., Jaspers, J., Prangsma, M., &amp; Kanselaar, G. (n.d.). Computers in Human Behavior. <i>Computers in Human Behavior</i>, <i>21</i>(3), 463–486.</li>
            <li>Evans, A. C., Wobbrock, J. O., &amp; Davis, K. (2016). Modeling Collaboration Patterns on an Interactive Tabletop in a Classroom Setting. <i>Proceedings of the 19th ACM Conference on Computer-Supported Cooperative Work &amp; Social Computing - CSCW ’16</i>, 858–869. <a href="https://doi.org/10.1145/2818048.2819972">https://doi.org/10.1145/2818048.2819972</a></li>
            <li>Fang, S., Achard, C., &amp; Dubuisson, S. (2016). Personality classification and behaviour interpretation: An approach based on feature categories. <i>Proceedings of the 18th ACM International Conference on Multimodal Interaction - ICMI 2016</i>, 225–232. <a href="https://doi.org/10.1145/2993148.2993201">https://doi.org/10.1145/2993148.2993201</a></li>
            <li>Gergle, D., &amp; Clark, A. T. (2011). See what I’m saying? Using dyadic mobile eye tracking to study collaborative reference. <i>Proceedings of the ACM 2011 Conference on Computer Supported Cooperative Work</i>, 435–444.</li>
            <li>Grafsgaard, J. F., Wiggins, J. B., Vail, A. K., Boyer, K. E., Wiebe, E. N., &amp; Lester, J. C. (2014a). The additive value of multimodal features for predicting engagement, frustration, and learning during tutoring. <i>Proceedings of the 16th International Conference on Multimodal Interaction</i>, 42–49.</li>
            <li>Grafsgaard, J. F., Wiggins, J. B., Vail, A. K., Boyer, K. E., Wiebe, E. N., &amp; Lester, J. C. (2014b). The Additive Value of Multimodal Features for Predicting Engagement, Frustration, and Learning during Tutoring. <i>Proceedings of the 16th International Conference on Multimodal Interaction</i>, 42–49.</li>
            <li>Hadwin, A. F., &amp; Webster, E. A. (2013). Calibration in goal setting: Examining the nature of judgments of confidence. <i>Learning and Instruction</i>, <i>24</i>, 37–47.</li>
            <li>Higuch, K., Yonetani, R., &amp; Sato, Y. (2016). Can Eye Help You?: Effects of Visualizing Eye Fixations on Remote Collaboration Scenarios for Physical Tasks. <i>Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems</i>, 5180–5190. <a href="https://doi.org/10.1145/2858036.2858438">https://doi.org/10.1145/2858036.2858438</a></li>
            <li>Hung, H., Jayagopi, D. B., Ba, S., Odobez, J.-M., &amp; Gatica-Perez, D. (2008). Investigating automatic dominance estimation in groups from visual attention and speaking activity. <i>Proceedings of the 10th International Conference on Multimodal Interfaces</i>, 233–236. <a href="https://doi.org/10.1145/1452392.1452441">https://doi.org/10.1145/1452392.1452441</a></li>
            <li>Järvelä, S., Kivikangas, J. M., Kätsyri, J., &amp; Ravaja, N. (2014). Physiological Linkage of Dyadic Gaming Experience. <i>Simulation &amp; Gaming</i>, <i>45</i>(1), 24–40. <a href="https://doi.org/10.1177/1046878113513080">https://doi.org/10.1177/1046878113513080</a></li>
            <li>Jayagopi, D., Sanchez-Cortes, D., Otsuka, K., Yamato, J., &amp; Gatica-Perez, D. (2012). Linking speaking and looking behavior patterns with group composition, perception, and performance. <i>Proceedings of the 14th ACM International Conference on Multimodal Interaction</i>, 433–440.</li>
            <li>Jermann, P., &amp; Nüssli, M.-A. (2012). Effects of sharing text selections on gaze cross-recurrence and interaction quality in a pair programming task. <i>Proceedings of the ACM 2012 Conference on Computer Supported Cooperative Work</i>, 1125–1134. <a href="https://doi.org/10.1145/2145204.2145371">https://doi.org/10.1145/2145204.2145371</a></li>
            <li>Joann, K., &amp; Wall, V. D. (1989). SYMLOG: Theory and method for measuring group and organizational communication. <i>Management Communication Quarterly</i>, <i>2.4</i>, 544–567.</li>
            <li>Kantharaju, R. B., Ringeval, F., &amp; Besacier, L. (2018). Automatic Recognition of Affective Laughter in Spontaneous Dyadic Interactions from Audiovisual Signals. <i>Proceedings of the 2018 on International Conference on Multimodal Interaction - ICMI ’18</i>, 220–228. <a href="https://doi.org/10.1145/3242969.3243012">https://doi.org/10.1145/3242969.3243012</a></li>
            <li>Kelly, J. R., &amp; Barsade, S. (n.d.). Mood and emotions in small groups and work teams. <i>Organizational Behavior and Human Decision Processes</i>, <i>86</i>(1), 99–130.</li>
            <li>Kelly, J. R., &amp; Barsade, S. (2001). Mood and emotions in small groups and work teams. <i>Organizational Behavior and Human Decision Processes</i>, <i>86</i>(1), 99–130.</li>
            <li>KÜTT, G. H., TANPRASERT, T., RODOLITZ, J., MOYZA, B., SO, S., KENDEROVA, G., &amp; PAPOUTSAKI, A. (2020). <i>Effects of Shared Gaze on Audio-Versus Text-Based Remote Collaborations</i>. <i>4</i>, 25. <a href="https://doi.org/10.1145/ 3415207">https://doi.org/10.1145/ 3415207</a></li>
            <li>Le Dantec, C. A., &amp; Do, E. Y.-L. (2009). The mechanisms of value transfer in design meetings. <i>Design Studies</i>, <i>30</i>(2), 119–137.</li>
            <li>Le Dantec, C., &amp; Yi-Luen Do, E. (2009). The mechanisms of value transfer in design meetings. <i>Design Studies</i>, <i>30</i>(2), 119–137.</li>
            <li>Lepri, B., Subramanian, R., Kalimeri, K., Staiano, J., Pianesi, F., &amp; Sebe, N. (2010). Employing social gaze and speaking activity for automatic determination of the extraversion trait. <i>International Conference on Multimodal Interfaces and the Workshop on Machine Learning for Multimodal Interaction</i>, 1–8.</li>
            <li>Li, W., Nüssli, M.-A., &amp; Jermann, P. (2010a). Gaze quality assisted automatic recognition of social contexts in collaborative Tetris. <i>International Conference on Multimodal Interfaces and the Workshop on Machine Learning for Multimodal Interaction</i>, 1–8.</li>
            <li>Li, W., Nüssli, M.-A., &amp; Jermann, P. (2010b). Gaze quality assisted automatic recognition of social contexts in collaborative Tetris. <i>International Conference on Multimodal Interfaces and the Workshop on Machine Learning for Multimodal Interaction</i>, 1–8.</li>
            <li>Lin, Y.-S., &amp; Lee, C.-C. (2018). Using Interlocutor-Modulated Attention BLSTM to Predict Personality Traits in Small Group Interaction. <i>Proceedings of the 2018 on International Conference on Multimodal Interaction - ICMI ’18</i>, 163–169. <a href="https://doi.org/10.1145/3242969.3243001">https://doi.org/10.1145/3242969.3243001</a></li>
            <li>Liu, Y., Wang, T., Wang, K., &amp; Zhang, Y. (2020). Predicting Collaborative Learning Quality through Physiological Synchrony Recorded by Wearable Biosensors. <i>BioRxiv</i>, 2020.06.01.127449. <a href="https://doi.org/10.1101/2020.06.01.127449">https://doi.org/10.1101/2020.06.01.127449</a></li>
            <li>Lord, R., Foti, R., &amp; Vader, C. D. (1984). A test of leadership categorization theory: Internal structure, information processing, and leadership perceptions. <i>Organizational Behavior and Human Performance</i>, <i>34</i>(3), 343–378.</li>
            <li>Lubold, N., &amp; Pon-Barry, H. (2014). Acoustic-Prosodic Entrainment and Rapport in Collaborative Learning Dialogues. <i>Proceedings of the 2014 ACM Workshop on Multimodal Learning Analytics Workshop and Grand Challenge - MLA ’14</i>, 5–12. <a href="https://doi.org/10.1145/2666633.2666635">https://doi.org/10.1145/2666633.2666635</a></li>
            <li>Luz, S. (2013). Automatic identification of experts and performance prediction in the multimodal math data corpus through analysis of speech interaction. <i>Proceedings of the 15th ACM on International Conference on Multimodal Interaction - ICMI ’13</i>, 575–582. <a href="https://doi.org/10.1145/2522848.2533788">https://doi.org/10.1145/2522848.2533788</a></li>
            <li>Malmberg, J., Haataja, E., Seppänen, T., &amp; Järvelä, S. (2019). Are we together or not? The temporal interplay of monitoring, physiological arousal and physiological synchrony during a collaborative exam. <i>International Journal of Computer-Supported Collaborative Learning</i>, <i>14</i>(4), 467–490. <a href="https://doi.org/10.1007/s11412-019-09311-4">https://doi.org/10.1007/s11412-019-09311-4</a></li>
            <li>Malmberg, J., Järvelä, S., Holappa, J., Haataja, E., Huang, X., &amp; Siipo, A. (2019). Going beyond what is visible: What multichannel data can reveal about interaction in the context of collaborative learning? <i>Computers in Human Behavior</i>, <i>96</i>, 235–245.</li>
            <li>Manson, J. H., Gregory, B. A., Gervais, M. M., &amp; Kline, M. A. (2013). Convergence of speech rate in conversation predicts cooperation. <i>Evolution and Human Behavior</i>, <i>34</i>(6), 419–426.</li>
            <li>Martinez, R., Wallace, J. R., Kay, J., &amp; Yacef, K. (2011). Modelling and Identifying Collaborative Situations in a Collocated Multi-display Groupware Setting. In G. Biswas, S. Bull, J. Kay, &amp; A. Mitrovic (Eds.), <i>Artificial Intelligence in Education</i> (Vol. 6738, pp. 196–204). Springer Berlin Heidelberg. <a href="https://doi.org/10.1007/978-3-642-21869-9_27">https://doi.org/10.1007/978-3-642-21869-9_27</a></li>
            <li>Martinez, R., Yacef, K., &amp; Kay, J. (2011). <i>Analysing frequent sequential patterns of collaborative learning activity around an interactive tabletop</i>. 10.</li>
            <li>Martinez-Maldonado, R., Dimitriadis, Y., Martinez-Monés, A., Kay, J., &amp; Yacef, K. (2013). Capturing and analyzing verbal and physical collaborative learning interactions at an enriched interactive tabletop. <i>International Journal of Computer-Supported Collaborative Learning</i>, <i>8</i>(4), 455–485. <a href="https://doi.org/10.1007/s11412-013-9184-1">https://doi.org/10.1007/s11412-013-9184-1</a></li>
            <li>Martinez-Maldonado, R., Kay, J., &amp; Yacef, K. (2013). An Automatic Approach for Mining Patterns of Collaboration around an Interactive Tabletop. In H. C. Lane, K. Yacef, J. Mostow, &amp; P. Pavlik (Eds.), <i>Artificial Intelligence in Education</i> (Vol. 7926, pp. 101–110). Springer Berlin Heidelberg. <a href="https://doi.org/10.1007/978-3-642-39112-5_11">https://doi.org/10.1007/978-3-642-39112-5_11</a></li>
            <li>McDuff, D., Thomas, P., Czerwinski, M., &amp; Craswell, N. (2017). Multimodal analysis of vocal collaborative search: A public corpus and results. <i>Proceedings of the 19th ACM International Conference on Multimodal Interaction</i>, 456–463.</li>
            <li>Miura, G., &amp; Okada, S. (2019). Task-independent Multimodal Prediction of Group Performance Based on Product Dimensions. <i>2019 International Conference on Multimodal Interaction</i>, 264–273.</li>
            <li>Mønster, D., Håkonsson, D. D., Eskildsen, J. K., &amp; Wallot, S. (2016). Physiological evidence of interpersonal dynamics in a cooperative production task. <i>Physiology &amp; Behavior</i>, <i>156</i>, 24–34. <a href="https://doi.org/10.1016/j.physbeh.2016.01.004">https://doi.org/10.1016/j.physbeh.2016.01.004</a></li>
            <li>Montague, E., Xu, J., &amp; Chiou, E. (2014). Shared Experiences of Technology and Trust: An Experimental Study of Physiological Compliance Between Active and Passive Users in Technology-Mediated Collaborative Encounters. <i>IEEE Transactions on Human-Machine Systems</i>, <i>44</i>(5), 614–624. <a href="https://doi.org/10.1109/THMS.2014.2325859">https://doi.org/10.1109/THMS.2014.2325859</a></li>
            <li>Muller, P. M., &amp; Bulling, A. (2019). Emergent Leadership Detection Across Datasets. <i>2019 International Conference on Multimodal Interaction</i>, 274–278.</li>
            <li>Murray, G., &amp; Oertel, C. (2018). Predicting Group Performance in Task-Based Interaction. <i>Proceedings of the 20th ACM International Conference on Multimodal Interaction</i>, 14–20. <a href="https://doi.org/10.1145/3242969.3243027">https://doi.org/10.1145/3242969.3243027</a></li>
            <li>Nakano, Y. I., Nihonyanagi, S., Takase, Y., Hayashi, Y., &amp; Okada, S. (2015). Predicting participation styles using co-occurrence patterns of nonverbal behaviors in collaborative learning. <i>Proceedings of the 2015 ACM on International Conference on Multimodal Interaction</i>, 91–98.</li>
            <li>Ochoa, X., Chiluiza, K., Méndez, G., Luzardo, G., Guamán, B., &amp; Castells, J. (2013). Expertise estimation based on simple multimodal features. <i>Proceedings of the 15th ACM on International Conference on Multimodal Interaction - ICMI ’13</i>, 583–590. <a href="https://doi.org/10.1145/2522848.2533789">https://doi.org/10.1145/2522848.2533789</a></li>
            <li>Ogan, A., Finkelstein, S., Walker, E., Carlson, R., &amp; Cassell, J. (2012a). Rudeness and rapport: Insults and learning gains in peer tutoring. <i>International Conference on Intelligent Tutoring Systems</i>, 11–21.</li>
            <li>Ogan, A., Finkelstein, S., Walker, E., Carlson, R., &amp; Cassell, J. (2012b). Rudeness and rapport: Insults and learning gains in peer tutoring. <i>International Conference on Intelligent Tutoring Systems</i>, 11–21.</li>
            <li>Okada, S., Aran, O., &amp; Gatica-Perez, D. (2015). Personality Trait Classification via Co-Occurrent Multiparty Multimodal Event Discovery. <i>Proceedings of the 2015 ACM on International Conference on Multimodal Interaction - ICMI ’15</i>, 15–22. <a href="https://doi.org/10.1145/2818346.2820757">https://doi.org/10.1145/2818346.2820757</a></li>
            <li>Okada, S., Ohtake, Y., Nakano, Y. I., Hayashi, Y., Huang, H.-H., Takase, Y., &amp; Nitta, K. (2016). Estimating communication skills using dialogue acts and nonverbal features in multiple discussion datasets. <i>Proceedings of the 18th ACM International Conference on Multimodal Interaction</i>, 169–176.</li>
            <li>Olsen, J., Sharma, K., Aleven, V., &amp; Rummel, N. (2018). <i>Combining gaze, dialogue, and action from a collaborative intelligent tutoring system to inform student learning processes</i>. ICLS 2018 Proceedings.</li>
            <li>Paas, F. G. (1992). Training strategies for attaining transfer of problem-solving skill in statistics: A cognitive-load approach. <i>Journal of Education Psychology</i>, <i>84</i>(4), 429.</li>
            <li>Pijeira-Díaz, H. J., Drachsler, H., Järvelä, S., &amp; Kirschner, P. A. (2019). Sympathetic arousal commonalities and arousal contagion during collaborative learning: How attuned are triad members? <i>Computers in Human Behavior</i>, <i>92</i>, 188–197. <a href="https://doi.org/10.1016/j.chb.2018.11.008">https://doi.org/10.1016/j.chb.2018.11.008</a></li>
            <li>Pijeira-Díaz, H. J., Drachsler, H., Järvelä, S., &amp; Kirschner, P. A. (2016). Investigating collaborative learning success with physiological coupling indices based on electrodermal activity. <i>Proceedings of the Sixth International Conference on Learning Analytics &amp; Knowledge</i>, 64–73.</li>
            <li>Pintrich, P. (1991). <i>A manual for the use of the Motivated Strategies for Learning Questionnaire (MSLQ)</i>.</li>
            <li>Ponce-López, V., Escalera, S., &amp; Baró, X. (2013). Multi-modal social signal analysis for predicting agreement in conversation settings. <i>Proceedings of the 15th ACM on International Conference on Multimodal Interaction</i>, 495–502. <a href="https://doi.org/10.1145/2522848.2532594">https://doi.org/10.1145/2522848.2532594</a></li>
            <li>Reilly, J. M., Ravenell, M., &amp; Schneider, B. (2018). Exploring Collaboration Using Motion Sensors and Multi-Modal Learning Analytics. <i>International Educational Data Mining Society</i>.</li>
            <li>Reilly, J. M., &amp; Schneider, B. (2019). <i>Predicting the Quality of Collaborative Problem Solving Through Linguistic Analysis of Discourse</i>. 9.</li>
            <li>Riquelme, F., Munoz, R., Mac Lean, R., Villarroel, R., Barcelos, T. S., &amp; de Albuquerque, V. H. C. (2019). Using multimodal learning analytics to study collaboration on discussion groups: A social network approach. <i>Universal Access in the Information Society</i>, <i>18</i>(3), 633–643. <a href="https://doi.org/10.1007/s10209-019-00683-w">https://doi.org/10.1007/s10209-019-00683-w</a></li>
            <li>Samrose, S., Zhao, R., White, J., Li, V., Nova, L., Lu, Y., Ali, M. R., &amp; Hoque, M. E. (2018). Coco: Collaboration coach for understanding team dynamics during video conferencing. <i>Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies</i>, <i>1</i>(4), 1–24.</li>
            <li>Sanchez-Cortes, D., Aran, O., Jayagopi, D. B., Mast, M. S., &amp; Gatica-Perez, D. (2013a). Emergent leaders through looking and speaking: From audio-visual data to multimodal recognition. <i>Journal on Multimodal User Interfaces</i>, <i>1–2</i>(7), 39–53.</li>
            <li>Sanchez-Cortes, D., Aran, O., Jayagopi, D. B., Mast, M. S., &amp; Gatica-Perez, D. (2013b). Emergent leaders through looking and speaking: From audio-visual data to multimodal recognition. <i>Journal of Multimodal User Interfaces</i>, <i>7.1</i>, 39–53.</li>
            <li>Scherer, S., Weibel, N., Morency, L.-P., &amp; Oviatt, S. (2012). Multimodal prediction of expertise and leadership in learning groups. <i>Proceedings of the 1st International Workshop on Multimodal Learning Analytics</i>, 1–8.</li>
            <li>Schlösser, C., Schlieker-Steens, P., Kienle, A., &amp; Harrer, A. (2015). Using Real-Time Gaze Based Awareness Methods to Enhance Collaboration. In N. Baloian, Y. Zorian, P. Taslakian, &amp; S. Shoukouryan (Eds.), <i>Collaboration and Technology</i> (pp. 19–27). Springer International Publishing. <a href="https://doi.org/10.1007/978-3-319-22747-4_2">https://doi.org/10.1007/978-3-319-22747-4_2</a></li>
            <li>Schneider, B. (2019). Unpacking collaborative learning processes during hands-on activities using mobile eye-trackers. <i>International Conference on Computer Supported Collaborative Learning</i>.</li>
            <li>Schneider, B., &amp; Blikstein, P. (2015). <i>Unraveling Students’ Interaction Around a Tangible Interface Using Multimodal Learning Analytics</i>. <i>7</i>(3), 28.</li>
            <li>Schneider, B., Dich, Y., &amp; Radu, I. (2020). Unpacking the Relationship between Existing and New Measures of Physiological Synchrony and Collaborative Learning: A Mixed Methods Study. <i>International Journal of Computer-Supported Collaborative Learning</i>, <i>15</i>(1), 89–113. <a href="https://doi.org/10.1007/s11412-020-09318-2">https://doi.org/10.1007/s11412-020-09318-2</a></li>
            <li>Schneider, B., &amp; Pea, R. (2013a). Real-time mutual gaze perception enhances collaborative learning and collaboration quality. <i>International Journal of Computer-Supported Collaborative Learning</i>, <i>8</i>(4), 375–397.</li>
            <li>Schneider, B., &amp; Pea, R. (2014a). Toward collaboration sensing. <i>International Journal of Computer-Supported Collaborative Learning</i>, <i>9</i>(4), 371–395. <a href="https://doi.org/10.1007/s11412-014-9202-y">https://doi.org/10.1007/s11412-014-9202-y</a></li>
            <li>Schneider, B., &amp; Pea, R. (2015). Does Seeing One Another’s Gaze Affect Group Dialogue? A Computational Approach. <i>Journal of Learning Analytics</i>, <i>2</i>(2), 107–133. <a href="https://doi.org/10.18608/jla.2015.22.9">https://doi.org/10.18608/jla.2015.22.9</a></li>
            <li>Schneider, B., &amp; Pea, R. (2014b). <i>The Effect of Mutual Gaze Perception on Students’ Verbal Coordination</i>. 7.</li>
            <li>Schneider, B., &amp; Pea, R. (2013b). Using Eye-Tracking Technology to Support Visual Coordination in Collaborative Problem-Solving Groups. <i>CSCL 2013 Proceedings</i>, <i>1</i>, 8.</li>
            <li>Schneider, B., Sharma, K., Cuendet, S., Zufferey, G., Dillenbourg, P., &amp; Pea, R. (2016a). Detecting Collaborative Dynamics Using Mobile Eye-Trackers. <i>International Society of the Learning Sciences</i>, 8.</li>
            <li>Schneider, B., Sharma, K., Cuendet, S., Zufferey, G., Dillenbourg, P., &amp; Pea, R. (2016b). Using Mobile Eye-Trackers to Unpack the Perceptual Benefits of a Tangible User Interface for Collaborative Learning. <i>ACM Transactions on Computer-Human Interaction</i>, <i>23</i>(6), 39:1-39:23. <a href="https://doi.org/10.1145/3012009">https://doi.org/10.1145/3012009</a></li>
            <li>Schneider, B., Sharma, K., Cuendet, S., Zufferey, G., Dillenbourg, P., &amp; Pea, R. (2018). Leveraging mobile eye-trackers to capture joint visual attention in co-located collaborative learning groups. <i>International Journal of Computer-Supported Collaborative Learning</i>, <i>13</i>(3), 241–261.</li>
            <li>Schneider, B., Sharma, K., Cuendet, S., Zufferey, G., Dillenbourg, P., &amp; Pea, R. D. (2015). 3D tangibles facilitate joint visual attention in dyads. <i>International Society of the Learning Sciences, Inc.[ISLS]</i>.</li>
            <li>Sharma, K., Caballero, D., Verma, H., Jermann, P., &amp; Dillenbourg, P. (2015). <i>Looking AT versus looking THROUGH: A dual eye-tracking study in MOOC context</i>. International Society of the Learning Sciences, Inc.[ISLS].</li>
            <li>Sharma, K., Jermann, P., Nüssli, M.-A., &amp; Dillenbourg, P. (2013). <i>Understanding Collaborative Program Comprehension: Interlacing Gaze and Dialogues</i>. <a href="https://repository.isls.org//handle/1/1944">https://repository.isls.org//handle/1/1944</a></li>
            <li>Sharma, K., Leftheriotis, I., Noor, J., &amp; Giannakos, M. (2017). <i>Dual Gaze as a Proxy for Collaboration in Informal Learning</i>. <a href="https://repository.isls.org//handle/1/230">https://repository.isls.org//handle/1/230</a></li>
            <li>Sharma, K., &amp; Olsen, J. (2019). An Alternate Statistical Lens to Look at Collaboration Data: Extreme Value Theory. <i>International Conference on Computer Supported Collaborative Learning</i>, 400–407.</li>
            <li>Spikol, D., Ruffaldi, E., &amp; Cukurova, M. (2017). <i>Using Multimodal Learning Analytics to Identify Aspects of Collaboration in Project-Based Learning</i>. 8.</li>
            <li>Spikol, D., Ruffaldi, E., Dabisias, G., &amp; Cukurova, M. (2018). Supervised machine learning in multimodal learning analytics for estimating success in project‐based learning. <i>Journal of Computer Assisted Learning</i>, 366–377.</li>
            <li>Spikol, D., Ruffaldi, E., Landolfi, L., &amp; Cukurova, M. (2017). Estimation of success in collaborative learning based on multimodal learning analytics features. <i>2017 IEEE 17th International Conference on Advanced Learning Technologies (ICALT)</i>, 269–273.</li>
            <li>Sriramulu, A., Lin, J., &amp; Oviatt, S. (2019). Dynamic Adaptive Gesturing Predicts Domain Expertise in Mathematics. <i>2019 International Conference on Multimodal Interaction</i>, 105–113. <a href="https://doi.org/10.1145/3340555.3353726">https://doi.org/10.1145/3340555.3353726</a></li>
            <li>Starr, E. L., Reilly, J. M., &amp; Schneider, B. (2018). Toward Using Multi-Modal Learning Analytics to Support and Measure Collaboration in Co-Located Dyads. <i>International Society of the Learning Sciences, Inc.[ISLS</i>.</li>
            <li>Stewart, A. E. B., Keirn, Z. A., &amp; D’Mello, S. K. (2018). Multimodal Modeling of Coordination and Coregulation Patterns in Speech Rate during Triadic Collaborative Problem Solving. <i>Proceedings of the 2018 on International Conference on Multimodal Interaction&nbsp; - ICMI ’18</i>, 21–30. <a href="https://doi.org/10.1145/3242969.3242989">https://doi.org/10.1145/3242969.3242989</a></li>
            <li>Stroebe, W., &amp; Strack, F. (2014). The alleged crisis and the illusion of exact replication. <i>Perspectives on Psychological Science</i>, <i>9</i>(1), 59–71.</li>
            <li>Tapola, A., Veermans, M., &amp; Niemivirta, M. (2013). Predictors and outcomes of situational interest during a science learning tasks. <i>Instructional Science</i>, <i>41</i>(6), 1047–1064.</li>
            <li>Terken, J., &amp; Sturm, J. (2010). Multimodal support for social dynamics in co-located meetings. <i>Personal and Ubiquitous Computing</i>, <i>14</i>(8), 703–714.</li>
            <li>Viswanathan, S. A., &amp; Vanlehn, K. (2017). High accuracy detection of collaboration from log data and superficial speech features. <i>International Society of the Learning Sciences</i>.</li>
            <li>Viswanathan, S. A., &amp; VanLehn, K. (2017). Using the tablet gestures and speech of pairs of students to classify their collaboration. <i>IEEE Transactions on Learning Technologies</i>, <i>11</i>(2), 230–242.</li>
            <li>Vrzakova, H., Amon, M. J., Stewart, A., Duran, N. D., &amp; D’Mello, S. K. (2020). Focused or stuck together: Multimodal patterns reveal triads’ performance in collaborative problem solving. <i>Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge</i>, 295–304.</li>
            <li>Vrzakova, H., Amon, M. J., Stewart, A. E., &amp; D’Mello, S. K. (2019). Dynamics of visual attention in multiparty collaborative problem solving using multidimensional recurrence quantification analysis. <i>Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems</i>, 1–14.</li>
            <li>Winne, P. H. (2010). Improving measurements of self-regulated learning. <i>Educational Psychologist</i>, <i>45</i>(4), 267–276.</li>
            <li>Worsley, M. (2018). <i>(Dis)Engagement Matters: Identifying Eﬀicacious Learning Practices with Multimodal Learning Analytics</i>. 5.</li>
            <li>Xie, B., Reilly, J. M., Dich, Y. L., &amp; Schneider, B. (2018). <i>Augmenting Qualitative Analyses of Collaborative Learning Groups Through Multi-Modal Sensing</i>. International Society of the Learning Sciences, Inc.[ISLS].</li>
            <li>Yamashita, N., Kaji, K., Kuzuoka, H., &amp; Hirata, K. (2011). Improving visibility of remote gestures in distributed tabletop collaboration. <i>Proceedings of the ACM 2011 Conference on Computer Supported Cooperative Work</i>, 95–104.</li>
            <li>Yoon, D., Chen, N., Randles, B., Cheatle, A., Löckenhoff, C. E., Jackson, S. J., Sellen, A., &amp; Guimbretière, F. (2016). RichReview++ Deployment of a Collaborative Multi-modal Annotation System for Instructor Feedback and Peer Discussion. <i>Proceedings of the 19th ACM Conference on Computer-Supported Cooperative Work &amp; Social Computing</i>, 195–205.</li>
            <li>Yvonne A. W. de Kort, IJsselsteijn, W. A., &amp; Poels, K. (2007). Digital games as social presence technology: Development of the Social Presence in Gaming Questionnaire (SPGQ). <i>Proceedings of PRESENCE</i>, 1–9.</li>
        </ul>
        </div>
    </div>
</body>
</html> 
