paper_id,paper_title,metric,metric_smaller_category,metric_larger_category,outcome,outcome_smaller_category,outcome_larger_category,outcome_instrument,analysis_method,analysis_result
1,An Interactive Table for Supporting Participation Balance in Face-to-Face Collaborative Learning,speech time,audio features,verbal,verbal participation,communication,product,survey,t-test,sig (t[38] = 2.18)
3,Understanding collaborative program comprehension: Interlacing gaze and dialogues,focused gaze,gaze / eye direction,gaze,program understanding,performance,product,researcher codes,anova,sig
3,Understanding collaborative program comprehension: Interlacing gaze and dialogues,focused gaze,gaze / eye direction,gaze,program understanding,performance,product,researcher codes,mixed linear model,sig
3,Understanding collaborative program comprehension: Interlacing gaze and dialogues,focused gaze,gaze / eye direction,gaze,dialogue episodes,coordination,product,researcher codes,mixed linear model,sig
3,Understanding collaborative program comprehension: Interlacing gaze and dialogues,together gaze,gaze / eye direction,gaze,program understanding,performance,product,researcher codes,anova,sig
3,Understanding collaborative program comprehension: Interlacing gaze and dialogues,together gaze,gaze / eye direction,gaze,program understanding,performance,product,researcher codes,mixed linear model,sig
3,Understanding collaborative program comprehension: Interlacing gaze and dialogues,together gaze,gaze / eye direction,gaze,dialogue episodes,coordination,product,researcher codes,mixed linear model,sig
3,Understanding collaborative program comprehension: Interlacing gaze and dialogues,gaze transitions,eye motion,gaze,dialogue episodes,coordination,product,researcher codes,anova,sig
4,Physiological Linkage of Dyadic Gaming Experience,physiological linkage,electrodermal activity,physiological,behavioral involvement,individual cognitive processes,process,social presence in gaming questionnaire,regression,sig
4,Physiological Linkage of Dyadic Gaming Experience,physiological linkage,electrodermal activity,physiological,empathy,affective state,process,social presence in gaming questionnaire,regression,sig
4,Physiological Linkage of Dyadic Gaming Experience,physiological linkage,electrodermal activity,physiological,negative feelings,affective state,process,social presence in gaming questionnaire,regression,sig
4,Physiological Linkage of Dyadic Gaming Experience,physiological linkage,electrodermal activity,physiological,perceived comprehension,learning,product,social presence inventory questionnaire,regression,sig
4,Physiological Linkage of Dyadic Gaming Experience,physiological linkage,electrodermal activity,physiological,behavioral involvement,individual cognitive processes,process,social presence in gaming questionnaire,regression,sig
4,Physiological Linkage of Dyadic Gaming Experience,physiological linkage,electrodermal activity,physiological,empathy,affective state,process,social presence in gaming questionnaire,regression,sig
4,Physiological Linkage of Dyadic Gaming Experience,physiological linkage,electrodermal activity,physiological,negative feelings,affective state,process,social presence in gaming questionnaire,regression,sig
4,Physiological Linkage of Dyadic Gaming Experience,physiological linkage,electrodermal activity,physiological,perceived comprehension,learning,product,social presence inventory questionnaire,regression,sig
5,A Multimodal-Sensor-Enabled Room for Unobtrusive Group Meeting Analysis,"non-verbal speaking metrics (speaking length, interruptions, etc) [10 metrics]",speech participation,verbal,leadership perceived,interpersonal relationship / perception,process,questionnaire,correlation,nonsig
5,A Multimodal-Sensor-Enabled Room for Unobtrusive Group Meeting Analysis,"non-verbal speaking metrics (speaking length, interruptions, etc) [10 metrics]",speech participation,verbal,contribution perceived,interpersonal relationship / perception,process,questionnaire,correlation,nonsig
5,A Multimodal-Sensor-Enabled Room for Unobtrusive Group Meeting Analysis,"non-verbal speaking metrics (speaking length, interruptions, etc) [10 metrics]",speech participation,verbal,leadership perceived,interpersonal relationship / perception,process,questionnaire,regression,nonsig
5,A Multimodal-Sensor-Enabled Room for Unobtrusive Group Meeting Analysis,"non-verbal speaking metrics (speaking length, interruptions, etc) [10 metrics]",speech participation,verbal,contribution perceived,interpersonal relationship / perception,process,questionnaire,regression,nonsig
5,A Multimodal-Sensor-Enabled Room for Unobtrusive Group Meeting Analysis,visual attention metrics [8 metrics],gaze / eye direction,gaze,leadership perceived,interpersonal relationship / perception,process,questionnaire,correlation,nonsig
5,A Multimodal-Sensor-Enabled Room for Unobtrusive Group Meeting Analysis,visual attention metrics [8 metrics],gaze / eye direction,gaze,contribution perceived,interpersonal relationship / perception,process,questionnaire,correlation,sig
5,A Multimodal-Sensor-Enabled Room for Unobtrusive Group Meeting Analysis,visual attention metrics [8 metrics],gaze / eye direction,gaze,leadership perceived,interpersonal relationship / perception,process,questionnaire,regression,nonsig
5,A Multimodal-Sensor-Enabled Room for Unobtrusive Group Meeting Analysis,visual attention metrics [8 metrics],gaze / eye direction,gaze,contribution perceived,interpersonal relationship / perception,process,questionnaire,regression,nonsig
5,A Multimodal-Sensor-Enabled Room for Unobtrusive Group Meeting Analysis,verbal dominance and information metrics [9 metrics],speech participation,verbal,leadership perceived,interpersonal relationship / perception,process,questionnaire,correlation,sig
5,A Multimodal-Sensor-Enabled Room for Unobtrusive Group Meeting Analysis,verbal dominance and information metrics [9 metrics],speech participation,verbal,contribution perceived,interpersonal relationship / perception,process,questionnaire,correlation,sig
5,A Multimodal-Sensor-Enabled Room for Unobtrusive Group Meeting Analysis,verbal dominance and information metrics [9 metrics],speech participation,verbal,leadership perceived,interpersonal relationship / perception,process,questionnaire,regression,nonsig
5,A Multimodal-Sensor-Enabled Room for Unobtrusive Group Meeting Analysis,verbal dominance and information metrics [9 metrics],speech participation,verbal,contribution perceived,interpersonal relationship / perception,process,questionnaire,regression,nonsig
6,"Are we together or not? The temporal interplay of monitoring, physiological arousal and physiological synchrony during a collaborative exam",eda peak detection,electrodermal activity,physiological,"monitoring of behavior, cognition, motivations and emotions",individual cognitive processes,process,"video coding of monitoring (behavior, cognition, motivations and emotions)",correlation,sig (r=0.663)
6,"Are we together or not? The temporal interplay of monitoring, physiological arousal and physiological synchrony during a collaborative exam",physiological concordance index,mixed,physiological,"monitoring of behavior, cognition, motivations and emotions",individual cognitive processes,process,"video coding of monitoring (behavior, cognition, motivations and emotions)",correlation,sig (r=0.663)
10,"Linking Speaking and Looking Behavior Patterns with Group Composition, Perception, and Performance","group participation speaking cues (speaking length, speaking turns, successful interruptions, unsuccessful interruptions, backchannels)",speech participation,verbal,"group composition (agreeableness, conscientiousness, extraversion, neuroticism, and openness to experience)",group composition,condition,neo-ffi questionnaire,correlation,sig
10,"Linking Speaking and Looking Behavior Patterns with Group Composition, Perception, and Performance","group participation speaking cues (speaking length, speaking turns, successful interruptions, unsuccessful interruptions, backchannels)",speech participation,verbal,"group interpersonal perception (perceived dominance, perceived leadership, perceived competence, perceived liking)",interpersonal relationship / perception,process,questionnaire,correlation,sig
10,"Linking Speaking and Looking Behavior Patterns with Group Composition, Perception, and Performance","group participation speaking cues (speaking length, speaking turns, successful interruptions, unsuccessful interruptions, backchannels)",speech participation,verbal,group performance (negative distance between expert list and group list),performance,product,negative distance between expert list and group list,correlation,sig
10,"Linking Speaking and Looking Behavior Patterns with Group Composition, Perception, and Performance","group participation speaking cues (speaking length, speaking turns, successful interruptions, unsuccessful interruptions, backchannels)",speech participation,verbal,"group composition (agreeableness, conscientiousness, extraversion, neuroticism, and openness to experience)",group composition,condition,neo-ffi questionnaire,regression,sig
10,"Linking Speaking and Looking Behavior Patterns with Group Composition, Perception, and Performance","silence and overlap cues (fraction of silence, fraction of nonoverlapped speech, fraction of two-people and three-people overlapped speech)",speech participation,verbal,"group composition (agreeableness, conscientiousness, extraversion, neuroticism, and openness to experience)",group composition,condition,neo-ffi questionnaire,correlation,sig
10,"Linking Speaking and Looking Behavior Patterns with Group Composition, Perception, and Performance","silence and overlap cues (fraction of silence, fraction of nonoverlapped speech, fraction of two-people and three-people overlapped speech)",speech participation,verbal,"group interpersonal perception (perceived dominance, perceived leadership, perceived competence, perceived liking)",interpersonal relationship / perception,process,questionnaire,correlation,sig
10,"Linking Speaking and Looking Behavior Patterns with Group Composition, Perception, and Performance","silence and overlap cues (fraction of silence, fraction of nonoverlapped speech, fraction of two-people and three-people overlapped speech)",speech participation,verbal,group performance (negative distance between expert list and group list),performance,product,negative distance between expert list and group list,correlation,sig
10,"Linking Speaking and Looking Behavior Patterns with Group Composition, Perception, and Performance","silence and overlap cues (fraction of silence, fraction of nonoverlapped speech, fraction of two-people and three-people overlapped speech)",speech participation,verbal,"group composition (agreeableness, conscientiousness, extraversion, neuroticism, and openness to experience)",group composition,condition,neo-ffi questionnaire,regression,sig
10,"Linking Speaking and Looking Behavior Patterns with Group Composition, Perception, and Performance","silence and overlap cues (fraction of silence, fraction of nonoverlapped speech, fraction of two-people and three-people overlapped speech)",speech participation,verbal,"group interpersonal perception (perceived dominance, perceived leadership, perceived competence, perceived liking)",interpersonal relationship / perception,process,questionnaire,regression,sig
10,"Linking Speaking and Looking Behavior Patterns with Group Composition, Perception, and Performance","silence and overlap cues (fraction of silence, fraction of nonoverlapped speech, fraction of two-people and three-people overlapped speech)",speech participation,verbal,group performance (negative distance between expert list and group list),performance,product,negative distance between expert list and group list,regression,sig
10,"Linking Speaking and Looking Behavior Patterns with Group Composition, Perception, and Performance","speaking distribution cues (speaking length skew, speaking turns skew, successful interruption skew, unsuccessful interruptions skew, backchannels skew)",speech participation,verbal,"group composition (agreeableness, conscientiousness, extraversion, neuroticism, and openness to experience)",group composition,condition,neo-ffi questionnaire,correlation,sig
10,"Linking Speaking and Looking Behavior Patterns with Group Composition, Perception, and Performance","speaking distribution cues (speaking length skew, speaking turns skew, successful interruption skew, unsuccessful interruptions skew, backchannels skew)",speech participation,verbal,"group interpersonal perception (perceived dominance, perceived leadership, perceived competence, perceived liking)",interpersonal relationship / perception,process,questionnaire,correlation,sig
10,"Linking Speaking and Looking Behavior Patterns with Group Composition, Perception, and Performance","speaking distribution cues (speaking length skew, speaking turns skew, successful interruption skew, unsuccessful interruptions skew, backchannels skew)",speech participation,verbal,group performance (negative distance between expert list and group list),performance,product,negative distance between expert list and group list,correlation,sig
10,"Linking Speaking and Looking Behavior Patterns with Group Composition, Perception, and Performance","speaking distribution cues (speaking length skew, speaking turns skew, successful interruption skew, unsuccessful interruptions skew, backchannels skew)",speech participation,verbal,"group interpersonal perception (perceived dominance, perceived leadership, perceived competence, perceived liking)",interpersonal relationship / perception,process,questionnaire,regression,sig
10,"Linking Speaking and Looking Behavior Patterns with Group Composition, Perception, and Performance",individual visual focus of attention,gaze / eye direction,gaze,"group composition (agreeableness, conscientiousness, extraversion, neuroticism, and openness to experience)",group composition,condition,neo-ffi questionnaire,correlation,sig
10,"Linking Speaking and Looking Behavior Patterns with Group Composition, Perception, and Performance",individual visual focus of attention,gaze / eye direction,gaze,"group interpersonal perception (perceived dominance, perceived leadership, perceived competence, perceived liking)",interpersonal relationship / perception,process,questionnaire,correlation,sig
10,"Linking Speaking and Looking Behavior Patterns with Group Composition, Perception, and Performance",individual visual focus of attention,gaze / eye direction,gaze,group performance (negative distance between expert list and group list),performance,product,negative distance between expert list and group list,correlation,sig
10,"Linking Speaking and Looking Behavior Patterns with Group Composition, Perception, and Performance","group looking cues (fraction of people gaze, fraction of convergent gaze, fraction of mutual gaze, fraction of shared gaze, gaze skew)",gaze / eye direction,gaze,"group composition (agreeableness, conscientiousness, extraversion, neuroticism, and openness to experience)",group composition,condition,neo-ffi questionnaire,correlation,sig
10,"Linking Speaking and Looking Behavior Patterns with Group Composition, Perception, and Performance","group looking cues (fraction of people gaze, fraction of convergent gaze, fraction of mutual gaze, fraction of shared gaze, gaze skew)",gaze / eye direction,gaze,"group interpersonal perception (perceived dominance, perceived leadership, perceived competence, perceived liking)",interpersonal relationship / perception,process,questionnaire,correlation,sig
10,"Linking Speaking and Looking Behavior Patterns with Group Composition, Perception, and Performance","group looking cues (fraction of people gaze, fraction of convergent gaze, fraction of mutual gaze, fraction of shared gaze, gaze skew)",gaze / eye direction,gaze,group performance (negative distance between expert list and group list),performance,product,negative distance between expert list and group list,correlation,sig
11,Automatic Recognition of Affective Laughter in Spontaneous Dyadic Interactions from Audiovisual Signals,88 gemaps acoustic features,audio features,verbal,voiced laughter,interpersonal relationship / perception,process,voicing probability and unvoiced frame ratio,sup. machine learning,ns
11,Automatic Recognition of Affective Laughter in Spontaneous Dyadic Interactions from Audiovisual Signals,88 gemaps acoustic features,audio features,verbal,unvoiced laughter,interpersonal relationship / perception,process,voicing probability and unvoiced frame ratio,sup. machine learning,ns
11,Automatic Recognition of Affective Laughter in Spontaneous Dyadic Interactions from Audiovisual Signals,88 gemaps acoustic features,audio features,verbal,speech laughter,interpersonal relationship / perception,process,human annotations,sup. machine learning,ns
11,Automatic Recognition of Affective Laughter in Spontaneous Dyadic Interactions from Audiovisual Signals,102 extended gemaps acoustic features,audio features,verbal,voiced laughter,interpersonal relationship / perception,process,voicing probability and unvoiced frame ratio,sup. machine learning,ns
11,Automatic Recognition of Affective Laughter in Spontaneous Dyadic Interactions from Audiovisual Signals,102 extended gemaps acoustic features,audio features,verbal,speech laughter,interpersonal relationship / perception,process,human annotations,sup. machine learning,ns
11,Automatic Recognition of Affective Laughter in Spontaneous Dyadic Interactions from Audiovisual Signals,12 mfccs,audio features,verbal,voiced laughter,interpersonal relationship / perception,process,voicing probability and unvoiced frame ratio,sup. machine learning,ns
11,Automatic Recognition of Affective Laughter in Spontaneous Dyadic Interactions from Audiovisual Signals,12 mfccs,audio features,verbal,unvoiced laughter,interpersonal relationship / perception,process,voicing probability and unvoiced frame ratio,sup. machine learning,ns
11,Automatic Recognition of Affective Laughter in Spontaneous Dyadic Interactions from Audiovisual Signals,12 mfccs,audio features,verbal,speech laughter,interpersonal relationship / perception,process,human annotations,sup. machine learning,ns
11,Automatic Recognition of Affective Laughter in Spontaneous Dyadic Interactions from Audiovisual Signals,51 facial action units,facial expression,head,voiced laughter,interpersonal relationship / perception,process,voicing probability and unvoiced frame ratio,sup. machine learning,ns
11,Automatic Recognition of Affective Laughter in Spontaneous Dyadic Interactions from Audiovisual Signals,51 facial action units,facial expression,head,unvoiced laughter,interpersonal relationship / perception,process,voicing probability and unvoiced frame ratio,sup. machine learning,ns
11,Automatic Recognition of Affective Laughter in Spontaneous Dyadic Interactions from Audiovisual Signals,51 facial action units,facial expression,head,speech laughter,interpersonal relationship / perception,process,human annotations,sup. machine learning,ns
12,Predicting Group Performance in Task-Based Interaction,speech features,audio features,verbal,group performance scores,performance,product,group scores compared to expert scores,sup. machine learning,64.4
12,Predicting Group Performance in Task-Based Interaction,linguistic features,verbal content,verbal,group performance scores,performance,product,group scores compared to expert scores,sup. machine learning,64.4
14,Looking AT versus Looking THROUGH: A Dual Eye-tracking Study in MOOC Context,perceptual with-me-ness (gaze),gaze / eye direction,gaze,learning gains,learning,product,pre-post test,correlation,sig (r = 0.51)
14,Looking AT versus Looking THROUGH: A Dual Eye-tracking Study in MOOC Context,conceptual with-me-ness (gaze),gaze / eye direction,gaze,learning gains,learning,product,pre-post test,correlation,sig (r = 0.41)
14,Looking AT versus Looking THROUGH: A Dual Eye-tracking Study in MOOC Context,gaze similarity,gaze / eye direction,gaze,learning gains,learning,product,pre-post test,correlation,sig (r = 0.39)
15,A Network Analytic Approach to Gaze Coordination during a Collaborative Task,gaze fixations,gaze / eye direction,gaze,reference-action sequence,performance,product,researcher codes,correlation,ns
15,A Network Analytic Approach to Gaze Coordination during a Collaborative Task,gaze saccades,eye motion,gaze,reference-action sequence,performance,product,researcher codes,correlation,ns
17,"The Additive Value of Multimodal Features for Predicting Engagement, Frustration, and Learning during Tutoring",dialogue acts,verbal content,verbal,engagement,individual cognitive processes,process,participant self report,regression,sig
17,"The Additive Value of Multimodal Features for Predicting Engagement, Frustration, and Learning during Tutoring",dialogue acts,verbal content,verbal,frustration,affective state,process,participant self report,regression,sig
17,"The Additive Value of Multimodal Features for Predicting Engagement, Frustration, and Learning during Tutoring",dialogue acts,verbal content,verbal,learning,learning,product,pre-post test,regression,sig
17,"The Additive Value of Multimodal Features for Predicting Engagement, Frustration, and Learning during Tutoring",facial expression,facial expression,head,engagement,individual cognitive processes,process,participant self report,regression,sig
17,"The Additive Value of Multimodal Features for Predicting Engagement, Frustration, and Learning during Tutoring",facial expression,facial expression,head,frustration,affective state,process,participant self report,regression,sig
17,"The Additive Value of Multimodal Features for Predicting Engagement, Frustration, and Learning during Tutoring",facial expression,facial expression,head,learning,learning,product,pre-post test,regression,sig
17,"The Additive Value of Multimodal Features for Predicting Engagement, Frustration, and Learning during Tutoring",gesture,hand motion,body,engagement,individual cognitive processes,process,participant self report,regression,sig
17,"The Additive Value of Multimodal Features for Predicting Engagement, Frustration, and Learning during Tutoring",gesture,hand motion,body,frustration,affective state,process,participant self report,regression,sig
17,"The Additive Value of Multimodal Features for Predicting Engagement, Frustration, and Learning during Tutoring",gesture,hand motion,body,learning,learning,product,pre-post test,regression,sig
18,(Dis)Engagement Maters: Identifying Efficacious Learning Practices with Multimodal Learning Analytics,clustered hand/wrist movement,hand motion,body,learning gains,learning,product,pre-post test (references to  principles or mechanisms that confer stability to three example structures),sup. machine learning,sig (acc
19,Dual Gaze as a Proxy for Collaboration in Informal Learning,(not) focused together,gaze / eye direction,gaze,level of understanding,learning,product,researcher codes,anova,"sig (f [1,16]=8.70)"
19,Dual Gaze as a Proxy for Collaboration in Informal Learning,(not) focused together,gaze / eye direction,gaze,level of understanding,learning,product,researcher codes,anova,"sig (f [1,61]=7.60)"
19,Dual Gaze as a Proxy for Collaboration in Informal Learning,"dialogue episodes (description, management)",verbal content,verbal,level of understanding,learning,product,researcher codes,anova,"sig (f [1,61]=7.60)"
20,Using Eye-Tracking Technology to Support Visual Coordination in Collaborative Problem-Solving Groups,joint visual attention,gaze / eye direction,gaze,learning gains,learning,product,learning test,regression,sig
20,Using Eye-Tracking Technology to Support Visual Coordination in Collaborative Problem-Solving Groups,joint visual attention,gaze / eye direction,gaze,quality of collaboration,coordination,product,coding scheme,regression,sig
20,Using Eye-Tracking Technology to Support Visual Coordination in Collaborative Problem-Solving Groups,joint visual attention,gaze / eye direction,gaze,quality of collaboration,communication,product,coding scheme,regression,sig
21,Analysing frequent sequential patterns of collaborative learning activity around an interactive tabletop,events,task-related,activity log,group performance,performance,product,researcher codes,unsup. machine learning,sig
22,The Effect of Mutual Gaze Perception on Students’ Verbal Coordination,joint visual attention,gaze / eye direction,gaze,learning gains,learning,product,learning test,anova,nosig
22,The Effect of Mutual Gaze Perception on Students’ Verbal Coordination,joint visual attention,gaze / eye direction,gaze,quality of collaboration,coordination,product,coding scheme,anova,nosig
22,The Effect of Mutual Gaze Perception on Students’ Verbal Coordination,joint visual attention,gaze / eye direction,gaze,quality of collaboration,communication,product,coding scheme,anova,nosig
22,The Effect of Mutual Gaze Perception on Students’ Verbal Coordination,joint visual attention,gaze / eye direction,gaze,learning gains,learning,product,learning test,correlation,sig
22,The Effect of Mutual Gaze Perception on Students’ Verbal Coordination,n-grams,verbal content,verbal,learning gains,learning,product,learning test,sup. machine learning,75%
22,The Effect of Mutual Gaze Perception on Students’ Verbal Coordination,cosine similarity scores,verbal content,verbal,learning gains,learning,product,learning test,sup. machine learning,75%
22,The Effect of Mutual Gaze Perception on Students’ Verbal Coordination,convergence measures,verbal content,verbal,learning gains,learning,product,learning test,anova,nosig
22,The Effect of Mutual Gaze Perception on Students’ Verbal Coordination,convergence measures,verbal content,verbal,quality of collaboration,coordination,product,coding scheme,anova,nosig
22,The Effect of Mutual Gaze Perception on Students’ Verbal Coordination,convergence measures,verbal content,verbal,quality of collaboration,communication,product,coding scheme,anova,nosig
22,The Effect of Mutual Gaze Perception on Students’ Verbal Coordination,convergence measures,verbal content,verbal,learning gains,learning,product,learning test,sup. machine learning,75%
22,The Effect of Mutual Gaze Perception on Students’ Verbal Coordination,coherence metrics,verbal content,verbal,learning gains,learning,product,learning test,correlation,sig
22,The Effect of Mutual Gaze Perception on Students’ Verbal Coordination,coherence metrics,verbal content,verbal,learning gains,learning,product,learning test,sup. machine learning,75%
24,Detecting Collaborative Dynamics Using Mobile Eye-Trackers,joint visual attention,gaze / eye direction,gaze,learning gains,learning,product,learning test,qualitative,sig
24,Detecting Collaborative Dynamics Using Mobile Eye-Trackers,joint visual attention,gaze / eye direction,gaze,learning gains,learning,product,learning test,correlation,sig
24,Detecting Collaborative Dynamics Using Mobile Eye-Trackers,gestures,hand motion,body,learning gains,learning,product,learning test,qualitative,sig
24,Detecting Collaborative Dynamics Using Mobile Eye-Trackers,speech duration,speech participation,verbal,learning gains,learning,product,learning test,qualitative,sig
25,Expertise estimation based on simple multimodal features,calculator use,task-related,activity log,odds of a student solving correctly a problem,performance,product,researcher codes,regression,sig
25,Expertise estimation based on simple multimodal features,calculator use,task-related,activity log,expert prediction,group composition,condition,researcher codes,sup. machine learning,sig
25,Expertise estimation based on simple multimodal features,total movement,gross body motion,body,odds of a student solving correctly a problem,performance,product,researcher codes,regression,sig
25,Expertise estimation based on simple multimodal features,total movement,gross body motion,body,expert prediction,group composition,condition,researcher codes,sup. machine learning,sig
25,Expertise estimation based on simple multimodal features,distance from the center of the table,location,body,odds of a student solving correctly a problem,performance,product,researcher codes,regression,sig
25,Expertise estimation based on simple multimodal features,distance from the center of the table,location,body,expert prediction,group composition,condition,researcher codes,sup. machine learning,sig
25,Expertise estimation based on simple multimodal features,number of interventions,speech participation,verbal,odds of a student solving correctly a problem,performance,product,researcher codes,regression,sig
25,Expertise estimation based on simple multimodal features,number of interventions,speech participation,verbal,expert prediction,group composition,condition,researcher codes,sup. machine learning,sig
25,Expertise estimation based on simple multimodal features,total speech duration,speech participation,verbal,odds of a student solving correctly a problem,performance,product,researcher codes,regression,sig
25,Expertise estimation based on simple multimodal features,total speech duration,speech participation,verbal,expert prediction,group composition,condition,researcher codes,sup. machine learning,sig
25,Expertise estimation based on simple multimodal features,times numbers were mentioned,verbal content,verbal,odds of a student solving correctly a problem,performance,product,researcher codes,regression,sig
25,Expertise estimation based on simple multimodal features,times numbers were mentioned,verbal content,verbal,expert prediction,group composition,condition,researcher codes,sup. machine learning,sig
25,Expertise estimation based on simple multimodal features,times mathematical terms were mentioned,verbal content,verbal,odds of a student solving correctly a problem,performance,product,researcher codes,regression,sig
25,Expertise estimation based on simple multimodal features,times mathematical terms were mentioned,verbal content,verbal,expert prediction,group composition,condition,researcher codes,sup. machine learning,sig
25,Expertise estimation based on simple multimodal features,times commands were pronounced,verbal content,verbal,odds of a student solving correctly a problem,performance,product,researcher codes,regression,sig
25,Expertise estimation based on simple multimodal features,times commands were pronounced,verbal content,verbal,expert prediction,group composition,condition,researcher codes,sup. machine learning,sig
25,Expertise estimation based on simple multimodal features,total number of pen strokes,writing action,activity log,odds of a student solving correctly a problem,performance,product,researcher codes,regression,sig
25,Expertise estimation based on simple multimodal features,total number of pen strokes,writing action,activity log,expert prediction,group composition,condition,researcher codes,sup. machine learning,sig
25,Expertise estimation based on simple multimodal features,average number of points,writing action,activity log,odds of a student solving correctly a problem,performance,product,researcher codes,regression,sig
25,Expertise estimation based on simple multimodal features,average number of points,writing action,activity log,expert prediction,group composition,condition,researcher codes,sup. machine learning,sig
25,Expertise estimation based on simple multimodal features,average stroke time length,writing action,activity log,odds of a student solving correctly a problem,performance,product,researcher codes,regression,sig
25,Expertise estimation based on simple multimodal features,average stroke time length,writing action,activity log,expert prediction,group composition,condition,researcher codes,sup. machine learning,sig
25,Expertise estimation based on simple multimodal features,average stroke path length,writing action,activity log,odds of a student solving correctly a problem,performance,product,researcher codes,regression,sig
25,Expertise estimation based on simple multimodal features,average stroke path length,writing action,activity log,expert prediction,group composition,condition,researcher codes,sup. machine learning,sig
25,Expertise estimation based on simple multimodal features,average stroke displacement,writing action,activity log,odds of a student solving correctly a problem,performance,product,researcher codes,regression,sig
25,Expertise estimation based on simple multimodal features,average stroke displacement,writing action,activity log,expert prediction,group composition,condition,researcher codes,sup. machine learning,sig
25,Expertise estimation based on simple multimodal features,average stroke pressure,writing action,activity log,odds of a student solving correctly a problem,performance,product,researcher codes,regression,sig
25,Expertise estimation based on simple multimodal features,average stroke pressure,writing action,activity log,expert prediction,group composition,condition,researcher codes,sup. machine learning,sig
26,Personality Trait Classification via Co-Occurrent Multiparty Multimodal Event Discovery,speaking status,speech participation,verbal,big five personality traits scores,group composition,condition,self-reported survey + researcher codes,unsup. machine learning,69.61%
26,Personality Trait Classification via Co-Occurrent Multiparty Multimodal Event Discovery,pitch,audio features,verbal,big five personality traits scores,group composition,condition,self-reported survey + researcher codes,unsup. machine learning,69.61%
26,Personality Trait Classification via Co-Occurrent Multiparty Multimodal Event Discovery,energy,audio features,verbal,big five personality traits scores,group composition,condition,self-reported survey + researcher codes,unsup. machine learning,69.61%
26,Personality Trait Classification via Co-Occurrent Multiparty Multimodal Event Discovery,head motion,head motion,head,big five personality traits scores,group composition,condition,self-reported survey + researcher codes,unsup. machine learning,69.61%
26,Personality Trait Classification via Co-Occurrent Multiparty Multimodal Event Discovery,body motion,gross body motion,body,big five personality traits scores,group composition,condition,self-reported survey + researcher codes,unsup. machine learning,69.61%
26,Personality Trait Classification via Co-Occurrent Multiparty Multimodal Event Discovery,motion energy images,gross body motion,body,big five personality traits scores,group composition,condition,self-reported survey + researcher codes,unsup. machine learning,69.61%
26,Personality Trait Classification via Co-Occurrent Multiparty Multimodal Event Discovery,gaze,gaze / eye direction,gaze,big five personality traits scores,group composition,condition,self-reported survey + researcher codes,unsup. machine learning,69.61%
27,Dynamic Adaptive Gesturing Predicts Domain Expertise in Mathematics,total manual gestures per second,hand motion,body,expertise,group composition,condition,researcher codes,wilcoxon signed ranks test,sig
27,Dynamic Adaptive Gesturing Predicts Domain Expertise in Mathematics,iconic gestures per second,hand motion,body,expertise,group composition,condition,researcher codes,wilcoxon signed ranks test,sig
27,Dynamic Adaptive Gesturing Predicts Domain Expertise in Mathematics,deictic gestures per second,hand motion,body,expertise,group composition,condition,researcher codes,wilcoxon signed ranks test,non-sig
28,Capturing and analyzing verbal and physical collaborative learning interactions at an enriched interactive tabletop,sequences of verbal utterances,verbal content,verbal,colloboration quality,coordination,product,researcher codes,unsup. machine learning,90%
28,Capturing and analyzing verbal and physical collaborative learning interactions at an enriched interactive tabletop,sequences of verbal utterances,verbal content,verbal,colloboration quality,communication,product,researcher codes,unsup. machine learning,90%
28,Capturing and analyzing verbal and physical collaborative learning interactions at an enriched interactive tabletop,sequences of meaningful actions,touch,activity log,colloboration quality,coordination,product,researcher codes,unsup. machine learning,90%
28,Capturing and analyzing verbal and physical collaborative learning interactions at an enriched interactive tabletop,sequences of meaningful actions,touch,activity log,colloboration quality,communication,product,researcher codes,unsup. machine learning,90%
32,Automatic identification of experts and performance prediction in the multimodal math data corpus through analysis of speech interaction. ,duration of all vocalisations,speech participation,verbal,identity of the expert,group composition,condition,"student with the highest cumulative score of points assigned to correctly and incorrectly solved problems, weighted according a discrete diffculty level scale",sup. machine learning,sig (acc
32,Automatic identification of experts and performance prediction in the multimodal math data corpus through analysis of speech interaction. ,average duration of vocalisation,speech participation,verbal,identity of the expert,group composition,condition,"student with the highest cumulative score of points assigned to correctly and incorrectly solved problems, weighted according a discrete diffculty level scale",sup. machine learning,sig (acc
32,Automatic identification of experts and performance prediction in the multimodal math data corpus through analysis of speech interaction. ,standard deviation of vocalisation,speech participation,verbal,identity of the expert,group composition,condition,"student with the highest cumulative score of points assigned to correctly and incorrectly solved problems, weighted according a discrete diffculty level scale",sup. machine learning,sig (acc
32,Automatic identification of experts and performance prediction in the multimodal math data corpus through analysis of speech interaction. ,"probability of a transition from floor"" (i.e. a pause, group switching pause or speaker switching pause) to a vocalisation",speech participation,verbal,identity of the expert,group composition,condition,"student with the highest cumulative score of points assigned to correctly and incorrectly solved problems, weighted according a discrete diffculty level scale",sup. machine learning,sig (acc
32,Automatic identification of experts and performance prediction in the multimodal math data corpus through analysis of speech interaction. ,probability of a transition from a vocalisation to floor,speech participation,verbal,identity of the expert,group composition,condition,"student with the highest cumulative score of points assigned to correctly and incorrectly solved problems, weighted according a discrete diffculty level scale",sup. machine learning,sig (acc
32,Automatic identification of experts and performance prediction in the multimodal math data corpus through analysis of speech interaction. ,probability of transitioning from a group vocalisation to speaker vocalisation,speech participation,verbal,identity of the expert,group composition,condition,"student with the highest cumulative score of points assigned to correctly and incorrectly solved problems, weighted according a discrete diffculty level scale",sup. machine learning,sig (acc
32,Automatic identification of experts and performance prediction in the multimodal math data corpus through analysis of speech interaction. ,probability of transitioning from a speaker vocalisation to a group vocalisation,speech participation,verbal,identity of the expert,group composition,condition,"student with the highest cumulative score of points assigned to correctly and incorrectly solved problems, weighted according a discrete diffculty level scale",sup. machine learning,sig (acc
32,Automatic identification of experts and performance prediction in the multimodal math data corpus through analysis of speech interaction. ,uncertainty in the transitions (turn taking) originating from a speaker,speech participation,verbal,identity of the expert,group composition,condition,"student with the highest cumulative score of points assigned to correctly and incorrectly solved problems, weighted according a discrete diffculty level scale",sup. machine learning,sig (acc
32,Automatic identification of experts and performance prediction in the multimodal math data corpus through analysis of speech interaction. ,transition probability between types of vocalisations,speech participation,verbal,task performance,performance,product,correct vs incorrect solutions,sup. machine learning,sig (f1
33,Unraveling Students' Interaction around a Tangible Interface Using Multimodal Learning Analytics.,amount of exploration,task-related,activity log,learning gains,learning,product,pre-post test,correlation,not sig
33,Unraveling Students' Interaction around a Tangible Interface Using Multimodal Learning Analytics.,amount of exploration,task-related,activity log,learning gain group (high or low),learning,product,pre-post test,sup. machine learning,sig (acc
33,Unraveling Students' Interaction around a Tangible Interface Using Multimodal Learning Analytics.,types of exploration,task-related,activity log,learning gains,learning,product,pre-post test,correlation,mixed
33,Unraveling Students' Interaction around a Tangible Interface Using Multimodal Learning Analytics.,types of exploration,task-related,activity log,learning gain group (high or low),learning,product,pre-post test,sup. machine learning,sig (acc
33,Unraveling Students' Interaction around a Tangible Interface Using Multimodal Learning Analytics.,amount of movement,gross body motion,body,learning gains,learning,product,pre-post test,correlation,not sig
33,Unraveling Students' Interaction around a Tangible Interface Using Multimodal Learning Analytics.,amount of movement,gross body motion,body,learning gain group (high or low),learning,product,pre-post test,sup. machine learning,sig (acc
33,Unraveling Students' Interaction around a Tangible Interface Using Multimodal Learning Analytics.,type of movement,gross body motion,body,learning gains,learning,product,pre-post test,correlation,sig
33,Unraveling Students' Interaction around a Tangible Interface Using Multimodal Learning Analytics.,type of movement,gross body motion,body,leadership (driver/passenger),group composition,condition,coding,anova,sig
33,Unraveling Students' Interaction around a Tangible Interface Using Multimodal Learning Analytics.,type of movement,gross body motion,body,learning gain group (high or low),learning,product,pre-post test,sup. machine learning,sig (acc
33,Unraveling Students' Interaction around a Tangible Interface Using Multimodal Learning Analytics.,body synchronization,gross body motion,body,learning gains,learning,product,pre-post test,anova,not sig
33,Unraveling Students' Interaction around a Tangible Interface Using Multimodal Learning Analytics.,body synchronization,gross body motion,body,learning gain group (high or low),learning,product,pre-post test,sup. machine learning,sig (acc
33,Unraveling Students' Interaction around a Tangible Interface Using Multimodal Learning Analytics.,body distance,location,body,learning gains,learning,product,pre-post test,correlation,not sig
33,Unraveling Students' Interaction around a Tangible Interface Using Multimodal Learning Analytics.,body distance,location,body,learning gain group (high or low),learning,product,pre-post test,sup. machine learning,sig (acc
39,Unpacking the relationship between existing and new measures of physiological synchrony and collaborative learning: a mixed methods study,physiological synchrony (pc),electrodermal activity,physiological,learning gains,learning,product,pre-post test,correlation,sig r = 0.35
39,Unpacking the relationship between existing and new measures of physiological synchrony and collaborative learning: a mixed methods study,physiological synchrony (da),electrodermal activity,physiological,collaboration quality,coordination,product,"meier, spada and rummel coding scheme",correlation,sig r =0.47
39,Unpacking the relationship between existing and new measures of physiological synchrony and collaborative learning: a mixed methods study,physiological synchrony (da),electrodermal activity,physiological,collaboration quality,communication,product,"meier, spada and rummel coding scheme",correlation,sig r =0.47
39,Unpacking the relationship between existing and new measures of physiological synchrony and collaborative learning: a mixed methods study,cycles of physiological synchrony (pc),electrodermal activity,physiological,collaboration quality,coordination,product,"meier, spada and rummel coding scheme",correlation,sig r = 0.57
39,Unpacking the relationship between existing and new measures of physiological synchrony and collaborative learning: a mixed methods study,cycles of physiological synchrony (pc),electrodermal activity,physiological,collaboration quality,communication,product,"meier, spada and rummel coding scheme",correlation,sig r = 0.57
39,Unpacking the relationship between existing and new measures of physiological synchrony and collaborative learning: a mixed methods study,cycles of physiological synchrony (pc),electrodermal activity,physiological,learning gains,learning,product,pre-post test,correlation,sig r = 0.47
40,Modeling Team-level Multimodal Dynamics during Multiparty Collaboration,speech rate,audio features,verbal,perceived collaboration quality,interpersonal relationship / perception,process,questionnaire,unsup. machine learning,ns
40,Modeling Team-level Multimodal Dynamics during Multiparty Collaboration,speech rate,audio features,verbal,perceived collaboration quality,coordination,product,questionnaire,unsup. machine learning,ns
40,Modeling Team-level Multimodal Dynamics during Multiparty Collaboration,speech rate,audio features,verbal,perceived valence,affective state,process,5-point likert scale,unsup. machine learning,ns
40,Modeling Team-level Multimodal Dynamics during Multiparty Collaboration,speech rate,audio features,verbal,perceived arousal,affective state,process,5-point likert scale,unsup. machine learning,ns
40,Modeling Team-level Multimodal Dynamics during Multiparty Collaboration,speech rate,audio features,verbal,task performance,performance,product,trophies earned,unsup. machine learning,ns
40,Modeling Team-level Multimodal Dynamics during Multiparty Collaboration,face and upper body movement,gross body motion,body,perceived collaboration quality,interpersonal relationship / perception,process,questionnaire,unsup. machine learning,ns
40,Modeling Team-level Multimodal Dynamics during Multiparty Collaboration,face and upper body movement,gross body motion,body,perceived collaboration quality,coordination,product,questionnaire,unsup. machine learning,ns
40,Modeling Team-level Multimodal Dynamics during Multiparty Collaboration,face and upper body movement,gross body motion,body,perceived valence,affective state,process,5-point likert scale,unsup. machine learning,ns
40,Modeling Team-level Multimodal Dynamics during Multiparty Collaboration,face and upper body movement,gross body motion,body,perceived arousal,affective state,process,5-point likert scale,unsup. machine learning,ns
40,Modeling Team-level Multimodal Dynamics during Multiparty Collaboration,face and upper body movement,gross body motion,body,task performance,performance,product,trophies earned,unsup. machine learning,ns
40,Modeling Team-level Multimodal Dynamics during Multiparty Collaboration,galvanic skin response,electrodermal activity,physiological,perceived collaboration quality,interpersonal relationship / perception,process,questionnaire,unsup. machine learning,ns
40,Modeling Team-level Multimodal Dynamics during Multiparty Collaboration,galvanic skin response,electrodermal activity,physiological,perceived collaboration quality,coordination,product,questionnaire,unsup. machine learning,ns
40,Modeling Team-level Multimodal Dynamics during Multiparty Collaboration,galvanic skin response,electrodermal activity,physiological,perceived valence,affective state,process,5-point likert scale,unsup. machine learning,ns
40,Modeling Team-level Multimodal Dynamics during Multiparty Collaboration,galvanic skin response,electrodermal activity,physiological,perceived arousal,affective state,process,5-point likert scale,unsup. machine learning,ns
40,Modeling Team-level Multimodal Dynamics during Multiparty Collaboration,galvanic skin response,electrodermal activity,physiological,task performance,performance,product,trophies earned,unsup. machine learning,ns
41,Leveraging Mobile Eye-Trackers to Capture Joint Visual Attention in Co-Located Collaborative Learning,joint visual attention,gaze / eye direction,gaze,collaboration quality,coordination,product,meier spada rummel coding scheme,correlation,sig r = 0.341
41,Leveraging Mobile Eye-Trackers to Capture Joint Visual Attention in Co-Located Collaborative Learning,joint visual attention,gaze / eye direction,gaze,collaboration quality,communication,product,meier spada rummel coding scheme,correlation,sig r = 0.341
41,Leveraging Mobile Eye-Trackers to Capture Joint Visual Attention in Co-Located Collaborative Learning,cycles of collaborative / individual work,gaze / eye direction,gaze,collaboration quality,coordination,product,meier spada rummel coding scheme,correlation,sig r = 0.347
41,Leveraging Mobile Eye-Trackers to Capture Joint Visual Attention in Co-Located Collaborative Learning,cycles of collaborative / individual work,gaze / eye direction,gaze,collaboration quality,communication,product,meier spada rummel coding scheme,correlation,sig r = 0.347
41,Leveraging Mobile Eye-Trackers to Capture Joint Visual Attention in Co-Located Collaborative Learning,cycles of collaborative / individual work,gaze / eye direction,gaze,learning gains,learning,product,pre-post test,correlation,sig r = 0.398
41,Leveraging Mobile Eye-Trackers to Capture Joint Visual Attention in Co-Located Collaborative Learning,cycles of collaborative / individual work,gaze / eye direction,gaze,task performance,performance,product,number of mazes solved,correlation,sig r = 0.355
42,Gaze quality assisted automatic recognition of social contexts in collaborative Tetris,gaze location,gaze / eye direction,gaze,social context,group composition,condition,experimental set-up,sup. machine learning,sig (acc
42,Gaze quality assisted automatic recognition of social contexts in collaborative Tetris,gaze saccade,eye motion,gaze,social context,group composition,condition,experimental set-up,sup. machine learning,sig (acc
42,Gaze quality assisted automatic recognition of social contexts in collaborative Tetris,gaze fixation,gaze / eye direction,gaze,social context,group composition,condition,experimental set-up,sup. machine learning,sig (acc
42,Gaze quality assisted automatic recognition of social contexts in collaborative Tetris,player actions,task-related,activity log,social context,group composition,condition,experimental set-up,sup. machine learning,sig (acc
42,Gaze quality assisted automatic recognition of social contexts in collaborative Tetris,zoid acceleration,task-related,activity log,social context,group composition,condition,experimental set-up,sup. machine learning,sig (acc
43,Acoustic-Prosodic Entrainment and Rapport in Collaborative Learning Dialogues,proximity,audio features,verbal,rapport level,interpersonal relationship / perception,process,human coding validated with self-reports,correlation,mixed (max r 0.842)
43,Acoustic-Prosodic Entrainment and Rapport in Collaborative Learning Dialogues,convergence,audio features,verbal,rapport level,interpersonal relationship / perception,process,human coding validated with self-reports,correlation,mixed (max r 0.741)
43,Acoustic-Prosodic Entrainment and Rapport in Collaborative Learning Dialogues,synchrony,audio features,verbal,rapport level,interpersonal relationship / perception,process,human coding validated with self-reports,correlation,mixed (max r 0.634)
44,Can Eye Help You?: Effects of Visualizing Eye Fixations on Remote Collaboration Scenarios for Physical Tasks,gaze location,gaze / eye direction,gaze,quality of remote collaboration,performance,product,seven-point scale questionnaire,wilcoxon signed ranks test,sig
44,Can Eye Help You?: Effects of Visualizing Eye Fixations on Remote Collaboration Scenarios for Physical Tasks,gaze location,gaze / eye direction,gaze,task completion time,performance,product,timer,wilcoxon signed ranks test,sig
46,Using Mobile Eye-Trackers to Unpack the Perceptual Benefits of a Tangible User Interface for Collaborative Learning,joint visual attention,gaze / eye direction,gaze,task performance,performance,product,calculation,correlation,sig (r = 0.59)
46,Using Mobile Eye-Trackers to Unpack the Perceptual Benefits of a Tangible User Interface for Collaborative Learning,joint visual attention,gaze / eye direction,gaze,learning gains,learning,product,pre post test,correlation,sig (r = 0.42)
47,Personality classification and behaviour interpretation: An approach based on feature categories,22 intra-personal features,facial expression,head,personality traits,group composition,condition,self-reported survey,regression,73.53%
47,Personality classification and behaviour interpretation: An approach based on feature categories,22 intra-personal features,facial expression,head,social impressions,interpersonal relationship / perception,process,questionnaire,regression,76.47%
47,Personality classification and behaviour interpretation: An approach based on feature categories,22 intra-personal features,facial expression,head,personality traits,group composition,condition,self-reported survey,regression,73.53%
47,Personality classification and behaviour interpretation: An approach based on feature categories,22 intra-personal features,facial expression,head,social impressions,interpersonal relationship / perception,process,questionnaire,regression,76.47%
47,Personality classification and behaviour interpretation: An approach based on feature categories,9 dyadic features,facial expression,head,personality traits,group composition,condition,self-reported survey,regression,60.54%
47,Personality classification and behaviour interpretation: An approach based on feature categories,9 dyadic features,facial expression,head,social impressions,interpersonal relationship / perception,process,questionnaire,regression,66.42%
47,Personality classification and behaviour interpretation: An approach based on feature categories,9 dyadic features,facial expression,head,personality traits,group composition,condition,self-reported survey,regression,60.54%
47,Personality classification and behaviour interpretation: An approach based on feature categories,9 dyadic features,facial expression,head,social impressions,interpersonal relationship / perception,process,questionnaire,regression,66.42%
47,Personality classification and behaviour interpretation: An approach based on feature categories,6 one vs all features,audio features,verbal,personality traits,group composition,condition,self-reported survey,regression,65.69%
47,Personality classification and behaviour interpretation: An approach based on feature categories,6 one vs all features,audio features,verbal,social impressions,interpersonal relationship / perception,process,questionnaire,regression,73.53%
47,Personality classification and behaviour interpretation: An approach based on feature categories,6 one vs all features,audio features,verbal,personality traits,group composition,condition,self-reported survey,regression,65.69%
47,Personality classification and behaviour interpretation: An approach based on feature categories,6 one vs all features,audio features,verbal,social impressions,interpersonal relationship / perception,process,questionnaire,regression,73.53%
48,Investigating Automatic Dominance Estimation in Groups From Visual Attention and Speaking Activity,audio energy features,audio features,verbal,visual dominance ratio,group composition,condition,manual annotation of videos,sup. machine learning,79.4%
48,Investigating Automatic Dominance Estimation in Groups From Visual Attention and Speaking Activity,visual focus of attention features,gaze / eye direction,gaze,visual dominance ratio,group composition,condition,manual annotation of videos,sup. machine learning,79.4%
49,High Accuracy Detection of Collaboration From Log Data and Superficial Speech Features,card movements,task-related,activity log,collaboration,coordination,product,manual annotation of videos,sup. machine learning,96%
49,High Accuracy Detection of Collaboration From Log Data and Superficial Speech Features,card movements,task-related,activity log,collaboration,communication,product,manual annotation of videos,sup. machine learning,96%
49,High Accuracy Detection of Collaboration From Log Data and Superficial Speech Features,card movements,task-related,activity log,cooperation,coordination,product,manual annotation of videos,sup. machine learning,96%
49,High Accuracy Detection of Collaboration From Log Data and Superficial Speech Features,card movements,task-related,activity log,cooperation,communication,product,manual annotation of videos,sup. machine learning,96%
49,High Accuracy Detection of Collaboration From Log Data and Superficial Speech Features,card movements,task-related,activity log,collaboration,coordination,product,manual annotation of videos,sup. machine learning,86%
49,High Accuracy Detection of Collaboration From Log Data and Superficial Speech Features,card movements,task-related,activity log,collaboration,communication,product,manual annotation of videos,sup. machine learning,86%
49,High Accuracy Detection of Collaboration From Log Data and Superficial Speech Features,card movements,task-related,activity log,assymetric contribution,coordination,product,manual annotation of videos,sup. machine learning,86%
49,High Accuracy Detection of Collaboration From Log Data and Superficial Speech Features,card movements,task-related,activity log,cooperation,coordination,product,manual annotation of videos,sup. machine learning,86%
49,High Accuracy Detection of Collaboration From Log Data and Superficial Speech Features,card movements,task-related,activity log,cooperation,communication,product,manual annotation of videos,sup. machine learning,86%
49,High Accuracy Detection of Collaboration From Log Data and Superficial Speech Features,scrolling,task-related,activity log,collaboration,coordination,product,manual annotation of videos,sup. machine learning,96%
49,High Accuracy Detection of Collaboration From Log Data and Superficial Speech Features,scrolling,task-related,activity log,collaboration,communication,product,manual annotation of videos,sup. machine learning,96%
49,High Accuracy Detection of Collaboration From Log Data and Superficial Speech Features,scrolling,task-related,activity log,cooperation,coordination,product,manual annotation of videos,sup. machine learning,96%
49,High Accuracy Detection of Collaboration From Log Data and Superficial Speech Features,scrolling,task-related,activity log,cooperation,communication,product,manual annotation of videos,sup. machine learning,96%
49,High Accuracy Detection of Collaboration From Log Data and Superficial Speech Features,scrolling,task-related,activity log,collaboration,coordination,product,manual annotation of videos,sup. machine learning,86%
49,High Accuracy Detection of Collaboration From Log Data and Superficial Speech Features,scrolling,task-related,activity log,collaboration,communication,product,manual annotation of videos,sup. machine learning,86%
49,High Accuracy Detection of Collaboration From Log Data and Superficial Speech Features,scrolling,task-related,activity log,assymetric contribution,coordination,product,manual annotation of videos,sup. machine learning,86%
49,High Accuracy Detection of Collaboration From Log Data and Superficial Speech Features,scrolling,task-related,activity log,cooperation,coordination,product,manual annotation of videos,sup. machine learning,86%
49,High Accuracy Detection of Collaboration From Log Data and Superficial Speech Features,scrolling,task-related,activity log,cooperation,communication,product,manual annotation of videos,sup. machine learning,86%
49,High Accuracy Detection of Collaboration From Log Data and Superficial Speech Features,zooming,task-related,activity log,collaboration,coordination,product,manual annotation of videos,sup. machine learning,96%
49,High Accuracy Detection of Collaboration From Log Data and Superficial Speech Features,zooming,task-related,activity log,collaboration,communication,product,manual annotation of videos,sup. machine learning,96%
49,High Accuracy Detection of Collaboration From Log Data and Superficial Speech Features,zooming,task-related,activity log,cooperation,coordination,product,manual annotation of videos,sup. machine learning,96%
49,High Accuracy Detection of Collaboration From Log Data and Superficial Speech Features,zooming,task-related,activity log,cooperation,communication,product,manual annotation of videos,sup. machine learning,96%
49,High Accuracy Detection of Collaboration From Log Data and Superficial Speech Features,zooming,task-related,activity log,collaboration,coordination,product,manual annotation of videos,sup. machine learning,86%
49,High Accuracy Detection of Collaboration From Log Data and Superficial Speech Features,zooming,task-related,activity log,collaboration,communication,product,manual annotation of videos,sup. machine learning,86%
49,High Accuracy Detection of Collaboration From Log Data and Superficial Speech Features,zooming,task-related,activity log,assymetric contribution,coordination,product,manual annotation of videos,sup. machine learning,86%
49,High Accuracy Detection of Collaboration From Log Data and Superficial Speech Features,zooming,task-related,activity log,cooperation,coordination,product,manual annotation of videos,sup. machine learning,86%
49,High Accuracy Detection of Collaboration From Log Data and Superficial Speech Features,zooming,task-related,activity log,cooperation,communication,product,manual annotation of videos,sup. machine learning,86%
49,High Accuracy Detection of Collaboration From Log Data and Superficial Speech Features,1582 audio features (from emobase),audio features,verbal,collaboration,coordination,product,manual annotation of videos,sup. machine learning,96%
49,High Accuracy Detection of Collaboration From Log Data and Superficial Speech Features,1582 audio features (from emobase),audio features,verbal,collaboration,communication,product,manual annotation of videos,sup. machine learning,96%
49,High Accuracy Detection of Collaboration From Log Data and Superficial Speech Features,1582 audio features (from emobase),audio features,verbal,cooperation,coordination,product,manual annotation of videos,sup. machine learning,96%
49,High Accuracy Detection of Collaboration From Log Data and Superficial Speech Features,1582 audio features (from emobase),audio features,verbal,cooperation,communication,product,manual annotation of videos,sup. machine learning,96%
49,High Accuracy Detection of Collaboration From Log Data and Superficial Speech Features,1582 audio features (from emobase),audio features,verbal,collaboration,coordination,product,manual annotation of videos,sup. machine learning,86%
49,High Accuracy Detection of Collaboration From Log Data and Superficial Speech Features,1582 audio features (from emobase),audio features,verbal,collaboration,communication,product,manual annotation of videos,sup. machine learning,86%
49,High Accuracy Detection of Collaboration From Log Data and Superficial Speech Features,1582 audio features (from emobase),audio features,verbal,assymetric contribution,coordination,product,manual annotation of videos,sup. machine learning,86%
49,High Accuracy Detection of Collaboration From Log Data and Superficial Speech Features,1582 audio features (from emobase),audio features,verbal,cooperation,coordination,product,manual annotation of videos,sup. machine learning,86%
49,High Accuracy Detection of Collaboration From Log Data and Superficial Speech Features,1582 audio features (from emobase),audio features,verbal,cooperation,communication,product,manual annotation of videos,sup. machine learning,86%
50,Using Interlocutor-Modulated Attention BLSTM to Predict Personality Traits in Small Group Interaction,speech utterances,verbal content,verbal,personality traits,group composition,condition,self-reported survey + perceived interaction scores,sup. machine learning,87.9%
52,Dynamics of Visual Attention in Multiparty Collaborative Problem Solving using Multidimensional Recurrence Quantification Analysis,cross-recurrence quantification analysis (crqa),gaze / eye direction,gaze,construction of shared knowledge,coordination,product,manual annotation of videos,regression,sig
52,Dynamics of Visual Attention in Multiparty Collaborative Problem Solving using Multidimensional Recurrence Quantification Analysis,cross-recurrence quantification analysis (crqa),gaze / eye direction,gaze,task score,performance,product,"expert coding of task, post-test score",regression,nosig
52,Dynamics of Visual Attention in Multiparty Collaborative Problem Solving using Multidimensional Recurrence Quantification Analysis,cross-recurrence quantification analysis (crqa),gaze / eye direction,gaze,team performance,performance,product,self-reported survey,regression,sig
52,Dynamics of Visual Attention in Multiparty Collaborative Problem Solving using Multidimensional Recurrence Quantification Analysis,cross-recurrence quantification analysis (crqa),gaze / eye direction,gaze,task score,performance,product,"expert coding of task, post-test score",regression,nosig
52,Dynamics of Visual Attention in Multiparty Collaborative Problem Solving using Multidimensional Recurrence Quantification Analysis,cross-recurrence quantification analysis (crqa),gaze / eye direction,gaze,team performance,performance,product,self-reported survey,regression,nosig
52,Dynamics of Visual Attention in Multiparty Collaborative Problem Solving using Multidimensional Recurrence Quantification Analysis,multidimensional recurrence quantification analysis (mdrqa),gaze / eye direction,gaze,negotiation,coordination,product,manual annotation of videos,regression,sig
52,Dynamics of Visual Attention in Multiparty Collaborative Problem Solving using Multidimensional Recurrence Quantification Analysis,multidimensional recurrence quantification analysis (mdrqa),gaze / eye direction,gaze,coordination,coordination,product,manual annotation of videos,regression,sig
53,Does Seeing One Another’s Gaze Affect Group Dialogue?,simple linguistic features,verbal content,verbal,learning,learning,product,pre-post test,sup. machine learning,75%
53,Does Seeing One Another’s Gaze Affect Group Dialogue?,convergence (of linguistic styles),verbal content,verbal,learning,learning,product,pre-post test,correlation,nosig
53,Does Seeing One Another’s Gaze Affect Group Dialogue?,convergence (of linguistic styles),verbal content,verbal,collaboration,coordination,product,coding scheme,correlation,nosig
53,Does Seeing One Another’s Gaze Affect Group Dialogue?,convergence (of linguistic styles),verbal content,verbal,collaboration,communication,product,coding scheme,correlation,nosig
53,Does Seeing One Another’s Gaze Affect Group Dialogue?,convergence (of linguistic styles),verbal content,verbal,joint visual attention,coordination,product,calculation,correlation,nosig
53,Does Seeing One Another’s Gaze Affect Group Dialogue?,convergence (of linguistic styles),verbal content,verbal,joint visual attention,communication,product,calculation,correlation,nosig
53,Does Seeing One Another’s Gaze Affect Group Dialogue?,coherence,verbal content,verbal,learning,learning,product,pre-post test,correlation,sig
53,Does Seeing One Another’s Gaze Affect Group Dialogue?,coherence,verbal content,verbal,joint visual attention,coordination,product,calculation,anova,sig
53,Does Seeing One Another’s Gaze Affect Group Dialogue?,coherence,verbal content,verbal,joint visual attention,communication,product,calculation,anova,sig
54,"Emergent leaders through looking and speaking: from
audio-visual data to multimodal recognition","speaking activity (speaking length, speaking turns, speaking interruptions, average speaking turn duration)",speech participation,verbal,perception of leadership and dominance from external observers,interpersonal relationship / perception,process,questionnaire,sup. machine learning,50%
54,"Emergent leaders through looking and speaking: from
audio-visual data to multimodal recognition","speaking activity (speaking length, speaking turns, speaking interruptions, average speaking turn duration)",speech participation,verbal,ranked dominance,interpersonal relationship / perception,process,questionnaire,sup. machine learning,59.1%
54,"Emergent leaders through looking and speaking: from
audio-visual data to multimodal recognition","audio-visual (looking while speaking, looking while listening, being looked while speaking, center of attention while speaking, visual dominance ratio)",gaze / eye direction,gaze,perception of leadership and dominance from external observers,interpersonal relationship / perception,process,questionnaire,sup. machine learning,50%
54,"Emergent leaders through looking and speaking: from
audio-visual data to multimodal recognition","audio-visual (looking while speaking, looking while listening, being looked while speaking, center of attention while speaking, visual dominance ratio)",gaze / eye direction,gaze,ranked dominance,interpersonal relationship / perception,process,questionnaire,sup. machine learning,59.1%
54,"Emergent leaders through looking and speaking: from
audio-visual data to multimodal recognition","audio-visual (looking while speaking, looking while listening, being looked while speaking, center of attention while speaking, visual dominance ratio)",gaze / eye direction,gaze,perception of leadership and dominance from external observers,interpersonal relationship / perception,process,questionnaire,sup. machine learning,50%
54,"Emergent leaders through looking and speaking: from
audio-visual data to multimodal recognition","audio-visual (looking while speaking, looking while listening, being looked while speaking, center of attention while speaking, visual dominance ratio)",gaze / eye direction,gaze,ranked dominance,interpersonal relationship / perception,process,questionnaire,sup. machine learning,59.1%
56,Predicting the Quality of Collaborative Problem Solving Through Linguistic Analysis of Discourse,35 coh-metrix indices,audio features,verbal,learning,learning,product,pre-post test,correlation,sig
56,Predicting the Quality of Collaborative Problem Solving Through Linguistic Analysis of Discourse,35 coh-metrix indices,audio features,verbal,collaboration,coordination,product,meier spada rummel coding scheme,correlation,sig
56,Predicting the Quality of Collaborative Problem Solving Through Linguistic Analysis of Discourse,35 coh-metrix indices,audio features,verbal,collaboration,communication,product,meier spada rummel coding scheme,correlation,sig
56,Predicting the Quality of Collaborative Problem Solving Through Linguistic Analysis of Discourse,35 coh-metrix indices,audio features,verbal,coh-metrix indices,communication,product,computational measures of transcripts,correlation,sig
56,Predicting the Quality of Collaborative Problem Solving Through Linguistic Analysis of Discourse,physical synchrony,gross body motion,body,coh-metrix indices,communication,product,computational measures of transcripts,correlation,sig
56,Predicting the Quality of Collaborative Problem Solving Through Linguistic Analysis of Discourse,physical synchrony,gross body motion,body,collaboration,coordination,product,meier spada rummel coding scheme,sup. machine learning,84%
56,Predicting the Quality of Collaborative Problem Solving Through Linguistic Analysis of Discourse,physical synchrony,gross body motion,body,collaboration,communication,product,meier spada rummel coding scheme,sup. machine learning,84%
56,Predicting the Quality of Collaborative Problem Solving Through Linguistic Analysis of Discourse,physiological synchrony,electrodermal activity,physiological,coh-metrix indices,communication,product,computational measures of transcripts,correlation,sig
56,Predicting the Quality of Collaborative Problem Solving Through Linguistic Analysis of Discourse,joint visual attention,gaze / eye direction,gaze,coh-metrix indices,communication,product,computational measures of transcripts,correlation,sig
57,Shared Experiences of Technology and Trust: An Experimental Study of Physiological Compliance Between Active and Passive Users in Technology-Mediated Collaborative Encounters,sm - eda,electrodermal activity,physiological,performance in task,performance,product,multiattribute task battery program,lme,non-sig
57,Shared Experiences of Technology and Trust: An Experimental Study of Physiological Compliance Between Active and Passive Users in Technology-Mediated Collaborative Encounters,sm - eda,electrodermal activity,physiological,self-rated workload,individual cognitive processes,process,questionnaire (nasa task load index),lme,non-sig
57,Shared Experiences of Technology and Trust: An Experimental Study of Physiological Compliance Between Active and Passive Users in Technology-Mediated Collaborative Encounters,idm - eda,electrodermal activity,physiological,performance in task,performance,product,multiattribute task battery program,lme,non-sig
57,Shared Experiences of Technology and Trust: An Experimental Study of Physiological Compliance Between Active and Passive Users in Technology-Mediated Collaborative Encounters,idm - eda,electrodermal activity,physiological,self-rated workload,individual cognitive processes,process,questionnaire (nasa task load index),lme,non-sig
57,Shared Experiences of Technology and Trust: An Experimental Study of Physiological Compliance Between Active and Passive Users in Technology-Mediated Collaborative Encounters,da - eda,electrodermal activity,physiological,performance in task,performance,product,multiattribute task battery program,lme,sig
57,Shared Experiences of Technology and Trust: An Experimental Study of Physiological Compliance Between Active and Passive Users in Technology-Mediated Collaborative Encounters,da - eda,electrodermal activity,physiological,self-rated workload,individual cognitive processes,process,questionnaire (nasa task load index),lme,non-sig
57,Shared Experiences of Technology and Trust: An Experimental Study of Physiological Compliance Between Active and Passive Users in Technology-Mediated Collaborative Encounters,cc - eda,electrodermal activity,physiological,performance in task,performance,product,multiattribute task battery program,lme,non-sig
57,Shared Experiences of Technology and Trust: An Experimental Study of Physiological Compliance Between Active and Passive Users in Technology-Mediated Collaborative Encounters,cc - eda,electrodermal activity,physiological,self-rated workload,individual cognitive processes,process,questionnaire (nasa task load index),lme,non-sig
57,Shared Experiences of Technology and Trust: An Experimental Study of Physiological Compliance Between Active and Passive Users in Technology-Mediated Collaborative Encounters,wc - eda,electrodermal activity,physiological,performance in task,performance,product,multiattribute task battery program,lme,non-sig
57,Shared Experiences of Technology and Trust: An Experimental Study of Physiological Compliance Between Active and Passive Users in Technology-Mediated Collaborative Encounters,wc - eda,electrodermal activity,physiological,self-rated workload,individual cognitive processes,process,questionnaire (nasa task load index),lme,non-sig
57,Shared Experiences of Technology and Trust: An Experimental Study of Physiological Compliance Between Active and Passive Users in Technology-Mediated Collaborative Encounters,sm - hr,heart rate,physiological,performance in task,performance,product,multiattribute task battery program,lme,non-sig
57,Shared Experiences of Technology and Trust: An Experimental Study of Physiological Compliance Between Active and Passive Users in Technology-Mediated Collaborative Encounters,sm - hr,heart rate,physiological,self-rated workload,individual cognitive processes,process,questionnaire (nasa task load index),lme,non-sig
57,Shared Experiences of Technology and Trust: An Experimental Study of Physiological Compliance Between Active and Passive Users in Technology-Mediated Collaborative Encounters,idm - hr,heart rate,physiological,performance in task,performance,product,multiattribute task battery program,lme,non-sig
57,Shared Experiences of Technology and Trust: An Experimental Study of Physiological Compliance Between Active and Passive Users in Technology-Mediated Collaborative Encounters,idm - hr,heart rate,physiological,self-rated workload,individual cognitive processes,process,questionnaire (nasa task load index),lme,non-sig
57,Shared Experiences of Technology and Trust: An Experimental Study of Physiological Compliance Between Active and Passive Users in Technology-Mediated Collaborative Encounters,da - hr,heart rate,physiological,performance in task,performance,product,multiattribute task battery program,lme,non-sig
57,Shared Experiences of Technology and Trust: An Experimental Study of Physiological Compliance Between Active and Passive Users in Technology-Mediated Collaborative Encounters,da - hr,heart rate,physiological,performance in task,performance,product,multiattribute task battery program,lme,non-sig
57,Shared Experiences of Technology and Trust: An Experimental Study of Physiological Compliance Between Active and Passive Users in Technology-Mediated Collaborative Encounters,cc - hr,heart rate,physiological,performance in task,performance,product,multiattribute task battery program,lme,non-sig
57,Shared Experiences of Technology and Trust: An Experimental Study of Physiological Compliance Between Active and Passive Users in Technology-Mediated Collaborative Encounters,cc - hr,heart rate,physiological,performance in task,performance,product,multiattribute task battery program,lme,non-sig
57,Shared Experiences of Technology and Trust: An Experimental Study of Physiological Compliance Between Active and Passive Users in Technology-Mediated Collaborative Encounters,wc - hr low frequency,heart rate,physiological,performance in task,performance,product,multiattribute task battery program,lme,non-sig
57,Shared Experiences of Technology and Trust: An Experimental Study of Physiological Compliance Between Active and Passive Users in Technology-Mediated Collaborative Encounters,wc - hr low frequency,heart rate,physiological,performance in task,performance,product,multiattribute task battery program,lme,non-sig
57,Shared Experiences of Technology and Trust: An Experimental Study of Physiological Compliance Between Active and Passive Users in Technology-Mediated Collaborative Encounters,wc - hr high frequency,heart rate,physiological,performance in task,performance,product,multiattribute task battery program,lme,non-sig
57,Shared Experiences of Technology and Trust: An Experimental Study of Physiological Compliance Between Active and Passive Users in Technology-Mediated Collaborative Encounters,wc - hr high frequency,heart rate,physiological,performance in task,performance,product,multiattribute task battery program,lme,non-sig
58,Using Multimodal Learning Analytics to Identify Aspects of Collaboration in Project-Based Learning,count of faces looking at screen,gaze / eye direction,gaze,synchronisation,coordination,product,researcher codes,regression,sig
58,Using Multimodal Learning Analytics to Identify Aspects of Collaboration in Project-Based Learning,distance between hands,hand motion,body,individual accountability,coordination,product,researcher codes,regression,sig
58,Using Multimodal Learning Analytics to Identify Aspects of Collaboration in Project-Based Learning,distance between hands,hand motion,body,individual accountability,individual cognitive processes,process,researcher codes,regression,sig
58,Using Multimodal Learning Analytics to Identify Aspects of Collaboration in Project-Based Learning,distance between hands,hand motion,body,synchronisation,coordination,product,researcher codes,regression,sig
58,Using Multimodal Learning Analytics to Identify Aspects of Collaboration in Project-Based Learning,distance between hands,hand motion,body,synchronisation,coordination,product,researcher codes,regression,sig
58,Using Multimodal Learning Analytics to Identify Aspects of Collaboration in Project-Based Learning,distance between hands,hand motion,body,physical engagement,individual cognitive processes,process,researcher codes,regression,sig
60,Modeling Collaboration Patterns on an Interactive Tabletop in a Classroom Setting,touch patterns,touch,activity log,social regulation,coordination,product,rogat and linnenbrink-garcia’s framework,calculation,84.2%
60,Modeling Collaboration Patterns on an Interactive Tabletop in a Classroom Setting,touch patterns,touch,activity log,social regulation,individual cognitive processes,process,rogat and linnenbrink-garcia’s framework,calculation,84.2%
61,Toward Using Multi-Modal Learning Analytics to Support and Measure Collaboration in Co-Located Dyads,total movement across upper body joints and body parts,gross body motion,body,collaboration quality,coordination,product,experimenter code,correlation,sig
61,Toward Using Multi-Modal Learning Analytics to Support and Measure Collaboration in Co-Located Dyads,total movement across upper body joints and body parts,gross body motion,body,collaboration quality,communication,product,experimenter code,correlation,sig
61,Toward Using Multi-Modal Learning Analytics to Support and Measure Collaboration in Co-Located Dyads,talking time,speech participation,verbal,collaboration quality,coordination,product,experimenter code,correlation,sig
61,Toward Using Multi-Modal Learning Analytics to Support and Measure Collaboration in Co-Located Dyads,talking time,speech participation,verbal,collaboration quality,communication,product,experimenter code,correlation,sig
62,An Alternate Statistical Lens to Look at Collaboration Data: Extreme Value Theory,evt of spatial entropy,gaze / eye direction,gaze,collaboration outcome,performance,product,ns,regression,sig
62,An Alternate Statistical Lens to Look at Collaboration Data: Extreme Value Theory,evt of spatial entropy,gaze / eye direction,gaze,post-test score,learning,product,pre-post test,regression,sig
63,Going beyond what is visible: What multichannel data can reveal about interaction in the context of collaborative learning?,facial expression,facial expression,head,type of working activity,coordination,product,researcher codes,calculation,ns
63,Going beyond what is visible: What multichannel data can reveal about interaction in the context of collaborative learning?,facial expression,facial expression,head,type of interaction,communication,product,researcher codes,anova,sig
63,Going beyond what is visible: What multichannel data can reveal about interaction in the context of collaborative learning?,physiological simultaneous arousal,electrodermal activity,physiological,type of working activity,coordination,product,researcher codes,calculation,ns
63,Going beyond what is visible: What multichannel data can reveal about interaction in the context of collaborative learning?,physiological simultaneous arousal,electrodermal activity,physiological,type of interaction,communication,product,researcher codes,calculation,ns
64,Task-independent Multimodal Prediction of Group Performance Based on Product Dimensions,speaking turn features (x4),speech participation,verbal,"quality of produced outcome (6 dimensions including length, originality, quality of presentation, etc)",performance,product,researcher codes,unsup. machine learning,sig
64,Task-independent Multimodal Prediction of Group Performance Based on Product Dimensions,acoustic features (x13),audio features,verbal,"quality of produced outcome (6 dimensions including length, originality, quality of presentation, etc)",performance,product,researcher codes,unsup. machine learning,sig
64,Task-independent Multimodal Prediction of Group Performance Based on Product Dimensions,head motion features (x5),head motion,head,"quality of produced outcome (6 dimensions including length, originality, quality of presentation, etc)",performance,product,researcher codes,unsup. machine learning,sig
64,Task-independent Multimodal Prediction of Group Performance Based on Product Dimensions,linguistic features (x300+),verbal content,verbal,"quality of produced outcome (6 dimensions including length, originality, quality of presentation, etc)",performance,product,researcher codes,unsup. machine learning,sig
66,Toward Collaboration Sensing,network features (+20),gaze / eye direction,gaze,collaboration quality,coordination,product,"meier, spada and rummel coding scheme",sup. machine learning,85-100%
66,Toward Collaboration Sensing,network features (+20),gaze / eye direction,gaze,collaboration quality,communication,product,"meier, spada and rummel coding scheme",sup. machine learning,85-100%
66,Toward Collaboration Sensing,network features (+20),gaze / eye direction,gaze,collaboration quality,coordination,product,"meier, spada and rummel coding scheme",correlation,sig
66,Toward Collaboration Sensing,network features (+20),gaze / eye direction,gaze,collaboration quality,communication,product,"meier, spada and rummel coding scheme",correlation,sig
69,Supervised machine learning in multimodal learning analytics for estimating success in project-based learning,number of faces looking at screen,head motion,head,artefact quality,performance,product,researcher codes,sup. machine learning,24%
69,Supervised machine learning in multimodal learning analytics for estimating success in project-based learning,mean distance between learners,location,body,artefact quality,performance,product,researcher codes,sup. machine learning,24%
69,Supervised machine learning in multimodal learning analytics for estimating success in project-based learning,mean distance between hands,hand motion,body,artefact quality,performance,product,researcher codes,sup. machine learning,24%
69,Supervised machine learning in multimodal learning analytics for estimating success in project-based learning,mean hand movement speed,hand motion,body,artefact quality,performance,product,researcher codes,sup. machine learning,24%
69,Supervised machine learning in multimodal learning analytics for estimating success in project-based learning,mean audio level,audio features,verbal,artefact quality,performance,product,researcher codes,sup. machine learning,24%
69,Supervised machine learning in multimodal learning analytics for estimating success in project-based learning,arduino measure of complexity,task-related,activity log,artefact quality,performance,product,researcher codes,sup. machine learning,24%
69,Supervised machine learning in multimodal learning analytics for estimating success in project-based learning,arduino active hardware blocks,task-related,activity log,artefact quality,performance,product,researcher codes,sup. machine learning,24%
69,Supervised machine learning in multimodal learning analytics for estimating success in project-based learning,arduino active software blocks,task-related,activity log,artefact quality,performance,product,researcher codes,sup. machine learning,24%
69,Supervised machine learning in multimodal learning analytics for estimating success in project-based learning,arduino active blocks,task-related,activity log,artefact quality,performance,product,researcher codes,sup. machine learning,24%
69,Supervised machine learning in multimodal learning analytics for estimating success in project-based learning,student work phases,task-related,activity log,artefact quality,performance,product,researcher codes,sup. machine learning,24%
70,Multimodal prediction of expertise and leadership in learning groups,pause duration,speech participation,verbal,leadership,group composition,condition,assigned,t-test,sig
70,Multimodal prediction of expertise and leadership in learning groups,pause duration,speech participation,verbal,expertise,group composition,condition,problem solving performance,t-test,non-sig
70,Multimodal prediction of expertise and leadership in learning groups,articulation rate,audio features,verbal,leadership,group composition,condition,assigned,t-test,sig
70,Multimodal prediction of expertise and leadership in learning groups,articulation rate,audio features,verbal,expertise,group composition,condition,problem solving performance,t-test,non-sig
70,Multimodal prediction of expertise and leadership in learning groups,peak slope,audio features,verbal,leadership,group composition,condition,assigned,t-test,sig
70,Multimodal prediction of expertise and leadership in learning groups,peak slope,audio features,verbal,expertise,group composition,condition,problem solving performance,t-test,sig
70,Multimodal prediction of expertise and leadership in learning groups,spectral stationarity,audio features,verbal,leadership,group composition,condition,assigned,t-test,sig
70,Multimodal prediction of expertise and leadership in learning groups,spectral stationarity,audio features,verbal,expertise,group composition,condition,problem solving performance,t-test,non-sig
71,Estimation of success in collaborative learning based on multimodal learning analytics features,faces looking at screen (fls),head motion,head,collaborative problem solving,coordination,product,researcher codes,sup. machine learning,sig
71,Estimation of success in collaborative learning based on multimodal learning analytics features,faces looking at screen (fls),head motion,head,collaborative problem solving,performance,product,researcher codes,sup. machine learning,sig
71,Estimation of success in collaborative learning based on multimodal learning analytics features,distance between learners (dbl),location,body,collaborative problem solving,coordination,product,researcher codes,sup. machine learning,sig
71,Estimation of success in collaborative learning based on multimodal learning analytics features,distance between learners (dbl),location,body,collaborative problem solving,performance,product,researcher codes,sup. machine learning,sig
71,Estimation of success in collaborative learning based on multimodal learning analytics features,audio level (aud),audio features,verbal,collaborative problem solving,coordination,product,researcher codes,sup. machine learning,sig
71,Estimation of success in collaborative learning based on multimodal learning analytics features,audio level (aud),audio features,verbal,collaborative problem solving,performance,product,researcher codes,sup. machine learning,sig
73,Unpacking Collaborative Learning Processes during Hands-on Activities using Mobile Eye-Trackers,joint-visual attention,gaze / eye direction,gaze,collaboration quality,coordination,product,researcher codes,correlation,sig
73,Unpacking Collaborative Learning Processes during Hands-on Activities using Mobile Eye-Trackers,joint-visual attention,gaze / eye direction,gaze,collaboration quality,communication,product,researcher codes,correlation,sig
73,Unpacking Collaborative Learning Processes during Hands-on Activities using Mobile Eye-Trackers,joint-visual attention,gaze / eye direction,gaze,collaboration quality,coordination,product,researcher codes,correlation,sig
73,Unpacking Collaborative Learning Processes during Hands-on Activities using Mobile Eye-Trackers,joint-visual attention,gaze / eye direction,gaze,collaboration quality,communication,product,researcher codes,correlation,sig
74,3D Tangibles Facilitate Joint Visual Attention in Dyads,joint visual attention,gaze / eye direction,gaze,collaboration quality,coordination,product,researcher codes,correlation,sig
74,3D Tangibles Facilitate Joint Visual Attention in Dyads,joint visual attention,gaze / eye direction,gaze,collaboration quality,communication,product,researcher codes,correlation,sig
74,3D Tangibles Facilitate Joint Visual Attention in Dyads,joint visual attention,gaze / eye direction,gaze,student performance,performance,product,correctness,regression,sig
74,3D Tangibles Facilitate Joint Visual Attention in Dyads,joint visual attention,gaze / eye direction,gaze,learning gains,learning,product,pre-post test,correlation,sig
75,Exploring Collaboration Using Motion Sensors and Multi-Modal Learning Analytics,joint movement,gross body motion,body,task performance,performance,product,researcher codes,correlation,sig
75,Exploring Collaboration Using Motion Sensors and Multi-Modal Learning Analytics,joint movement,gross body motion,body,collaboration,coordination,product,researcher codes,correlation,non-sig
75,Exploring Collaboration Using Motion Sensors and Multi-Modal Learning Analytics,joint movement,gross body motion,body,collaboration,communication,product,researcher codes,correlation,non-sig
75,Exploring Collaboration Using Motion Sensors and Multi-Modal Learning Analytics,joint angle,gross body motion,body,task performance,performance,product,researcher codes,correlation,sig / mixed
75,Exploring Collaboration Using Motion Sensors and Multi-Modal Learning Analytics,dyad proximity,location,body,collaboration,coordination,product,researcher codes,correlation,non-sig
75,Exploring Collaboration Using Motion Sensors and Multi-Modal Learning Analytics,dyad proximity,location,body,collaboration,communication,product,researcher codes,correlation,non-sig
75,Exploring Collaboration Using Motion Sensors and Multi-Modal Learning Analytics,dyad proximity,location,body,task performance,performance,product,researcher codes,correlation,sig
78,Real-time mutual gaze perception,joint visual attention,gaze / eye direction,gaze,learning gains,learning,product,pre/post test,mediation,sig
78,Real-time mutual gaze perception,cognitive load (from pupil size),eye physiology,gaze,learning gains,learning,product,pre/post test,mediation,nonsig
80,Investigating collaborative learning success with physiological coupling indices based on electrodermal activity,"signal matching (sm),",electrodermal activity,physiological,collaborative will,interpersonal relationship / perception,process,self report,regression,nonsig
80,Investigating collaborative learning success with physiological coupling indices based on electrodermal activity,"signal matching (sm),",electrodermal activity,physiological,collaborative learning product,performance,product,researcher coded,regression,nonsig
80,Investigating collaborative learning success with physiological coupling indices based on electrodermal activity,"signal matching (sm),",electrodermal activity,physiological,dual learning gains,learning,product,researcher coded,regression,nonsig
80,Investigating collaborative learning success with physiological coupling indices based on electrodermal activity,"instantaneous derivative matching (idm),",electrodermal activity,physiological,collaborative will,interpersonal relationship / perception,process,self report,regression,sig
80,Investigating collaborative learning success with physiological coupling indices based on electrodermal activity,"instantaneous derivative matching (idm),",electrodermal activity,physiological,collaborative learning product,performance,product,researcher coded,regression,sig
80,Investigating collaborative learning success with physiological coupling indices based on electrodermal activity,"instantaneous derivative matching (idm),",electrodermal activity,physiological,dual learning gains,learning,product,researcher coded,regression,nonsig
80,Investigating collaborative learning success with physiological coupling indices based on electrodermal activity,"directional agreement (da),",electrodermal activity,physiological,collaborative will,interpersonal relationship / perception,process,self report,regression,nonsig
80,Investigating collaborative learning success with physiological coupling indices based on electrodermal activity,"directional agreement (da),",electrodermal activity,physiological,collaborative learning product,performance,product,researcher coded,regression,nonsig
80,Investigating collaborative learning success with physiological coupling indices based on electrodermal activity,"directional agreement (da),",electrodermal activity,physiological,dual learning gains,learning,product,researcher coded,regression,sig
80,Investigating collaborative learning success with physiological coupling indices based on electrodermal activity,pearson’s correlation coefficient (pcc),electrodermal activity,physiological,collaborative will,interpersonal relationship / perception,process,self report,regression,nonsig
80,Investigating collaborative learning success with physiological coupling indices based on electrodermal activity,pearson’s correlation coefficient (pcc),electrodermal activity,physiological,collaborative learning product,performance,product,researcher coded,regression,nonsig
80,Investigating collaborative learning success with physiological coupling indices based on electrodermal activity,pearson’s correlation coefficient (pcc),electrodermal activity,physiological,dual learning gains,learning,product,researcher coded,regression,nonsig
80,Investigating collaborative learning success with physiological coupling indices based on electrodermal activity,fisher’s z-transform (fzt) of the pcc,electrodermal activity,physiological,collaborative will,interpersonal relationship / perception,process,self report,regression,nonsig
80,Investigating collaborative learning success with physiological coupling indices based on electrodermal activity,fisher’s z-transform (fzt) of the pcc,electrodermal activity,physiological,collaborative learning product,performance,product,researcher coded,regression,nonsig
80,Investigating collaborative learning success with physiological coupling indices based on electrodermal activity,fisher’s z-transform (fzt) of the pcc,electrodermal activity,physiological,dual learning gains,learning,product,researcher coded,regression,nonsig
82,Multimodal Analysis of Vocal Collaborative Search:A Public Corpus and Results,linguistic features from transcript (65 features),verbal content,verbal,perception of peer helpfulness,interpersonal relationship / perception,process,questionnaire,correlation,sig
82,Multimodal Analysis of Vocal Collaborative Search:A Public Corpus and Results,linguistic features from transcript (65 features),verbal content,verbal,perception of peer understanding,interpersonal relationship / perception,process,questionnaire,correlation,sig
82,Multimodal Analysis of Vocal Collaborative Search:A Public Corpus and Results,linguistic features from transcript (65 features),verbal content,verbal,perception of peer clarity,interpersonal relationship / perception,process,questionnaire,correlation,sig
82,Multimodal Analysis of Vocal Collaborative Search:A Public Corpus and Results,linguistic features from transcript (65 features),verbal content,verbal,perception of peer helpfulness,interpersonal relationship / perception,process,questionnaire,correlation,nonsig
82,Multimodal Analysis of Vocal Collaborative Search:A Public Corpus and Results,linguistic features from transcript (65 features),verbal content,verbal,perception of peer understanding,interpersonal relationship / perception,process,questionnaire,correlation,nonsig
82,Multimodal Analysis of Vocal Collaborative Search:A Public Corpus and Results,linguistic features from transcript (65 features),verbal content,verbal,perception of peer clarity,interpersonal relationship / perception,process,questionnaire,correlation,nonsig
82,Multimodal Analysis of Vocal Collaborative Search:A Public Corpus and Results,voice features (4 features),audio features,verbal,perception of peer helpfulness,interpersonal relationship / perception,process,questionnaire,correlation,sig
82,Multimodal Analysis of Vocal Collaborative Search:A Public Corpus and Results,voice features (4 features),audio features,verbal,perception of peer understanding,interpersonal relationship / perception,process,questionnaire,correlation,sig
82,Multimodal Analysis of Vocal Collaborative Search:A Public Corpus and Results,voice features (4 features),audio features,verbal,perception of peer clarity,interpersonal relationship / perception,process,questionnaire,correlation,sig
82,Multimodal Analysis of Vocal Collaborative Search:A Public Corpus and Results,voice features (4 features),audio features,verbal,perception of peer helpfulness,interpersonal relationship / perception,process,questionnaire,correlation,nonsig
82,Multimodal Analysis of Vocal Collaborative Search:A Public Corpus and Results,voice features (4 features),audio features,verbal,perception of peer understanding,interpersonal relationship / perception,process,questionnaire,correlation,nonsig
82,Multimodal Analysis of Vocal Collaborative Search:A Public Corpus and Results,voice features (4 features),audio features,verbal,perception of peer clarity,interpersonal relationship / perception,process,questionnaire,correlation,nonsig
82,Multimodal Analysis of Vocal Collaborative Search:A Public Corpus and Results,facial expression features (60 features),facial expression,head,perception of peer helpfulness,interpersonal relationship / perception,process,questionnaire,correlation,sig
82,Multimodal Analysis of Vocal Collaborative Search:A Public Corpus and Results,facial expression features (60 features),facial expression,head,perception of peer understanding,interpersonal relationship / perception,process,questionnaire,correlation,sig
82,Multimodal Analysis of Vocal Collaborative Search:A Public Corpus and Results,facial expression features (60 features),facial expression,head,perception of peer clarity,interpersonal relationship / perception,process,questionnaire,correlation,sig
82,Multimodal Analysis of Vocal Collaborative Search:A Public Corpus and Results,facial expression features (60 features),facial expression,head,perception of peer helpfulness,interpersonal relationship / perception,process,questionnaire,correlation,sig
82,Multimodal Analysis of Vocal Collaborative Search:A Public Corpus and Results,facial expression features (60 features),facial expression,head,perception of peer understanding,interpersonal relationship / perception,process,questionnaire,correlation,sig
82,Multimodal Analysis of Vocal Collaborative Search:A Public Corpus and Results,facial expression features (60 features),facial expression,head,perception of peer clarity,interpersonal relationship / perception,process,questionnaire,correlation,sig
83,Focused or Stuck Together: Multimodal Patterns Reveal Triads’ Performance in Collaborative Problem Solving,type of activity done in task,task-related,activity log,task success score,performance,product,researcher coded,correlation,sig
83,Focused or Stuck Together: Multimodal Patterns Reveal Triads’ Performance in Collaborative Problem Solving,type of activity done in task,task-related,activity log,subjective perception of collaboration,interpersonal relationship / perception,process,self report,correlation,sig
83,Focused or Stuck Together: Multimodal Patterns Reveal Triads’ Performance in Collaborative Problem Solving,amount of face and body movement,gross body motion,body,task success score,performance,product,researcher coded,correlation,sig
83,Focused or Stuck Together: Multimodal Patterns Reveal Triads’ Performance in Collaborative Problem Solving,amount of face and body movement,gross body motion,body,subjective perception of collaboration,interpersonal relationship / perception,process,self report,correlation,sig
83,Focused or Stuck Together: Multimodal Patterns Reveal Triads’ Performance in Collaborative Problem Solving,target for discussion partner,task-related,activity log,task success score,performance,product,researcher coded,correlation,sig
83,Focused or Stuck Together: Multimodal Patterns Reveal Triads’ Performance in Collaborative Problem Solving,target for discussion partner,task-related,activity log,subjective perception of collaboration,interpersonal relationship / perception,process,self report,correlation,sig
84,"Using Physiological Synchrony as an Indicator of Collaboration Quality, Task Performance and Learning","signal matching (sm),",mixed,physiological,learning gains,learning,product,pre/post tests,correlation,nonsig
84,"Using Physiological Synchrony as an Indicator of Collaboration Quality, Task Performance and Learning","instantaneous derivative matching (idm),",mixed,physiological,learning gains,learning,product,pre/post tests,correlation,nonsig
84,"Using Physiological Synchrony as an Indicator of Collaboration Quality, Task Performance and Learning","directional agreement (da),",mixed,physiological,learning gains,learning,product,pre/post tests,correlation,nonsig
84,"Using Physiological Synchrony as an Indicator of Collaboration Quality, Task Performance and Learning",pearson’s correlation (pc),mixed,physiological,learning gains,learning,product,pre/post tests,correlation,sig
84,"Using Physiological Synchrony as an Indicator of Collaboration Quality, Task Performance and Learning",speech activity,speech participation,verbal,learning gains,learning,product,pre/post tests,anova,sig
84,"Using Physiological Synchrony as an Indicator of Collaboration Quality, Task Performance and Learning","signal matching (sm),",mixed,physiological,collaboration quality: dialogue management,communication,product,researcher coded,correlation,nonsig
84,"Using Physiological Synchrony as an Indicator of Collaboration Quality, Task Performance and Learning","signal matching (sm),",mixed,physiological,collaboration quality: reaching consensus,coordination,product,researcher coded,correlation,nonsig
84,"Using Physiological Synchrony as an Indicator of Collaboration Quality, Task Performance and Learning","signal matching (sm),",mixed,physiological,colaboration quality: reciprocal interaction,coordination,product,researcher coded,correlation,nonsig
84,"Using Physiological Synchrony as an Indicator of Collaboration Quality, Task Performance and Learning","signal matching (sm),",mixed,physiological,collaboration quality: information pooling,coordination,product,researcher coded,correlation,nonsig
84,"Using Physiological Synchrony as an Indicator of Collaboration Quality, Task Performance and Learning","instantaneous derivative matching (idm),",mixed,physiological,collaboration quality: dialogue management,communication,product,researcher coded,correlation,nonsig
84,"Using Physiological Synchrony as an Indicator of Collaboration Quality, Task Performance and Learning","instantaneous derivative matching (idm),",mixed,physiological,collaboration quality: reaching consensus,coordination,product,researcher coded,correlation,nonsig
84,"Using Physiological Synchrony as an Indicator of Collaboration Quality, Task Performance and Learning","instantaneous derivative matching (idm),",mixed,physiological,colaboration quality: reciprocal interaction,coordination,product,researcher coded,correlation,nonsig
84,"Using Physiological Synchrony as an Indicator of Collaboration Quality, Task Performance and Learning","instantaneous derivative matching (idm),",mixed,physiological,collaboration quality: information pooling,coordination,product,researcher coded,correlation,nonsig
84,"Using Physiological Synchrony as an Indicator of Collaboration Quality, Task Performance and Learning","directional agreement (da),",mixed,physiological,collaboration quality: dialogue management,communication,product,researcher coded,correlation,sig
84,"Using Physiological Synchrony as an Indicator of Collaboration Quality, Task Performance and Learning","directional agreement (da),",mixed,physiological,collaboration quality: reaching consensus,coordination,product,researcher coded,correlation,sig
84,"Using Physiological Synchrony as an Indicator of Collaboration Quality, Task Performance and Learning","directional agreement (da),",mixed,physiological,colaboration quality: reciprocal interaction,coordination,product,researcher coded,correlation,sig
84,"Using Physiological Synchrony as an Indicator of Collaboration Quality, Task Performance and Learning","directional agreement (da),",mixed,physiological,collaboration quality: information pooling,coordination,product,researcher coded,correlation,nonsig
84,"Using Physiological Synchrony as an Indicator of Collaboration Quality, Task Performance and Learning",pearson’s correlation (pc),mixed,physiological,collaboration quality: dialogue management,communication,product,researcher coded,correlation,nonsig
84,"Using Physiological Synchrony as an Indicator of Collaboration Quality, Task Performance and Learning",pearson’s correlation (pc),mixed,physiological,collaboration quality: reaching consensus,coordination,product,researcher coded,correlation,nonsig
84,"Using Physiological Synchrony as an Indicator of Collaboration Quality, Task Performance and Learning",pearson’s correlation (pc),mixed,physiological,colaboration quality: reciprocal interaction,coordination,product,researcher coded,correlation,nonsig
84,"Using Physiological Synchrony as an Indicator of Collaboration Quality, Task Performance and Learning",pearson’s correlation (pc),mixed,physiological,collaboration quality: information pooling,coordination,product,researcher coded,correlation,sig
89,Moving as a Leader: Detecting Emergent Leadership in Small Groups using Body Pose,head/body movement,gross body motion,body,emerging leadership,group composition,condition,researcher codes,sup. machine learning,sig
89,Moving as a Leader: Detecting Emergent Leadership in Small Groups using Body Pose,head/body movement,gross body motion,body,emerging leadership,group composition,condition,researcher codes,sup. machine learning,mixed
89,Moving as a Leader: Detecting Emergent Leadership in Small Groups using Body Pose,(non)concurrent speaking length,speech participation,verbal,emerging leadership,group composition,condition,researcher codes,sup. machine learning,sig
89,Moving as a Leader: Detecting Emergent Leadership in Small Groups using Body Pose,(non)concurrent speaking length,speech participation,verbal,emerging leadership,group composition,condition,researcher codes,sup. machine learning,mixed
89,Moving as a Leader: Detecting Emergent Leadership in Small Groups using Body Pose,speaking turn duration / number,speech participation,verbal,emerging leadership,group composition,condition,researcher codes,sup. machine learning,sig
89,Moving as a Leader: Detecting Emergent Leadership in Small Groups using Body Pose,speaking turn duration / number,speech participation,verbal,emerging leadership,group composition,condition,researcher codes,sup. machine learning,mixed
89,Moving as a Leader: Detecting Emergent Leadership in Small Groups using Body Pose,interruption,speech participation,verbal,emerging leadership,group composition,condition,researcher codes,sup. machine learning,sig
89,Moving as a Leader: Detecting Emergent Leadership in Small Groups using Body Pose,interruption,speech participation,verbal,emerging leadership,group composition,condition,researcher codes,sup. machine learning,mixed
90,Detecting Emergent Leader in a Meeting Environment,visual field of attention on a person features,gaze / eye direction,gaze,emerging leadership (surveyed) + emerging leadership (observed),group composition,condition,questionnaire (the systematic method for the multiple level observation of groups; general leader impression scale) + researcher codes (glis-observers),correlation,mixed
90,Detecting Emergent Leader in a Meeting Environment,visual field of attention on a person features,gaze / eye direction,gaze,emerging leadership (surveyed) + emerging leadership (observed),group composition,condition,questionnaire (the systematic method for the multiple level observation of groups; general leader impression scale) + researcher codes (glis-observers),sup. machine learning,sig
92,An Automatic Approach for Mining Patterns of Collaboration around an Interactive Tabletop,speech time and frequency,speech participation,verbal,collaboration,coordination,product,researcher codes (meier et al.),sup. machine learning,sig (85%)
92,An Automatic Approach for Mining Patterns of Collaboration around an Interactive Tabletop,speech time and frequency,speech participation,verbal,collaboration,communication,product,researcher codes (meier et al.),sup. machine learning,sig (85%)
92,An Automatic Approach for Mining Patterns of Collaboration around an Interactive Tabletop,symmetry of speech among group,speech participation,verbal,collaboration,coordination,product,researcher codes (meier et al.),sup. machine learning,sig (85%)
92,An Automatic Approach for Mining Patterns of Collaboration around an Interactive Tabletop,symmetry of speech among group,speech participation,verbal,collaboration,communication,product,researcher codes (meier et al.),sup. machine learning,sig (85%)
92,An Automatic Approach for Mining Patterns of Collaboration around an Interactive Tabletop,total number of touch actions,touch,activity log,collaboration,coordination,product,researcher codes (meier et al.),sup. machine learning,sig (85%)
92,An Automatic Approach for Mining Patterns of Collaboration around an Interactive Tabletop,total number of touch actions,touch,activity log,collaboration,communication,product,researcher codes (meier et al.),sup. machine learning,sig (85%)
92,An Automatic Approach for Mining Patterns of Collaboration around an Interactive Tabletop,symmetry of touch actions among group,touch,activity log,collaboration,coordination,product,researcher codes (meier et al.),sup. machine learning,sig (85%)
92,An Automatic Approach for Mining Patterns of Collaboration around an Interactive Tabletop,symmetry of touch actions among group,touch,activity log,collaboration,communication,product,researcher codes (meier et al.),sup. machine learning,sig (85%)
93,Modelling and Identifying Collaborative Situations in a Collocated Multi-display Groupware Setting,speech quantity,speech participation,verbal,collaboration,communication,product,researcher codes,sup. machine learning,sig
93,Modelling and Identifying Collaborative Situations in a Collocated Multi-display Groupware Setting,physical participation quantity,task-related,activity log,collaboration,communication,product,researcher codes,sup. machine learning,sig
93,Modelling and Identifying Collaborative Situations in a Collocated Multi-display Groupware Setting,number of active participants in group,speech participation,verbal,collaboration,communication,product,researcher codes,sup. machine learning,sig
93,Modelling and Identifying Collaborative Situations in a Collocated Multi-display Groupware Setting,verbal participation symmetry among group,speech participation,verbal,collaboration,communication,product,researcher codes,sup. machine learning,sig
94,Using Motion Sensors to Understand Collaborative Interactions in Digital Fabrication Labs,time spent individually,location,body,technical ability,performance,product,teaching team assessment,correlation,mixed
94,Using Motion Sensors to Understand Collaborative Interactions in Digital Fabrication Labs,time spent as a group,location,body,technical ability,performance,product,teaching team assessment,correlation,mixed
94,Using Motion Sensors to Understand Collaborative Interactions in Digital Fabrication Labs,time spent as a group,location,body,social ability,interpersonal relationship / perception,process,teaching team assessment,correlation,mixed
94,Using Motion Sensors to Understand Collaborative Interactions in Digital Fabrication Labs,transition probabilities between collaborative state,location,body,technical ability,performance,product,teaching team assessment,correlation,mixed
94,Using Motion Sensors to Understand Collaborative Interactions in Digital Fabrication Labs,transition probabilities between collaborative state,location,body,time spent in makerspace,performance,product,n/a,correlation,mixed
94,Using Motion Sensors to Understand Collaborative Interactions in Digital Fabrication Labs,transition probabilities between collaborative state,location,body,reported frustration,affective state,process,survey,correlation,mixed
96,What does physiological synchrony reveal about metacognitive experiences and group performance?,physiological synchrony,electrodermal activity,physiological,judgement of confidence,affective state,process,questionnaire,regression,non-sig
96,What does physiological synchrony reveal about metacognitive experiences and group performance?,physiological synchrony,electrodermal activity,physiological,mental effort,individual cognitive processes,process,questionnaire,regression,sig (positive)
96,What does physiological synchrony reveal about metacognitive experiences and group performance?,physiological synchrony,electrodermal activity,physiological,task difficulty,individual cognitive processes,process,questionnaire,regression,non-sig
96,What does physiological synchrony reveal about metacognitive experiences and group performance?,physiological synchrony,electrodermal activity,physiological,task interest,individual cognitive processes,process,questionnaire,regression,non-sig
96,What does physiological synchrony reveal about metacognitive experiences and group performance?,physiological synchrony,electrodermal activity,physiological,emotional valence,affective state,process,questionnaire,regression,non-sig
96,What does physiological synchrony reveal about metacognitive experiences and group performance?,physiological synchrony,electrodermal activity,physiological,perceived group performance,performance,product,questionnaire,regression,non-sig
96,What does physiological synchrony reveal about metacognitive experiences and group performance?,physiological synchrony,electrodermal activity,physiological,objective group performance score,performance,product,performance score,regression,non-sig
97,Physiological evidence of interpersonal dynamics in a cooperative production task,eda synchrony,electrodermal activity,physiological,team cohesion,interpersonal relationship / perception,process,questionnaire,regression,sig (negative)
97,Physiological evidence of interpersonal dynamics in a cooperative production task,smiling synchrony,facial expression,head,team cohesion,interpersonal relationship / perception,process,questionnaire,regression,sig
97,Physiological evidence of interpersonal dynamics in a cooperative production task,smiling synchrony,facial expression,head,routine choice,coordination,product,researcher codes,regression,sig
99,Brain-to-Brain Synchrony Tracks Real-World Dynamic Group Interactions in the Classroom,brain synchrony,neural activity,physiological,engagement,individual cognitive processes,process,questionnaire,regression,sig
99,Brain-to-Brain Synchrony Tracks Real-World Dynamic Group Interactions in the Classroom,brain synchrony,neural activity,physiological,social dynamics,interpersonal relationship / perception,process,questionnaire,regression,sig
101,Multi-modal Social Signal Analysis for Predicting Agreement in Conversation Settings,upper body agitation,gross body motion,body,agreement,coordination,product,expert rating,sup. machine learning,sig (75% accuracy)
101,Multi-modal Social Signal Analysis for Predicting Agreement in Conversation Settings,hand agitation,hand motion,body,agreement,coordination,product,expert rating,sup. machine learning,sig (75% accuracy)
101,Multi-modal Social Signal Analysis for Predicting Agreement in Conversation Settings,head orientation,head motion,head,agreement,coordination,product,expert rating,sup. machine learning,sig (75% accuracy)
101,Multi-modal Social Signal Analysis for Predicting Agreement in Conversation Settings,speaking time / turns,speech participation,verbal,agreement,coordination,product,expert rating,sup. machine learning,sig (75% accuracy)
103,Predicting Collaborative Learning Quality through Physiological Synchrony Recorded by Wearable Biosensors,eda synchrony,electrodermal activity,physiological,collaboration quality,coordination,product,researcher codes,sup. machine learning,sig
103,Predicting Collaborative Learning Quality through Physiological Synchrony Recorded by Wearable Biosensors,eda synchrony,electrodermal activity,physiological,collaboration quality,individual cognitive processes,process,researcher codes,sup. machine learning,sig
103,Predicting Collaborative Learning Quality through Physiological Synchrony Recorded by Wearable Biosensors,heart rate sychrony,heart rate,physiological,collaboration quality,coordination,product,researcher codes,sup. machine learning,non-sig
103,Predicting Collaborative Learning Quality through Physiological Synchrony Recorded by Wearable Biosensors,heart rate sychrony,heart rate,physiological,collaboration quality,individual cognitive processes,process,researcher codes,sup. machine learning,non-sig
104,Effects of Shared Gaze on Audio- Versus Text-Based Remote Collaborations,shared gaze,gaze / eye direction,gaze,cognitive load,individual cognitive processes,process,questionnaire,anova,non-sig
104,Effects of Shared Gaze on Audio- Versus Text-Based Remote Collaborations,shared gaze,gaze / eye direction,gaze,quality of collaboration,coordination,product,self-report,anova,sig
104,Effects of Shared Gaze on Audio- Versus Text-Based Remote Collaborations,shared gaze,gaze / eye direction,gaze,quality of collaboration,communication,product,self-report,anova,sig
104,Effects of Shared Gaze on Audio- Versus Text-Based Remote Collaborations,shared gaze,gaze / eye direction,gaze,task performance,performance,product,"completion time, correctness",anova,sig
104,Effects of Shared Gaze on Audio- Versus Text-Based Remote Collaborations,shared gaze,gaze / eye direction,gaze,gaze overlap,coordination,product,calculation,anova,sig
105,Body synchrony in triadic interaction,body synchronization,gross body motion,body,cooperation,interpersonal relationship / perception,process,task outcome (prisoner's dilemna),regression,non-sig
105,Body synchrony in triadic interaction,body synchronization,gross body motion,body,cultural style matching,group composition,condition,video coding,regression,sig (neg)
105,Body synchrony in triadic interaction,body synchronization,gross body motion,body,language style matching,group composition,condition,video coding,regression,sig (neg)
105,Body synchrony in triadic interaction,body synchronization,gross body motion,body,colaughter,interpersonal relationship / perception,process,video coding,regression,sig
174,Using the Tablet Gestures and Speech of Pairs of Students to Classify Their Collaboration,speech time,speech participation,verbal,collaboration quality,coordination,product,self made researcher coding scheme (4-scale ranging between collaboration and cooperation),sup. machine learning,sig
174,Using the Tablet Gestures and Speech of Pairs of Students to Classify Their Collaboration,speech time,speech participation,verbal,collaboration quality,communication,product,self made researcher coding scheme (4-scale ranging between collaboration and cooperation),sup. machine learning,sig
174,Using the Tablet Gestures and Speech of Pairs of Students to Classify Their Collaboration,thousands of features prosodic speech,audio features,verbal,collaboration quality,coordination,product,self made researcher coding scheme (4-scale ranging between collaboration and cooperation),sup. machine learning,sig
174,Using the Tablet Gestures and Speech of Pairs of Students to Classify Their Collaboration,thousands of features prosodic speech,audio features,verbal,collaboration quality,communication,product,self made researcher coding scheme (4-scale ranging between collaboration and cooperation),sup. machine learning,sig
174,Using the Tablet Gestures and Speech of Pairs of Students to Classify Their Collaboration,movement of objects including revisiting past actions,task-related,activity log,collaboration quality,coordination,product,self made researcher coding scheme (4-scale ranging between collaboration and cooperation),sup. machine learning,sig
174,Using the Tablet Gestures and Speech of Pairs of Students to Classify Their Collaboration,movement of objects including revisiting past actions,task-related,activity log,collaboration quality,communication,product,self made researcher coding scheme (4-scale ranging between collaboration and cooperation),sup. machine learning,sig
181,Improving Visibility of Remote Gestures in Distributed Tabletop Collaboration,gesture type and location,hand motion,body,frequency of utterances,communication,product,calculation,anova,sig
181,Improving Visibility of Remote Gestures in Distributed Tabletop Collaboration,gesture type and location,hand motion,body,subjective workload,individual cognitive processes,process,survey,anova,sig
182,Employing Social Gaze and Speaking Activity for Automatic Determination of the Extraversion Trait,speaking time % per individual,speech participation,verbal,extraversion rating,group composition,condition,big marker five scale (? researcher coded ?),sup. machine learning,sig
182,Employing Social Gaze and Speaking Activity for Automatic Determination of the Extraversion Trait,attention received per person,gaze / eye direction,gaze,extraversion rating,group composition,condition,big marker five scale (? researcher coded ?),sup. machine learning,sig
182,Employing Social Gaze and Speaking Activity for Automatic Determination of the Extraversion Trait,attention given by person,gaze / eye direction,gaze,extraversion rating,group composition,condition,big marker five scale (? researcher coded ?),sup. machine learning,sig
503,See What I’m Saying? Using Dyadic Mobile Eye Tracking to Study Collaborative Reference,gaze overlap,gaze / eye direction,gaze,referential form,communication,product,self-made researcher codes,regression,ns
506,Privacy-Preserving Speech Analytics for Automatic Assessment of Student Collaboration,duration of speech by each student,speech participation,verbal,collaboration quality of group [4 level scale],coordination,product,video coded scale of collaboration,sup. machine learning,ns
506,Privacy-Preserving Speech Analytics for Automatic Assessment of Student Collaboration,duration in which each student was only speaker,speech participation,verbal,collaboration quality of group [4 level scale],coordination,product,video coded scale of collaboration,sup. machine learning,ns
506,Privacy-Preserving Speech Analytics for Automatic Assessment of Student Collaboration,duration of overlapping speech from pairs of students,speech participation,verbal,collaboration quality of group [4 level scale],coordination,product,video coded scale of collaboration,sup. machine learning,ns
506,Privacy-Preserving Speech Analytics for Automatic Assessment of Student Collaboration,duration of overlapping speech from all people,speech participation,verbal,collaboration quality of group [4 level scale],coordination,product,video coded scale of collaboration,sup. machine learning,ns
506,Privacy-Preserving Speech Analytics for Automatic Assessment of Student Collaboration,duration of silence for all people,speech participation,verbal,collaboration quality of group [4 level scale],coordination,product,video coded scale of collaboration,sup. machine learning,ns
506,Privacy-Preserving Speech Analytics for Automatic Assessment of Student Collaboration,prosodic and tone features [many],audio features,verbal,collaboration quality of group [4 level scale],coordination,product,video coded scale of collaboration,sup. machine learning,ns
