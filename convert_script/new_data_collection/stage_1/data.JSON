{"0":{"paper_id_new":1,"title":"An Interactive Table for Supporting Participation Balance in Face-to-Face Collaborative Learning","year":2010,"authors":"Khaled Bachour, Frederic Kaplan, Pierre Dillenbourg","data_standardized":["III) Audio"],"sensor":["III) microphone"],"metric":["1) speech time"],"metric_larger_category":["1) Verbal"],"metric_smaller_category":["1) Speech Features"],"data_per_metric":["1) III"],"outcome":["A) verbal participation"],"outcome_instrument":["A) survey"],"outcome_smaller_category":["A) communication"],"outcome_larger_category":["A) process"],"analysis_and_results mm-oo:analysis:resultsig":["1-A: t-test: sig (t[38] = 2.18)"]},"1":{"paper_id_new":2,"title":"Understanding collaborative program comprehension: Interlacing gaze and dialogues","year":2013,"authors":"Kshitij Sharma, Patrick Jermann, Marc-Antoine N\u00fcssli, Pierre Dillenbourg","data_standardized":["I) Eye gaze","III) Audio"],"sensor":["I) eye-tracker","III) microphone"],"metric":["1) focused gaze ","2) together gaze ","3) dialogue episode","4) gaze transitions"],"metric_larger_category":["1) Gaze","2) Gaze","3) Verbal","4) Gaze"],"metric_smaller_category":["1) Visual Attention","2) Visual Attention","3) Speech Content","4) Eye Motion"],"data_per_metric":["1) I","2) I","3) III","4) I"],"outcome":["A) program understanding","B) dialogue episodes"],"outcome_instrument":["A) researcher codes","B) researcher codes"],"outcome_smaller_category":["A) performance","B) coordination"],"outcome_larger_category":["A) product","B) process"],"analysis_and_results mm-oo:analysis:resultsig":["1,2-A: anova: sig","1,2-A,B: mixed linear model: sig","4-B: anova: sig"]},"2":{"paper_id_new":3,"title":"Physiological Linkage of Dyadic Gaming Experience","year":2014,"authors":"Simo J\u00e4rvel\u00e4, J. Matias Kivikangas, Jari K\u00e4tsyri, Niklas Ravaja","data_standardized":["VI) Physiological","VII) Physiological"],"sensor":["VI) Varioport 16-bit digital skin conductance amplifier","VII) modified Lead II configuration"],"metric":["1) physiological linkage"],"metric_larger_category":["1) Physiological"],"metric_smaller_category":["1) EDA"],"data_per_metric":["1) VI&VII"],"outcome":["A) behavioral Involvement","B) empathy","C) negative Feelings","D) perceived comprehension"],"outcome_instrument":["A) Social Presence in Gaming Questionnaire","B) Social Presence in Gaming Questionnaire","C) Social Presence in Gaming Questionnaire","D) Social Presence Inventory Questionnaire"],"outcome_smaller_category":["A) cognitive engagement","B) affective","C) affective","D) learning"],"outcome_larger_category":["A) process","B) process","C) process","D) product"],"analysis_and_results mm-oo:analysis:resultsig":["1-A:regression: sig","1-B:regression: sig","1-C:regression: sig","1-D:regression: sig"]},"3":{"paper_id_new":4,"title":"A Multimodal-Sensor-Enabled Room for Unobtrusive Group Meeting Analysis","year":2018,"authors":null,"data_standardized":["II) Video","III) Audio","X) Body language"],"sensor":["II) kinect","III) microphone","X) IRMA Matrix ToF sensors"],"metric":["1) non-verbal speaking metrics","2) visual attention","3) verbal dominance and information metrics"],"metric_larger_category":["1) Verbal","2) Gaze","3) Verbal"],"metric_smaller_category":["1) Speech Participation","2) Visual Attention","3) Speech Participation"],"data_per_metric":["1) X","2) II","3) III"],"outcome":["A) perceived leadership","B) perceived contribution"],"outcome_instrument":["A) questionnaire","B) questionnaire"],"outcome_smaller_category":["A) interpersonal relationship \/ perception","B) interpersonal relationship \/ perception"],"outcome_larger_category":["A) process","B) process"],"analysis_and_results mm-oo:analysis:resultsig":["3-A : correlation : sig","3-B : correlation : sig","1-A : correlation : nonsig","1-B : correlation : nonsig","2-A : correlation : nonsig","2-B : correlation : sig","1,2,3-A : regression: nonsig","1,2,3-B : regression: nonsig"]},"4":{"paper_id_new":5,"title":"Are we together or not? The temporal interplay of monitoring, physiological arousal and physiological synchrony during a collaborative exam","year":2019,"authors":"Jonna Malmberm, Eetu Haataja, Tapio Sepp\u00e4nen, Sanna J\u00e4rvel\u00e4","data_standardized":["VI) Physiological","II) Video"],"sensor":["VI) EDA sensor","II) camera"],"metric":["1) eda peak detection ","2) physiological concordance index"],"metric_larger_category":["1) Physiological","2) Physiological"],"metric_smaller_category":["1) EDA","2) Combined"],"data_per_metric":["1) VI","2) II"],"outcome":["A) monitoring of behavior, cognition, motivations and emotions "],"outcome_instrument":["A) Video coding of monitoring (behavior, cognition, motivations and emotions) "],"outcome_smaller_category":["A) cognitive engagement"],"outcome_larger_category":["A) process"],"analysis_and_results mm-oo:analysis:resultsig":["1,2-A: correlation: sig (r=0.663)"]},"5":{"paper_id_new":6,"title":"Linking Speaking and Looking Behavior Patterns with Group Composition, Perception, and Performance","year":2012,"authors":"Dineshbabu Jayagopi, Dairazalia Sanchez-Cortes, Kazuhiro Otsuka Junji Yamato\n,Daniel Gatica-Perez\n","data_standardized":["III) Audio","II) Video"],"sensor":["III) Microphone","II) camera"],"metric":["1) group participation speaking cues","2) silence and overlap cues","3) speaking distribution cues","4) individual visual focus of attention","5) group looking cues"],"metric_larger_category":["1) Verbal","2) Verbal","3) Verbal","4) Gaze","5) Gaze"],"metric_smaller_category":["1) Speech Participation","2) Speech Participation","3) Speech Participation","4) Visual Attention","5) Visual Attention "],"data_per_metric":["1) III","2) III","3) III","4) IV","5) IV"],"outcome":["A) group composition","B) group interpersonal perception","C) group performance"],"outcome_instrument":["A) NEO-FFI questionnaire","B) Questionnaire","C) Negative distance between expert list and group list"],"outcome_smaller_category":["A) group composition","B) interpersonal relationship \/ perception","C) performance"],"outcome_larger_category":["A) condition","B) process","C) product"],"analysis_and_results mm-oo:analysis:resultsig":["1-A:correlation: sig","2-A:correlation: sig","3-A:correlation: sig","4-A:correlation: sig","5-A:correlation: sig","1-B:correlation: sig","2-B:correlation: sig","3-B:correlation: sig","4-B:correlation: sig","5-B:correlation: sig","1-C:correlation: sig","2-C:correlation: sig","3-C:correlation: sig","4-C:correlation: sig","5-C:correlation: sig","1,2-A:regression:sig","2,3-B:regression:sig","2-C:regression:sig"]},"6":{"paper_id_new":7,"title":"Automatic Recognition of Affective Laughter in Spontaneous Dyadic Interactions from Audiovisual Signals","year":2018,"authors":"Reshmashree B. Kantharaju, Fabien Ringeval","data_standardized":["I) Audio","II) Video"],"sensor":["I) NS","II) NS"],"metric":["1) GeMAPS acoustic features","2) extended GeMAPs acoustic features","3) MFCCs","4) facial action units"],"metric_larger_category":["1) Verbal","2) Verbal","3) Verbal","4) Head"],"metric_smaller_category":["1) Speech Features","2) Speech Features","3) Speech Features","4) Facial Expressions"],"data_per_metric":["1) I","2) I","3) I","4) II"],"outcome":["A) voiced Laughter","B) unvoiced Laughter","C) speech Laughter"],"outcome_instrument":["A) Voicing probability and unvoiced frame ratio","B) Voicing probability and unvoiced frame ratio","C) Human Annotations"],"outcome_smaller_category":["A) interpersonal relationship \/ perception","B) interpersonal relationship \/ perception","C) interpersonal relationship \/ perception"],"outcome_larger_category":["A) process","B) process","C) process"],"analysis_and_results mm-oo:analysis:resultsig":["1,2,3,4-A:sup. machine learning: nonsig","1,3,3,4-B:sup. machine learning: nonsig","1,2,3,4-C:sup. machine learning: nonsig"]},"7":{"paper_id_new":8,"title":"Predicting Group Performance in Task-Based Interaction","year":2018,"authors":"Gabriel Murray, Catharine Oertel, Laurent Besacier","data_standardized":["I) Audio"],"sensor":["I) Microphone"],"metric":["1) speech features ","2) linguistic features"],"metric_larger_category":["1) Verbal","2) Verbal"],"metric_smaller_category":["1) Speech Features","2) Speech Content"],"data_per_metric":["1) I","2) I "],"outcome":["A) group performance"],"outcome_instrument":["A) group scores compared to expert scores"],"outcome_smaller_category":["A) performance"],"outcome_larger_category":["A) product"],"analysis_and_results mm-oo:analysis:resultsig":["1,2-A: sup. machine learning: 64.4"]},"8":{"paper_id_new":9,"title":"Looking AT versus Looking THROUGH: A Dual Eye-tracking Study in MOOC Context","year":2015,"authors":"Sharma, K., Caballero, D., Verma, H., Jermann, P., & Dillenbourg, P.","data_standardized":["I) Eye gaze"],"sensor":["I) eyetracker"],"metric":["1) perceptual with-me-ness (gaze)","2) conceptual with-me-ness (gaze)","3) gaze similarity "],"metric_larger_category":["1) Gaze","2) Gaze","3) Gaze"],"metric_smaller_category":["1) Visual Attention","2) Visual Attention","3) Visual Attention"],"data_per_metric":["1) I","2) I","3) I"],"outcome":["A) learning gains"],"outcome_instrument":["A) pre-post test"],"outcome_smaller_category":["A) learning"],"outcome_larger_category":["A) product"],"analysis_and_results mm-oo:analysis:resultsig":["1-A: correlation: sig (r = 0.51)","2-A: correlation: sig (r = 0.41)","3-A: correlation: sig (r = 0.39)"]},"9":{"paper_id_new":10,"title":"A Network Analytic Approach to Gaze Coordination during a Collaborative Task","year":2018,"authors":"Andrist, S., Ruis, A.R., Shaffer, D.W.","data_standardized":["I) Eye gaze"],"sensor":["I) eye-tracking glasses"],"metric":["1) gaze fixations","2) gaze saccades"],"metric_larger_category":["1) Gaze","2) Gaze"],"metric_smaller_category":["1) Visual Attention","2) Eye Motion"],"data_per_metric":["1) I","2) I"],"outcome":["A) reference-action sequence"],"outcome_instrument":["A) researcher codes"],"outcome_smaller_category":["A) performance"],"outcome_larger_category":["A) product"],"analysis_and_results mm-oo:analysis:resultsig":["1,2-A:correlation: nonsig"]},"10":{"paper_id_new":11,"title":"The Additive Value of Multimodal Features for Predicting Engagement, Frustration, and Learning during Tutoring","year":2014,"authors":"Grafsgaard, Wiggins, Vail, Boyer","data_standardized":["II) Video","IV) Body language","V) Log data"],"sensor":["II) Kinect","IV) Kinect","V) own application"],"metric":["1) dialogue acts","2) facial expression","3) gesture","4) task actions"],"metric_larger_category":["1) Verbal","2) Head","3) Body","4) Log Data"],"metric_smaller_category":["1) Speech Content","2) Facial Expressions","3) Hand Motion","4) Task-related"],"data_per_metric":["1) V","2) II","3) IV","4) V"],"outcome":["A) engagement","B) frustration","C) learning gains"],"outcome_instrument":["A) participant self report","B) participant self report","C) pre-post test"],"outcome_smaller_category":["A) cognitive engagement","B) affective","C) learning"],"outcome_larger_category":["A) process","B) process","C) product"],"analysis_and_results mm-oo:analysis:resultsig":["1,2,3-A: regression: sig","1,2,3-B: regression: sig","1,2,3-C: regression: sig"]},"11":{"paper_id_new":12,"title":"(Dis)Engagement Maters: Identifying Efficacious Learning Practices with Multimodal Learning Analytics","year":2018,"authors":"Marcelo Worsley","data_standardized":["IV) Body language","II) Video"],"sensor":["IV) Kinect","II) camera"],"metric":["1) clustered hand\/wrist movement","2) object manipulation"],"metric_larger_category":["1) Body","2) Log Data"],"metric_smaller_category":["1) Hand Motion","2) Task-related"],"data_per_metric":["1) IV","2) II"],"outcome":["A) learning gains"],"outcome_instrument":["A) pre-post test (references to  principles or mechanisms that confer stability to three example structures) "],"outcome_smaller_category":["A) learning"],"outcome_larger_category":["A) product"],"analysis_and_results mm-oo:analysis:resultsig":["1-A:sup. machine learning: sig (ACC: 76%)"]},"12":{"paper_id_new":13,"title":"Dual Gaze as a Proxy for Collaboration in Informal Learning","year":2017,"authors":"Kshitij Sharma, Patrick Jermann, Marc-Antoine Nu\u0308ssli, Pierre Dillenbourg","data_standardized":["I) Eye gaze","II) Audio"],"sensor":["I) eye-tracker","II) Microphone"],"metric":["1) (not) focused together","2) dialogue episodes"],"metric_larger_category":["1) Gaze","2) Verbal"],"metric_smaller_category":["1) Visual Attention","2) Speech Content"],"data_per_metric":["1) I","2) II"],"outcome":["A) level of understanding"],"outcome_instrument":["A) researcher codes"],"outcome_smaller_category":["A) learning"],"outcome_larger_category":["A) product"],"analysis_and_results mm-oo:analysis:resultsig":["1-A: anova: sig (F [1,16]=8.70)","1,2-A: anova: sig (F [1,61]=7.60)"]},"13":{"paper_id_new":14,"title":"Using Eye-Tracking Technology to Support Visual Coordination in Collaborative Problem-Solving Groups","year":2013,"authors":"Bertrand Schneider, Roy Pea","data_standardized":["I) Eye gaze"],"sensor":["I) eye-tracker"],"metric":["1) joint visual attention"],"metric_larger_category":["1) Gaze"],"metric_smaller_category":["1) Visual Attention"],"data_per_metric":["1) I"],"outcome":["A) learning gains","B) collaboration quality"],"outcome_instrument":["A) learning test","B) coding scheme"],"outcome_smaller_category":["A) learning","B) coordination , communication"],"outcome_larger_category":["A) product","B) process"],"analysis_and_results mm-oo:analysis:resultsig":["1-A:regression: sig","1-B:regression: sig"]},"14":{"paper_id_new":15,"title":"Analysing frequent sequential patterns of collaborative learning activity around an interactive tabletop","year":2011,"authors":"Martinez, R., Yacef, K., Kay, J., Al-Qaraghuli, A., & Kharrufa, A.","data_standardized":["V) Log data"],"sensor":["V) interactive tabletop"],"metric":["1) events"],"metric_larger_category":["1) Log Data"],"metric_smaller_category":["1) Task-related "],"data_per_metric":["1) V"],"outcome":["A) group performance"],"outcome_instrument":["A) researcher codes"],"outcome_smaller_category":["A) performance"],"outcome_larger_category":["A) product"],"analysis_and_results mm-oo:analysis:resultsig":["1-A: unsup. machine learning: sig"]},"15":{"paper_id_new":16,"title":"The Effect of Mutual Gaze Perception on Students\u2019 Verbal Coordination","year":2014,"authors":"Bertrand Schneider, Roy Pea","data_standardized":["I) Eye gaze","V) Log data"],"sensor":["I) eye-tracker","V) NS"],"metric":["1) joint visual attention","2) n-grams","3) cosine similarity scores","4) convergence measures","5) coherence metrics"],"metric_larger_category":["1) Gaze","2) Verbal","3) Verbal","4) Verbal","5) Verbal"],"metric_smaller_category":["1) Visual Attention","2) Speech Content","3) Speech Content","4) Speech Content","5) Speech Content"],"data_per_metric":["1) I","2) V","3) V","4) V","5) V"],"outcome":["A) learning gains","B) collaboration quality"],"outcome_instrument":["A) learning test","B) coding scheme"],"outcome_smaller_category":["A) learning","B) coordination , communication"],"outcome_larger_category":["A) product","B) process"],"analysis_and_results mm-oo:analysis:resultsig":["1,4-A: ANOVA: nonsig","1,4-B:ANOVA: nonsig","1,5-A:correlation: sig","2,3,4,5-A: sup. machine learning: 75%"]},"16":{"paper_id_new":17,"title":"Detecting Collaborative Dynamics Using Mobile Eye-Trackers","year":2016,"authors":"Schneider, B., Sharma, K., Cuendet, S., Zufferey, G., Dillenbourg, P., & Pea, R","data_standardized":["I) Eye gaze","II) Video","III) Audio"],"sensor":["I) eye-tracker","II) camera","III) microphone "],"metric":["1) joint visual attention","2) gestures","3) speech duration"],"metric_larger_category":["1) Gaze","2) Body","3) Verbal"],"metric_smaller_category":["1) Visual Attention","2) Hand Motion","3) Speech Participation"],"data_per_metric":["1) I","2) II","3) III"],"outcome":["A) group performance ","B) learning gains"],"outcome_instrument":["A) researcher codes","B) learning test"],"outcome_smaller_category":["A) performance","B) learning"],"outcome_larger_category":["A) product","B) product"],"analysis_and_results mm-oo:analysis:resultsig":["1,2,3-B:qualitative: sig","1-B:correlation: sig"]},"17":{"paper_id_new":18,"title":"Expertise estimation based on simple multimodal features","year":2013,"authors":"Ochoa, X., Chiluiza, K., M\u00e9ndez, G., Luzardo, G., Guam\u00e1n, B., & Castells, J.","data_standardized":["II) Video","III) Audio","V) Log data"],"sensor":["II) camera","III) microphone","V) digital pen"],"metric":["1) calculator use","2) total movement","3) distance from the center of the table","4) number of interventions","5) speech duration","6) times numbers were mentioned","7) times mathematical terms were mentioned","8) times commands were pronounced","9) total number of pen strokes","10) average number of points","11) average stroke time length","12) average stroke path length","13) average stroke displacement","14) average stroke pressure"],"metric_larger_category":["1) Log Data","2) Body","3) Body","4) Verbal","5) Verbal","6) Verbal","7) Verbal","8) Verbal","9) Log Data ","10) Log Data ","11) Log Data ","12) Log Data ","13) Log Data ","14) Log Data"],"metric_smaller_category":["1) Task-related","2) Gross Body Motion","3) Location","4) Speech Participation","5) Speech Participation","6) Speech Content","7) Speech Content","8) Speech Content","9) Text ","10) Text ","11) Text ","12) Text ","13) Text ","14) Text "],"data_per_metric":["1) II","2) II","3) II","4) III","5) III","6) III","7) III","8) III","9) V","10) V","11) V","12) V","13) V","14) V"],"outcome":["A) odds of a student solving correctly a problem","B) expert prediction"],"outcome_instrument":["A) researcher codes","B) researcher codes"],"outcome_smaller_category":["A) performance","B) group composition"],"outcome_larger_category":["A) product","B) condition"],"analysis_and_results mm-oo:analysis:resultsig":["1,2,3,4,5,6,7,8,9,10,11,12,13,14-A:regression: sig","1,2,3,4,5,6,7,8,9,10,11,12,13,14-B:sup. machine learning: sig"]},"18":{"paper_id_new":19,"title":"Personality Trait Classification via Co-Occurrent Multiparty Multimodal Event Discovery","year":2015,"authors":"Okada, S., Aran, O., & Gatica-Perez, D.","data_standardized":["II) Video","III) Audio"],"sensor":["II) camera","III) microcone"],"metric":["1) speaking status","2) pitch","3) energy","4) head motion","5) body motion","6) motion energy images","7) gaze"],"metric_larger_category":["1) Verbal","2) Verbal","3) Verbal","4) Head","5) Body","6) Body","7) Gaze"],"metric_smaller_category":["1) Speech Participation","2) Speech Features","3) Speech Features","4) Head Motion","5) Gross Body Motion","6) Gross Body Motion","7) Visual Attention"],"data_per_metric":["1) III","2) III","3) III","4) II","5) II","6) II","7) II"],"outcome":["A) personality traits"],"outcome_instrument":["A) self-reported survey , researcher codes"],"outcome_smaller_category":["A) group composition"],"outcome_larger_category":["A) condition"],"analysis_and_results mm-oo:analysis:resultsig":["1,2,3,4,5,6,7-A:unsup. machine learning: 69.61%"]},"19":{"paper_id_new":20,"title":"Dynamic Adaptive Gesturing Predicts Domain Expertise in Mathematics","year":2019,"authors":"Sriramulu, A., Lin, J., & Oviatt, S.","data_standardized":["II) Video","III) Audio","V) Log data"],"sensor":["II) camera","III) microphone","V) digital pen and digital paper"],"metric":["1) total manual gestures per second","2) iconic gestures per second","3) deictic gestures per second"],"metric_larger_category":["1) Body","2) Body","3) Body"],"metric_smaller_category":["1) Hand Motion","2) Hand Motion","3) Hand Motion"],"data_per_metric":["1) II","2) II","3) II"],"outcome":["A) expertise"],"outcome_instrument":["A) researcher codes"],"outcome_smaller_category":["A) group composition"],"outcome_larger_category":["A) condition"],"analysis_and_results mm-oo:analysis:resultsig":["1-A:Wilcoxon Signed Ranks test: sig","2-A:Wilcoxon Signed Ranks test: sig","3-A:Wilcoxon Signed Ranks test: nonsig"]},"20":{"paper_id_new":21,"title":"Capturing and analyzing verbal and physical collaborative learning interactions at an enriched interactive tabletop","year":2013,"authors":"Martinez-Maldonado, R., Dimitriadis, Y., Martinez-Mon\u00e9s, A., Kay, J., & Yacef, K.","data_standardized":["III) Audio","V) Log data"],"sensor":["III) microphone","V) interactive tabletop"],"metric":["1) sequences of verbal utterances","2) sequences of meaningful actions"],"metric_larger_category":["1) Verbal","2) Log Data"],"metric_smaller_category":["1) Speech Content","2) Touch "],"data_per_metric":["1) III","2) V"],"outcome":["A) colloboration quality"],"outcome_instrument":["A) researcher codes"],"outcome_smaller_category":["A) coordination , communication"],"outcome_larger_category":["A) process"],"analysis_and_results mm-oo:analysis:resultsig":["1,2-A:unsup. machine learning: 90%"]},"21":{"paper_id_new":22,"title":"Automatic identification of experts and performance prediction in the multimodal math data corpus through analysis of speech interaction. ","year":2013,"authors":"Luz, S.","data_standardized":["III) Audio"],"sensor":["III) microphone"],"metric":["1) duration of all vocalisations","2) average duration of vocalisation","3) standard deviation of vocalisation","4) probability of a transition from floor to a vocalisation","5) probability of a transition from a vocalisation to floor","6) probability of transitioning from a group vocalisation to speaker vocalisation","7) probability of transitioning from a speaker vocalisation to a group vocalisation","8) uncertainty in the transitions originating from a speaker"],"metric_larger_category":["1) Verbal","2) Verbal","3) Verbal","4) Verbal","5) Verbal","6) Verbal","7) Verbal","8) Verbal"],"metric_smaller_category":["1) Speech Participation","2) Speech Participation","3) Speech Participation","4) Speech Participation","5) Speech Participation","6) Speech Participation","7) Speech Participation","8) Speech Participation"],"data_per_metric":["1) III","2) III","3) III","4) III","5) III","6) III","7) III","8) III"],"outcome":["A) identity of the expert"],"outcome_instrument":["A) student with the highest cumulative score of points assigned to correctly and incorrectly solved problems, weighted according a discrete diffculty level scale"],"outcome_smaller_category":["A) group composition"],"outcome_larger_category":["A) condition"],"analysis_and_results mm-oo:analysis:resultsig":["1,2,3,4,5,6,7,8-A: sup. machine learning: sig (ACC: 92%)"]},"22":{"paper_id_new":23,"title":"Unraveling Students' Interaction around a Tangible Interface Using Multimodal Learning Analytics.","year":2015,"authors":"Schneider, B., & Blikstein, P. ","data_standardized":["IV) Body language","V) Log data"],"sensor":["IV) Kinect","V) Own application"],"metric":["1) amount of exploration","2) types of exploration","3) amount of movement","4) type of movement","5) body synchronization","6) body distance"],"metric_larger_category":["1) Log Data","2) Log Data","3) Body","4) Body","5) Body","6) Body"],"metric_smaller_category":["1) Task-related","2) Task-related","3) Gross Body Motion","4) Gross Body Motion","5) Gross Body Motion","6) Location"],"data_per_metric":["1) V","2) V","3) IV","4) IV","5) IV","6) IV"],"outcome":["A) individual learning gains","B) group learning gains","C) leadership"],"outcome_instrument":["A) pre-post test","B) pre-post test","C) coding"],"outcome_smaller_category":["A) learning","B) learning","C) group composition"],"outcome_larger_category":["A) product","B) product","C) condition"],"analysis_and_results mm-oo:analysis:resultsig":["1-A: correlation: nonsig","2-A: correlation: mixed","3-A: correlation: nonsig","4-A: correlation: sig","4-C: ANOVA: sig","5-A: ANOVA: nonsig","6-A: correlation: nonsig","1,2,3,4,5,6-B: sup. machine learning: sig (ACC: 100%)"]},"23":{"paper_id_new":24,"title":"Unpacking the relationship between existing and new measures of physiological synchrony and collaborative learning: a mixed methods study","year":2020,"authors":"Bertrand Schneider, Yong Dich, Iulian Radu","data_standardized":["VI) Physiological","II) Audio"],"sensor":["VI) Smart Wristband","II) Microphone "],"metric":["1) physiological synchrony (PC)","2) physiological synchrony (DA)","3) physiological synchrony (SM)","4) physiological synchrony (IDM)","5) cycles of physiological synchrony (PC)"],"metric_larger_category":["1) Physiological","2) Physiological","3) Physiological","4) Physiological","5) Physiological"],"metric_smaller_category":["1) EDA","2) EDA","3) EDA","4) EDA","5) EDA"],"data_per_metric":["1) VI","2) VI","3) VI","4) VI","5) VI"],"outcome":["A) collaboration quality","B) task performance","C) learning gains"],"outcome_instrument":["A) Meier, Spada and Rummel coding scheme","B) Performance score","C) pre-post test"],"outcome_smaller_category":["A) coordination , communication","B) performance","C) learning"],"outcome_larger_category":["A) process","B) product","C) product"],"analysis_and_results mm-oo:analysis:resultsig":["1-C: correlation: sig r = 0.35","2-A: correlation: sig r =0.47","5-A: correlation: sig r = 0.57","5-C: correlation: sig r = 0.47"]},"24":{"paper_id_new":25,"title":"Modeling Team-level Multimodal Dynamics during Multiparty Collaboration","year":2019,"authors":"Eloy, L., Stewart, A. E. B., Amon, M. J., Reinhardt, C., Michaels, A., Chen, S., Shute, V., Duran, N. D., & D\u2019Mello, S. K.","data_standardized":["I) Audio","II) Video","VI) Physiological"],"sensor":["I) microphone","II) webcam","VI) electrodes"],"metric":["1) speech rate","2) face and upper body movement","3) galvanic skin response"],"metric_larger_category":["1) Verbal","2) Body","3) Physiological"],"metric_smaller_category":["1) Speech Features","2) Gross Body Motion","3) EDA"],"data_per_metric":["1) I","2) II","3) VI"],"outcome":["A) perceived collaboration quality","B) perceived valence","C) perceived arousal","D) task performance"],"outcome_instrument":["A) Questionnaire","B) 5-point Likert scale","C) 5-point Likert scale","D) Trophies earned"],"outcome_smaller_category":["A) interpersonal relationship \/ perception, coordination","B) affective","C) affective","D) performance"],"outcome_larger_category":["A) process","B) process","C) process","D) product"],"analysis_and_results mm-oo:analysis:resultsig":["1,2,3-A:unsup. machine learning: nonsig","1,2,3-B:unsup. machine learning: nonsig","1,2,3-C:unsup. machine learning: nonsig","1,2,3-D:unsup. machine learning: nonsig"]},"25":{"paper_id_new":26,"title":"Leveraging Mobile Eye-Trackers to Capture Joint Visual Attention in Co-Located Collaborative Learning","year":2018,"authors":"Bertrand Schneider","data_standardized":["I) Eye gaze"],"sensor":["I) eyetracker"],"metric":["1) joint visual attention","2) cycles of collaborative \/ individual work"],"metric_larger_category":["1) Gaze","2) Gaze"],"metric_smaller_category":["1) Visual Attention","2) Visual Attention"],"data_per_metric":["1) I","2) I"],"outcome":["A) learning gains","B) collaboration quality","C) task performance"],"outcome_instrument":["A) pre-post test","B) Meier Spada Rummel coding scheme","C) number of mazes solved"],"outcome_smaller_category":["A) learning","B) coordination , communication","C) performance"],"outcome_larger_category":["A) product","B) process","C) product"],"analysis_and_results mm-oo:analysis:resultsig":["1-B: correlation: sig r = 0.341","2-B: correlation: sig r = 0.347","2-A: correlation: sig r = 0.398","2-C: correlation: sig r = 0.355"]},"26":{"paper_id_new":27,"title":"Gaze quality assisted automatic recognition of social contexts in collaborative Tetris","year":2010,"authors":"Weifeng Li, Marc-Antoine N\u00fcssli, Patrick Jermann","data_standardized":["I) Eye gaze","VI) Log data"],"sensor":["I) eye-tracker","VI) digital"],"metric":["1) gaze location","2) gaze saccade","3) gaze fixation","4) player actions","5) zoid acceleration"],"metric_larger_category":["1) Gaze","2) Gaze","3) Gaze","4) Log Data","5) Log Data"],"metric_smaller_category":["1) Visual Attention","2) Eye Motion","3) Visual Attention","4) Task-related","5) Task-related "],"data_per_metric":["1) I","2) I","3) I","4) VI","5) VI"],"outcome":["A) social context"],"outcome_instrument":["A) experimental set-up"],"outcome_smaller_category":["A) group composition"],"outcome_larger_category":["A) condition"],"analysis_and_results mm-oo:analysis:resultsig":["1,2,3,4,5-A: sup. machine learning: sig (ACC:81.43)"]},"27":{"paper_id_new":28,"title":"Acoustic-Prosodic Entrainment and Rapport in Collaborative Learning Dialogues","year":2014,"authors":"Nichola Lubold, Heather Pon-Barry","data_standardized":["III) Audio"],"sensor":["III) microphone"],"metric":["1) pitch","2) intensity","3) voice quality","4) speaking rate","5) proximity","6) convergence","7) synchrony"],"metric_larger_category":["1) Verbal","2) Verbal","3) Verbal","4) Verbal","5) Verbal","6) Verbal","7) Verbal"],"metric_smaller_category":["1) Speech Features","2) Speech Features","3) Speech Features","4) Speech Features","5) Speech Features","6) Speech Features","7) Speech Features"],"data_per_metric":["1) III","2) III","3) III","4) III","5) III","6) III","7) III"],"outcome":["A) rapport level"],"outcome_instrument":["A) human coding validated with self-reports"],"outcome_smaller_category":["A) interpersonal relationship \/ perception"],"outcome_larger_category":["A) process"],"analysis_and_results mm-oo:analysis:resultsig":["5-A: correlation: mixed (Max r 0.842)","6-A: correlation: mixed (Max r 0.741)","7-A: correlation: mixed (Max r 0.634)"]},"28":{"paper_id_new":29,"title":"Can Eye Help You?: Effects of Visualizing Eye Fixations on Remote Collaboration Scenarios for Physical Tasks","year":2016,"authors":"Keita Higuch, Ryo Yonetani, & Yoichi Sato","data_standardized":["I) Eye gaze"],"sensor":["I) optical see-through head-mounted display"],"metric":["1) gaze location"],"metric_larger_category":["1) Gaze"],"metric_smaller_category":["1) Visual Attention"],"data_per_metric":["1) I"],"outcome":["A) quality of remote collaboration","B) task completion time"],"outcome_instrument":["A) seven-point scale questionnaire","B) timer"],"outcome_smaller_category":["A) performance","B) performance"],"outcome_larger_category":["A) product","B) product"],"analysis_and_results mm-oo:analysis:resultsig":["1-A:Wilcoxon Signed Ranks test: sig","1-B:Wilcoxon Signed Ranks test: sig"]},"29":{"paper_id_new":30,"title":"Using Mobile Eye-Trackers to Unpack the Perceptual Benefits of a Tangible User Interface for Collaborative Learning","year":2016,"authors":"Schneider, B., *Sharma., K., *Cuendet, S., Zufferey, G., Dillenbourg, P., & Pea, R","data_standardized":["I) Eye gaze"],"sensor":["I) eyetracker"],"metric":["1) joint visual attention"],"metric_larger_category":["1) Gaze"],"metric_smaller_category":["1) Visual Attention"],"data_per_metric":["1) I"],"outcome":["A) task performance","B) learning gains"],"outcome_instrument":["A) calculation","B) pre post test"],"outcome_smaller_category":["A) performance","B) learning"],"outcome_larger_category":["A) product","B) product"],"analysis_and_results mm-oo:analysis:resultsig":["1-A: correlation: sig (r = 0.59)","1-B: correlation: sig (r = 0.42)"]},"30":{"paper_id_new":31,"title":"Personality classification and behaviour interpretation: An approach based on feature categories","year":2016,"authors":"Sheng Fang, Catherine Achard, & Severine Dubuisson","data_standardized":["II) Video","III) Audio"],"sensor":["II) camera","III) microcone"],"metric":["1) intra-personal features","2) dyadic features","3) one vs all features"],"metric_larger_category":["1) Head","2) Head","3) Verbal"],"metric_smaller_category":["1) Facial Expressions","2) Facial Expressions","3) Speech Features"],"data_per_metric":["1) II&III","2) II&III","3) II&III"],"outcome":["A) personality traits","B) social impressions"],"outcome_instrument":["A) self-reported survey","B) questionnaire"],"outcome_smaller_category":["A) group composition","B) interpersonal relationship \/ perception"],"outcome_larger_category":["A) condition","B) process"],"analysis_and_results mm-oo:analysis:resultsig":["1-A: Regression: 73.53%","1-B:Regression: 76.47%","2-A: Regression: 60.54%","2-B: Regression: 66.42%","3-A: Regression: 65.69%","3-B: Regression: 73.53%"]},"31":{"paper_id_new":32,"title":"Investigating Automatic Dominance Estimation in Groups From Visual Attention and Speaking Activity","year":2008,"authors":"Hayley Hung, Dinesh Babu Jayagopi, Sileye Ba, Jean-Marc Odobez, Daniel Gatica-Perez","data_standardized":["II) Video","III) Audio"],"sensor":["II) camera","III) microphone"],"metric":["1) audio energy features","2) visual focus of attention"],"metric_larger_category":["1) Verbal","2) Gaze"],"metric_smaller_category":["1) Speech Features","2) Visual Attention"],"data_per_metric":["1) III","2) II"],"outcome":["A) visual dominance ratio"],"outcome_instrument":["A) manual annotation of videos"],"outcome_smaller_category":["A) group composition"],"outcome_larger_category":["A) condition"],"analysis_and_results mm-oo:analysis:resultsig":["1,2-A: sup. machine learning: 79.4%"]},"32":{"paper_id_new":33,"title":"High Accuracy Detection of Collaboration From Log Data and Superficial Speech Features","year":2017,"authors":"Sree Aurovindh Viswanathan, Kurt VanLehn","data_standardized":["II) Video","III) Audio","V) Log data","X) Log data"],"sensor":["II) camera","III) microphone","V) digital","X) digital"],"metric":["1) card movements","2) scrolling","3) zooming","4) audio features"],"metric_larger_category":["1) Log Data","2) Log Data","3) Log Data","4) Verbal"],"metric_smaller_category":["1) Task-related ","2) Task-related ","3) Task-related ","4) Speech Features"],"data_per_metric":["1) V","2) V","3) V","4) III"],"outcome":["A) collaboration","B) asymmetric contribution","C) cooperation"],"outcome_instrument":["A) manual annotation of videos","B) manual annotation of videos","C) manual annotation of videos"],"outcome_smaller_category":["A) coordination , communication","B) coordination","C) coordination , communication"],"outcome_larger_category":["A) process","B) process","C) process"],"analysis_and_results mm-oo:analysis:resultsig":["1,2,3,4-A,C: sup. machine learning: 96%","1,2,3,4-A,B,C: sup. machine learning: 86%"]},"33":{"paper_id_new":34,"title":"Using Interlocutor-Modulated Attention BLSTM to Predict Personality Traits in Small Group Interaction","year":2018,"authors":"Yun-Shao Lin, Chi-Chun Lee","data_standardized":["III) Audio"],"sensor":["III) NS"],"metric":["1) speech utterances"],"metric_larger_category":["1) Verbal"],"metric_smaller_category":["1) Speech Content"],"data_per_metric":["1) III"],"outcome":["A) personality traits"],"outcome_instrument":["A) self-reported survey , perceived interaction scores"],"outcome_smaller_category":["A) group composition"],"outcome_larger_category":["A) condition"],"analysis_and_results mm-oo:analysis:resultsig":["1-A: sup. machine learning : 87.9%"]},"34":{"paper_id_new":35,"title":"Dynamics of Visual Attention in Multiparty Collaborative Problem Solving using Multidimensional Recurrence Quantification Analysis","year":2019,"authors":"Hana Vrzakova, Angela E. B. Stewart, Mary Jean Amon, Sidney K. D\u2019Mello","data_standardized":["I) Eye gaze"],"sensor":["I) eye-tracker"],"metric":["1) gaze area of interest ","2) cross-recurrence quantification analysis","3) multidimensional recurrence quantification analysis"],"metric_larger_category":["1) Gaze","2) Gaze","3) Gaze"],"metric_smaller_category":["1) Visual Attention","2) Visual Attention","3) Visual Attention"],"data_per_metric":["1) I","2) I","3) I"],"outcome":["A) construction of shared knowledge","B) negotiation","C) coordination","D) task score","E) group performance"],"outcome_instrument":["A) manual annotation of videos","B) manual annotation of videos","C) manual annotation of videos","D) expert coding of task, post-test score","E) self-reported survey"],"outcome_smaller_category":["A) coordination","B) coordination","C) coordination","D) performance","E) performance"],"outcome_larger_category":["A) process","B) process","C) process","D) product","E) product"],"analysis_and_results mm-oo:analysis:resultsig":["2-A: regression: sig ","3-B: regression: sig","3-C: regression: sig","2-D: regression: nonsig","2-E: regression: sig","2-D: regression: nonsig","2-E: regression:nosig"]},"35":{"paper_id_new":36,"title":"Does Seeing One Another\u2019s Gaze Affect Group Dialogue?","year":2015,"authors":"Bertrand Schneider, Roy Pea","data_standardized":["I) Eye gaze","III) Audio"],"sensor":["I) eye-tracker","III) microphone"],"metric":["1) joint visual attention","2) simple linguistic features","3) convergence of linguistic styles","4) coherence"],"metric_larger_category":["1) Gaze","2) Verbal","3) Verbal","4) Verbal"],"metric_smaller_category":["1) Visual Attention","2) Speech Content","3) Speech Content","4) Speech Content"],"data_per_metric":["1) I","2) III","3) III","4) III"],"outcome":["A) learning gains","B) collaboration","C) joint visual attention"],"outcome_instrument":["A) pre-post test","B) Coding scheme","C) calculation"],"outcome_smaller_category":["A) learning","B) coordination, communication","C) coordination, communication"],"outcome_larger_category":["A) product","B) process","C) process"],"analysis_and_results mm-oo:analysis:resultsig":["3-A,B,C: correlation: nonsig","4-A: correlation: sig","4-C: ANOVA: sig","2-A: sup. machine learning: 75%"]},"36":{"paper_id_new":37,"title":"Emergent leaders through looking and speaking: from\naudio-visual data to multimodal recognition","year":2013,"authors":"Dairazalia Sanchez-Cortes, Oya Aran, Dinesh Babu Jayagopi, Marianne Schmid Mast, Daniel Gatica-Perez","data_standardized":["III) Audio","II) Video"],"sensor":["III) microphone","II) camera"],"metric":["1) speaking activity","2) visual attention","3) audio-visual"],"metric_larger_category":["1) Verbal","2) Gaze","3) Gaze"],"metric_smaller_category":["1) Speech Participation","2) Visual Attention","3) Visual Attention"],"data_per_metric":["1) III","2) II","3) III&II"],"outcome":["A) personality traits","B) percieved interaction score","C) ranked dominance","D) task performance","E) perceived leadership and dominance"],"outcome_instrument":["A) NEO-FFI, PRF","B) questionnaire ","C) questionnaire","D) expert grading","E) questionnaire"],"outcome_smaller_category":["A) group composition","B) interpersonal relationship \/ perception","C) interpersonal relationship \/ perception","D) performance","E) interpersonal relationship \/ perception"],"outcome_larger_category":["A) condition","B) process","C) process","D) product","E) process"],"analysis_and_results mm-oo:analysis:resultsig":["1,3-E: sup. machine learning: 50%","1,3-C: sup. machine learning: 59.1%"]},"37":{"paper_id_new":38,"title":"Predicting the Quality of Collaborative Problem Solving Through Linguistic Analysis of Discourse","year":2019,"authors":"Joseph Reilly, Bertrand Schneider","data_standardized":["I) Eye gaze","III) Audio","IV) Body language","VI) Physiological"],"sensor":["I) Mobile eye-tracker","III) microphone","IV) Kinect v2","VI) Smart wristband"],"metric":["1) coh-metrix indices","2) physical synchrony","3) physiological synchrony","4) joint visual attention"],"metric_larger_category":["1) Verbal","2) Body","3) Physiological","4) Gaze"],"metric_smaller_category":["1) Speech Features","2) Gross Body Motion","3) EDA","4) Visual Attention"],"data_per_metric":["1) III","2) IV","3) VI","4) I"],"outcome":["A) learning gains","B) collaboration","C) coh-metrix indices"],"outcome_instrument":["A) pre-post test","B) Meier Spada Rummel coding scheme","C) computational measures of transcripts"],"outcome_smaller_category":["A) learning","B) coordination , communication","C) communication"],"outcome_larger_category":["A) product","B) process","C) process"],"analysis_and_results mm-oo:analysis:resultsig":["1-A: correlation: sig","1-B: correlation: sig","1-C: correlation: sig","2-C: correlation: sig","3-C: correlation: sig","4-C: correlation: sig","2-B: sup. machine learning: 84%"]},"38":{"paper_id_new":39,"title":"Shared Experiences of Technology and Trust: An Experimental Study of Physiological Compliance Between Active and Passive Users in Technology-Mediated Collaborative Encounters","year":2014,"authors":"Enid Montague, Jie Xu, and Erin Chiou","data_standardized":["VI) Physiological","VII) Physiological"],"sensor":["VI) wearable sensor","VII) wearable sensor"],"metric":["1) SM - EDA","2) IDM - EDA","3) DA - EDA","4) CC - EDA","5) WC - EDA","6) SM - HR","7) IDM - HR","8) DA - HR","9) CC - HR","10) WC - HR low frequency","11) WC - HR high frequency"],"metric_larger_category":["1) Physiological","2) Physiological","3) Physiological","4) Physiological","5) Physiological","6) Physiological","7) Physiological","8) Physiological","9) Physiological","10) Physiological","11) Physiological"],"metric_smaller_category":["1) EDA","2) EDA","3) EDA","4) EDA","5) EDA","6) Heart","7) Heart","8) Heart","9) Heart","10) Heart","11) Heart"],"data_per_metric":["1) VI","2) VI","3) VI","4) VI","5) VI","6) VII","7) VII","8) VII","9) VII","10) VII","11) VII"],"outcome":["A) task performance","B) self-rated workload"],"outcome_instrument":["A) Multiattribute Task Battery program","B) Questionnaire (NASA Task Load Index)"],"outcome_smaller_category":["A) performance","B) cognitive engagement"],"outcome_larger_category":["A) product","B) process"],"analysis_and_results mm-oo:analysis:resultsig":["1-A: LME: nonsig","2-A: LME: nonsig","3-A: LME: sig","4-A: LME: nonsig","5-A: LME: nonsig","6-A: LME: nonsig","7-A: LME: nonsig","8-A: LME: nonsig","9-A: LME: nonsig","10-A: LME: nonsig","11-A: LME: nonsig","1-B: LME: nonsig","2-B: LME: nonsig","3-B: LME: nonsig","4-B: LME: nonsig","5-B: LME: nonsig","6-B: LME: nonsig","7-B: LME: nonsig","8-A: LME: nonsig","9-A: LME: nonsig","10-A: LME: nonsig","11-A: LME: nonsig"]},"39":{"paper_id_new":40,"title":"Using Multimodal Learning Analytics to Identify Aspects of Collaboration in Project-Based Learning","year":2017,"authors":"Daniel Spikol, Emanuele Ruffaldi, Mutlu Cukurova","data_standardized":["II) Video"],"sensor":["II) camera"],"metric":["1) count of faces looking at screen","2) distance between learners","3) distance between hands","4) hand motion speed"],"metric_larger_category":["1) Gaze","2) Body","3) Body","4) Body"],"metric_smaller_category":["1) Visual Attention","2) Location","3) Hand Motion","4) Hand Motion"],"data_per_metric":["1) II","2) II","3) II","4) II"],"outcome":["A) physical engagement","B) synchronisation","C) individual accountability"],"outcome_instrument":["A) Researcher codes","B) Researcher codes","C) Researcher codes"],"outcome_smaller_category":["A) cognitive engagement","B) coordination","C) coordination, cognitive engagement"],"outcome_larger_category":["A) process","B) process","C) process"],"analysis_and_results mm-oo:analysis:resultsig":["3-C:regression: sig","3-B:regression: sig","1+3-B:regression: sig","3-A:regression: sig"]},"40":{"paper_id_new":41,"title":"Modeling Collaboration Patterns on an Interactive Tabletop in a Classroom Setting","year":2016,"authors":"Abigail C. Evans, Jacob O. Wobbrock and Katie Davis","data_standardized":["V) Log data"],"sensor":["V) touch screen"],"metric":["1) touch patterns"],"metric_larger_category":["1) Log Data"],"metric_smaller_category":["1) Touch"],"data_per_metric":["1) V"],"outcome":["A) social regulation"],"outcome_instrument":["A) Rogat and Linnenbrink-Garcia\u2019s framework"],"outcome_smaller_category":["A) coordination, cognitive engagement"],"outcome_larger_category":["A) process"],"analysis_and_results mm-oo:analysis:resultsig":["1-A:calculation:84.2%"]},"41":{"paper_id_new":42,"title":"Toward Using Multi-Modal Learning Analytics to Support and Measure Collaboration in Co-Located Dyads","year":2018,"authors":"Emma L. Starr, Joseph M. Reilly, and Bertrand Schneider","data_standardized":["III) Audio","I) Eye gaze","VI) Physiological","IV) Body language"],"sensor":["III) kinect","I) eye-tracker","VI) wearable sensor ","IV) kinect"],"metric":["1) total movement across upper body joints and body parts","2) talking time"],"metric_larger_category":["1) Body","2) Verbal"],"metric_smaller_category":["1) Gross Body Motion","2) Speech Participation"],"data_per_metric":["1) IV","2) III"],"outcome":["A) collaboration quality"],"outcome_instrument":["A) experimenter code"],"outcome_smaller_category":["A) coordination , communication"],"outcome_larger_category":["A) process"],"analysis_and_results mm-oo:analysis:resultsig":["1-A: correlation: sig","2-A: correlation: sig"]},"42":{"paper_id_new":43,"title":"An Alternate Statistical Lens to Look at Collaboration Data: Extreme Value Theory","year":2019,"authors":"Kshitij Sharma, Jennifer Olsen","data_standardized":["I) Eye gaze"],"sensor":["I) eye-tracker"],"metric":["1) EVT of spatial entropy"],"metric_larger_category":["1) Gaze"],"metric_smaller_category":["1) Visual Attention"],"data_per_metric":["1) I"],"outcome":["A) collaboration outcome","B) post-test score"],"outcome_instrument":["A) NS","B) Pre-post test"],"outcome_smaller_category":["A) performance","B) learning"],"outcome_larger_category":["A) product","B) product"],"analysis_and_results mm-oo:analysis:resultsig":["1-A: regression: sig","1-B: regression: sig"]},"43":{"paper_id_new":44,"title":"Going beyond what is visible: What multichannel data can reveal about interaction in the context of collaborative learning?","year":2019,"authors":"Jonna Malmberg, Sanna J\u00e4rvel\u00e4, Jukka Holappa, Eetu Haataja, Xiaohua Huang, Antti Siipo","data_standardized":["II) Video","III) Audio","VI) Physiological"],"sensor":["II) camera","III) microphone","VI) wearable sensor"],"metric":["1) facial expression","2) physiological simultaneous arousal"],"metric_larger_category":["1) Head","2) Physiological"],"metric_smaller_category":["1) Facial Expressions","2) EDA"],"data_per_metric":["1) II","2) VI"],"outcome":["A) type of working activity","B) type of interaction"],"outcome_instrument":["A) Researcher codes","B) Researcher codes"],"outcome_smaller_category":["A) coordination","B) communication"],"outcome_larger_category":["A) process","B) process"],"analysis_and_results mm-oo:analysis:resultsig":["1-A: calculation: nonsig","1-B: ANOVA: sig","2-A: calculation: nonsig","2-B: calculation: nonsig"]},"44":{"paper_id_new":45,"title":"Task-independent Multimodal Prediction of Group Performance Based on Product Dimensions","year":2019,"authors":"Go Miura, Shogo Okada","data_standardized":["III) Audio","IV) Body language","V) Log data"],"sensor":["III) microphone","IV) wearable sensor","V) digital"],"metric":["1) speaking turn features","2) acoustic features","3) head motion features","4) linguistic features"],"metric_larger_category":["1) Verbal","2) Verbal","3) Head","4) Verbal"],"metric_smaller_category":["1) Speech Participation","2) Speech Features","3) Head Motion","4) Speech Content"],"data_per_metric":["1) III","2) III","3) IV","4) V"],"outcome":["A) artefact quality"],"outcome_instrument":["A) Researcher codes"],"outcome_smaller_category":["A) performance"],"outcome_larger_category":["A) product"],"analysis_and_results mm-oo:analysis:resultsig":["1,2,3,4-A : unsup. machine learning : sig"]},"45":{"paper_id_new":46,"title":"Toward Collaboration Sensing","year":2014,"authors":"Schneider & Pea","data_standardized":["I) Eye gaze"],"sensor":["I) eye-tracker"],"metric":["1) network features"],"metric_larger_category":["1) Gaze"],"metric_smaller_category":["1) Visual Attention"],"data_per_metric":["1) I"],"outcome":["A) collaboration quality"],"outcome_instrument":["A) Meier, Spada and Rummel coding scheme"],"outcome_smaller_category":["A) coordination , communication"],"outcome_larger_category":["A) process"],"analysis_and_results mm-oo:analysis:resultsig":["1-A: sup. machine learning: 85-100%","1-A: correlation: sig"]},"46":{"paper_id_new":47,"title":"Supervised machine learning in multimodal learning analytics for estimating success in project-based learning","year":2017,"authors":"Daniel Spikol, Emanuele Ruffaldi, Giacomo Dabisias, Mutlu Cukurova","data_standardized":["IV) Body language","VI) Body language","III) Audio","V) Log data"],"sensor":["IV) kinect","VI) kinect","III) microphone","V) arduino IDE "],"metric":["1) number of faces looking at screen","2) mean distance between learners ","3) mean distance between hands","4) mean hand movement speed","5) mean audio level","6) arduino measure of complexity","7) arduino active hardware blocks","8) arduino active software blocks","9) arduino active blocks","10) student work phases"],"metric_larger_category":["1) Head","2) Body","3) Body","4) Body","5) Verbal","6) Log Data","7) Log Data","8) Log Data","9) Log Data","10) Log Data"],"metric_smaller_category":["1) Head Motion","2) Location","3) Hand Motion","4) Hand Motion","5) Speech Features","6) Task-related","7) Task-related","8) Task-related","9) Task-related","10) Task-related"],"data_per_metric":["1) IV","2) VI","3) VI","4) VI","5) III","6) V","7) V","8) V","9) V","10) IV"],"outcome":["A) artefact quality"],"outcome_instrument":["A) Researcher codes"],"outcome_smaller_category":["A) performance"],"outcome_larger_category":["A) product"],"analysis_and_results mm-oo:analysis:resultsig":["1,2,3,4,5,6,7,8,9,10-A: sup. machine learning: 24%"]},"47":{"paper_id_new":48,"title":"Multimodal prediction of expertise and leadership in learning groups","year":2012,"authors":"Stefan Scherer, Nadir Weibel, Sharon Oviatt, Louis-Philippe Morency","data_standardized":["III) Audio","V) Log data "],"sensor":["III) microphone","V) digital pen"],"metric":["1) pause duration","2) energy","3) articulation rate","4) fundamental frequency","5) peak slope","6) spectral stationarity ","7) writing rate","8) writing area ","9) aspect ration","10) pressure","11) uninterrupted writing ","12) pause distribution\/average pauses"],"metric_larger_category":["1) Verbal","2) Verbal","3) Verbal","4) Verbal","5) Verbal","6) Verbal","7) Log Data","8) Log Data","9) Log Data","10) Log Data","11) Log Data","12) Log Data"],"metric_smaller_category":["1) Speech Participation","2) Speech Features","3) Speech Features","4) Speech Features","5) Speech Features","6) Speech Features","7) Text","8) Text","9) Text","10) Text","11) Text","12) Text"],"data_per_metric":["1) III","2) III","3) III","4) III","5) III","6) III","7) V","8) V","9) V","10) V","11) V","12) V"],"outcome":["A) leadership","B) expertise"],"outcome_instrument":["A) assigned","B) problem solving performance"],"outcome_smaller_category":["A) group composition","B) group composition"],"outcome_larger_category":["A) condition","B) condition"],"analysis_and_results mm-oo:analysis:resultsig":["1-A: t-test: sig","3-A: t-test: sig","5-A: t-test: sig","6-A: t-test: sig","5-B: t-test: sig","3-B: t-test: nonsig","1-B: t-test: nonsig","6-B: t-test: nonsig"]},"48":{"paper_id_new":49,"title":"Estimation of success in collaborative learning based on multimodal learning analytics features","year":2017,"authors":"Daniel Spikol, Emanuele Ruffaldi, Lorenzo Landolfi, Mutlu Cukurova","data_standardized":["IV) Body language","V) Log data","III) Audio"],"sensor":["IV) kinect","V) Arduino IDE","III) microphone"],"metric":["1) faces looking at screen ","2) distance between learners","3) distance between hands","4) number of active blocks","5) variety of hardware blocks","6) variety of software blocks ","7) number of interconnections between blocks ","8) audio level"],"metric_larger_category":["1) Head","2) Body","3) Body","4) Log Data","5) Log Data","6) Log Data","7) Log Data","8) Verbal"],"metric_smaller_category":["1) Head Motion","2) Location","3) Hand Motion","4) Task-related","5) Task-related","6) Task-related","7) Task-related","8) Speech Features"],"data_per_metric":["1) IV","2) IV","3) IV","4) V","5) V","6) V","7) V","8) III"],"outcome":["A) collaborative problem solving "],"outcome_instrument":["A) researcher codes "],"outcome_smaller_category":["A) coordination, performance"],"outcome_larger_category":["A) process, product"],"analysis_and_results mm-oo:analysis:resultsig":["1,2-A: sup. machine learning: sig","8-A: sup. machine learning: sig"]},"49":{"paper_id_new":50,"title":"Unpacking Collaborative Learning Processes during Hands-on Activities using Mobile Eye-Trackers","year":2017,"authors":"Bertrand Schneider","data_standardized":["I) Eye gaze"],"sensor":["I) eye-tracker"],"metric":["1) joint visual attention"],"metric_larger_category":["1) Gaze"],"metric_smaller_category":["1) Visual attention"],"data_per_metric":["1) I"],"outcome":["A) collaboration quality","B) cycles of collaboration","C) learning gains ","D) task performance"],"outcome_instrument":["A) researcher codes","B) researcher analysis","C) pre-post test","D) correctness "],"outcome_smaller_category":["A) coordination , communication","B) coordination","C) learning","D) performance"],"outcome_larger_category":["A) process","B) process","C) product","D) product"],"analysis_and_results mm-oo:analysis:resultsig":["1-A: correlation: sig","1-A: correlation: sig"]},"50":{"paper_id_new":51,"title":"3D Tangibles Facilitate Joint Visual Attention in Dyads","year":2015,"authors":"Bertrand Schneider, Kshitij Sharma, S\u00e9bastien Cuendet, Guillaume Zufferey, Pierre Dillenbourg, Roy Pea","data_standardized":["I) Eye gaze"],"sensor":["I) eye-tracker"],"metric":["1) joint visual attention"],"metric_larger_category":["1) Gaze"],"metric_smaller_category":["1) Visual attention"],"data_per_metric":["1) I"],"outcome":["A) collaboration quality","B) student performance ","C) learning gains "],"outcome_instrument":["A) researcher codes","B) correctness","C) pre-post test"],"outcome_smaller_category":["A) coordination , communication","B) performance","C) learning"],"outcome_larger_category":["A) process","B) product","C) product"],"analysis_and_results mm-oo:analysis:resultsig":["1-A: correlation: sig","1-B: regression: sig","1-C: correlation: sig"]},"51":{"paper_id_new":52,"title":"Exploring Collaboration Using Motion Sensors and Multi-Modal Learning Analytics","year":2018,"authors":"Joseph M. Reilly, Milan Ravenell, Bertrand Schneider","data_standardized":["IV) Body language"],"sensor":["IV) kinect"],"metric":["1) joint movement","2) joint angle","3) dyad proximity"],"metric_larger_category":["1) Body","2) Body","3) Body"],"metric_smaller_category":["1) Gross Body Motion","2) Gross Body Motion","3) Location"],"data_per_metric":["1) IV","2) IV","3) IV"],"outcome":["A) task performance","B) collaboration","C) learning gains"],"outcome_instrument":["A) researcher codes","B) researcher codes","C) pre-post test"],"outcome_smaller_category":["A) performance","B) coordination , communication","C) learning"],"outcome_larger_category":["A) product","B) process","C) product"],"analysis_and_results mm-oo:analysis:resultsig":["1-A: correlation: sig","1-B: correlation: nonsig","2-A: correlation: sig \/ mixed","3-B: correlation: nonsig","3-A: correlation: sig"]},"52":{"paper_id_new":53,"title":"Real-time mutual gaze perception enhances collaborative  learning and collaboration quality","year":2013,"authors":"Bertrand Schneider, Roy Pea","data_standardized":["I) Eye gaze","II) Video"],"sensor":["I) eye tracker","II) eye tracker"],"metric":["1) joint visual attention","2) cognitive load (from pupil size)"],"metric_larger_category":["1) Gaze","2) Gaze"],"metric_smaller_category":["1) Visual Attention","2) Eye Physiology"],"data_per_metric":["1) I","2) II"],"outcome":["A) learning gains"],"outcome_instrument":["A) pre\/post test"],"outcome_smaller_category":["A) learning"],"outcome_larger_category":["A) product"],"analysis_and_results mm-oo:analysis:resultsig":["1-A :mediation: sig","2-A:mediation: nonsig"]},"53":{"paper_id_new":54,"title":"Investigating collaborative learning success with physiological coupling indices based on electrodermal activity","year":2016,"authors":"H\u00e9ctor J. Pijeira-D\u00edaz, Hendrik Drachsler, Sanna J\u00e4rvel\u00e4, Paul A. Kirschner","data_standardized":["VI) Physiological"],"sensor":["VI) wearable sensor"],"metric":["1) signal matching ","2) instantaneous derivative matching ","3) directional agreement","4) pearson\u2019s correlation coefficient ","5) fisher\u2019s z-transform of pearson's correlation coefficient"],"metric_larger_category":["1) Physiological","2) Physiological","3) Physiological","4) Physiological","5) Physiological"],"metric_smaller_category":["1) EDA","2) EDA","3) EDA","4) EDA","5) EDA"],"data_per_metric":["1) VI","2) VI","3) VI","4) VI","5) VI"],"outcome":["A) collaborative will","B) collaborative learning product","C) dual learning gains"],"outcome_instrument":["A) self report","B) researcher coded","C) researcher coded"],"outcome_smaller_category":["A) interpersonal relationship \/ perception","B) performance","C) learning"],"outcome_larger_category":["A) process","B) product","C) product"],"analysis_and_results mm-oo:analysis:resultsig":["1,3,4,5-A : regression : nonsig","2-A:regression: sig","1,3,4,5-B : regression : nonsig","2-B:regression: sig","1,2,4,5-C : regression : nonsig","3-C:regression: sig"]},"54":{"paper_id_new":55,"title":"Multimodal Analysis of Vocal Collaborative Search:A Public Corpus and Results","year":2017,"authors":"McDuff, Thomas, Czerwinski, Craswell","data_standardized":["II) Video","III) Audio"],"sensor":["II) camera","III) microphone"],"metric":["1) linguistic features","2) voice features","3) facial expression features"],"metric_larger_category":["1) Verbal","2) Verbal","3) Head"],"metric_smaller_category":["1) Speech Content","2) Speech Features","3) Facial Expressions"],"data_per_metric":["1) III","2) III","3) II"],"outcome":["A) perception of peer helpfulness","B) perception of peer understanding","C) perception of peer clarity"],"outcome_instrument":["A) questionnaire","B) questionnaire","C) questionnaire"],"outcome_smaller_category":["A) interpersonal relationship \/ perception","B) interpersonal relationship \/ perception","C) interpersonal relationship \/ perception"],"outcome_larger_category":["A) process","B) process","C) process"],"analysis_and_results mm-oo:analysis:resultsig":["1,2,3-A: correlation: sig","1,2,3-B: correlation: sig","1,2,3-C:correlation: sig","1,2-A:correlation: nonsig","1,2-B:correlation: nonsig","1,2-C:correlation: nonsig","3-A:correlation: sig","3-B:correlation: sig","3-C:correlation: sig"]},"55":{"paper_id_new":56,"title":"Focused or Stuck Together: Multimodal Patterns Reveal Triads\u2019 Performance in Collaborative Problem Solving","year":2020,"authors":"Hana Vrzakova,Mary Jean Amon,Angela E.B. Stewart,Nicholas D Duran,Sidney K D'Mello","data_standardized":["II) Video","V) Log data"],"sensor":["II) camera","V) own application"],"metric":["1) type of activity done in task","2) amount of face and body movement","3) target for discussion partner"],"metric_larger_category":["1) Log Data","2) Body","3) Log Data"],"metric_smaller_category":["1) Task-related","2) Gross Body Motion","3) Task-related"],"data_per_metric":["1) V","2) II","3) II"],"outcome":["A) task success score","B) subjective perception of collaboration"],"outcome_instrument":["A) researcher coded","B) self report"],"outcome_smaller_category":["A) performance","B) interpersonal relationship \/ perception"],"outcome_larger_category":["A) product","B) process"],"analysis_and_results mm-oo:analysis:resultsig":["1,2,3-A : correlation : sig","1,2,3-B : correlation : sig"]},"56":{"paper_id_new":57,"title":"Using Physiological Synchrony as an Indicator of Collaboration Quality, Task Performance and Learning","year":2018,"authors":"Yong\u00a0Dich, Joseph\u00a0Reilly, Bertrand\u00a0Schneider","data_standardized":["II) Video","III) Audio","VI) Physiological"],"sensor":["II) kinect","III) kinect","VI) wearable sensor"],"metric":["1) signal matching","2) instantaneous derivative matching ","3) directional agreement ","4) pearson\u2019s correlation coefficient","5) speech activity"],"metric_larger_category":["1) Physiological","2) Physiological","3) Physiological","4) Physiological","5) Verbal"],"metric_smaller_category":["1) Combined","2) Combined","3) Combined","4) Combined","5) Speech Participation"],"data_per_metric":["1) VI","2) VI","3) VI","4) VI","5) III"],"outcome":["A) learning gains","B) collaboration quality: dialogue management","C) collaboration quality: reaching consensus","D) colaboration quality: reciprocal interaction","E) collaboration quality: information pooling"],"outcome_instrument":["A) pre\/post tests","B) researcher coded","C) researcher coded","D) researcher coded","E) researcher coded"],"outcome_smaller_category":["A) learning","B) communication","C) coordination","D) coordination","E) coordination"],"outcome_larger_category":["A) product","B) process","C) process","D) process","E) process"],"analysis_and_results mm-oo:analysis:resultsig":["1,2,3-A : correlation : nonsig","4-A:correlation: sig","5-A : ANOVA : sig","1,2,4-B : correlation : nonsig","3-B:correlation: sig","1,2,4-C:correlation : nonsig","3-C:correlation: sig","1,2,4-D:correlation: nonsig","3-D:correlation: sig","1,2,3-E:correlation : nonsig","4-E:correlation: sig"]},"57":{"paper_id_new":58,"title":"Moving as a Leader: Detecting Emergent Leadership in Small Groups using Body Pose","year":2017,"authors":"Cigdem Beyan, Vasiliki-Maria Katsageorgiou, Vittorio Murino","data_standardized":["II) Video","III) Audio"],"sensor":["II) camera","III) microphone"],"metric":["1) head\/body movement","2) (non)concurrent speaking length","3) speaking turn duration\/number","4) interruption"],"metric_larger_category":["1) Body","2) Verbal","3) Verbal","4) Verbal"],"metric_smaller_category":["1) Gross Body Motion","2) Speech Participation","3) Speech Participation","4) Speech Participation"],"data_per_metric":["1) II","2) III","3) III","4) III"],"outcome":["A) emerging leadership"],"outcome_instrument":["A) researcher codes"],"outcome_smaller_category":["A) group composition"],"outcome_larger_category":["A) condition"],"analysis_and_results mm-oo:analysis:resultsig":["1-A: sup. machine learning: sig","2,3,4-A: sup. machine learning: sig","1,2,3,4-A: sup. machine learning: mixed"]},"58":{"paper_id_new":59,"title":"Detecting Emergent Leader in a Meeting Environment","year":2016,"authors":"Cigdem Beyan, Nicol\u00f2 Carissimi, Francesca Capozzi, Sebastiano Vascon, Matteo Bustreo, Antonio Pierro, Cristina Becchio, Vittorio Murino","data_standardized":["II) Video"],"sensor":["II) camera"],"metric":["1) visual field of attention on a person features"],"metric_larger_category":["1) Gaze"],"metric_smaller_category":["1) Visual Attention"],"data_per_metric":["1) II"],"outcome":["A) surveyed and observed emerging leadership"],"outcome_instrument":["A) questionnaire (The SYstematic method for the Multiple Level Observation of Groups; General Leader Impression Scale) + researcher codes (GLIS-observers)"],"outcome_smaller_category":["A) group composition"],"outcome_larger_category":["A) condition"],"analysis_and_results mm-oo:analysis:resultsig":["1-A: correlation: mixed","1-A: sup. machine learning: sig"]},"59":{"paper_id_new":60,"title":"An Automatic Approach for Mining Patterns of Collaboration around an Interactive Tabletop","year":2013,"authors":"Roberto Martinez-Maldonado, Judy Kay, Kalina Yacef","data_standardized":["III) Audio","V) Log data"],"sensor":["III) Microphone","V) own application"],"metric":["1) speech time and frequency","2) symmetry of speech among group","3) total number of touch actions","4) symmetry of touch actions among group"],"metric_larger_category":["1) Verbal","2) Verbal","3) Log Data","4) Log Data"],"metric_smaller_category":["1) Speech Participation","2) Speech Participation","3) Touch ","4) Touch"],"data_per_metric":["1) III","2) III","3) V","4) V"],"outcome":["A) collaboration"],"outcome_instrument":["A) researcher codes (Meier et al.)"],"outcome_smaller_category":["A) coordination , communication"],"outcome_larger_category":["A) process"],"analysis_and_results mm-oo:analysis:resultsig":["1,2,3,4-A: sup. machine learning: sig (85%)"]},"60":{"paper_id_new":61,"title":"Modelling and Identifying Collaborative Situations in a Collocated Multi-display Groupware Setting","year":2011,"authors":"Roberto Martinez, James R. Wallace, Judy Kay, Kalina Yacef","data_standardized":["III) Audio","V) Log data"],"sensor":["III) Microphone","V) own application"],"metric":["1) speech quantity","2) physical participation quantity","3) number of active participants in group","4) verbal participation symmetry among group","5) physical participation symmetry among group"],"metric_larger_category":["1) Verbal","2) Log Data","3) Verbal","4) Verbal","5) Log Data"],"metric_smaller_category":["1) Speech Participation","2) Task-related","3) Speech Participation","4) Speech Participation","5) Task-related"],"data_per_metric":["1) III","2) V","3) III","4) III","5) V"],"outcome":["A) collaboration"],"outcome_instrument":["A) researcher codes"],"outcome_smaller_category":["A) communication"],"outcome_larger_category":["A) process"],"analysis_and_results mm-oo:analysis:resultsig":["1,2,3,4-A: sup. machine learning: sig"]},"61":{"paper_id_new":62,"title":"Using Motion Sensors to Understand Collaborative Interactions in Digital Fabrication Labs","year":2020,"authors":"Edwin Chng, Mohamed Raouf Seyam, William Yao, Bertrand Schneider","data_standardized":["IV) Body language"],"sensor":["IV) kinect"],"metric":["1) time spent individually","2) time spent as a group","3) diversity of collaborative interactions","4) transition probabilities between collaborative state"],"metric_larger_category":["1) Body","2) Body","3) Body","4) Body"],"metric_smaller_category":["1) Location","2) Location","3) Location","4) Location"],"data_per_metric":["1) IV","2) IV","3) IV","4) IV"],"outcome":["A) technical ability","B) social ability","C) time spent in makerspace","D) reported frustration"],"outcome_instrument":["A) teaching team assessment","B) teaching team assessment","C) N\/A","D) survey"],"outcome_smaller_category":["A) performance","B) interpersonal relationship \/ perception","C) performance","D) affective"],"outcome_larger_category":["A) product","B) process","C) product","D) process"],"analysis_and_results mm-oo:analysis:resultsig":["1-A: correlation: mixed","2-A: correlation: mixed","2-B: correlation: mixed","4-A: correlation: mixed","4-C: correlation: mixed","4-D: correlation: mixed"]},"62":{"paper_id_new":63,"title":"What does physiological synchrony reveal about metacognitive experiences and group performance?","year":2020,"authors":"Muhterem Dindar, Sanna J\u00e4rvel\u00e4 and Eetu Haataja","data_standardized":["VI) Physiological"],"sensor":["VI) Electrodes"],"metric":["1) physiological synchrony "],"metric_larger_category":["1) Physiological"],"metric_smaller_category":["1) EDA"],"data_per_metric":["1) VI"],"outcome":["A) judgement of confidence","B) mental effort","C) task difficulty","D) task interest","E) emotional valence","F) perceived group performance","G) objective group performance score"],"outcome_instrument":["A) questionnaire","B) questionnaire","C) questionnaire","D) questionnaire","E) questionnaire","F) questionnaire","G) performance score"],"outcome_smaller_category":["A) affective","B) cognitive engagement","C) cognitive engagement","D) cognitive engagement","E) affective","F) performance","G) performance"],"outcome_larger_category":["A) process","B) process","C) process","D) process","E) process","F) product","G) product"],"analysis_and_results mm-oo:analysis:resultsig":["1-A: regression: nonsig","1-B: regression: sig (positive)","1-C: regression: nonsig","1-D: regression: nonsig","1-E: regression: nonsig","1-F: regression: nonsig","1-G:regression: nonsig"]},"63":{"paper_id_new":64,"title":"Physiological evidence of interpersonal dynamics in a cooperative production task","year":2016,"authors":"Dan M\u00f8nster, Dorthe D\u00f8jbak H\u00e5konsson, Jacob Kj\u00e6r Eskildsen, Sebastian Wallot","data_standardized":["VI) Physiological","VII) Physiological","X) Facial expression"],"sensor":["VI) Electrodes","VII) Electrodes","X) Electrodes"],"metric":["1) EDA synchrony","2) smiling synchrony","3) heart rate sychrony"],"metric_larger_category":["1) Physiological","2) Head","3) Physiological"],"metric_smaller_category":["1) EDA","2) Facial Expressions","3) Heart"],"data_per_metric":["1) VI","2) X","3) VII"],"outcome":["A) team cohesion","B) routine choice"],"outcome_instrument":["A) questionnaire","B) researcher codes"],"outcome_smaller_category":["A) interpersonal relationship \/ perception","B) coordination"],"outcome_larger_category":["A) process","B) process"],"analysis_and_results mm-oo:analysis:resultsig":["1-A: regression: sig (negative)","2-A: regression: sig","2-B: regression: sig"]},"64":{"paper_id_new":65,"title":"Brain-to-Brain Synchrony Tracks Real-World Dynamic Group Interactions in the Classroom","year":2017,"authors":"Suzanne Dikker, Lu Wan, Ido Davidesco, ..., Jay J. Van Bavel, Mingzhou Ding, David Poeppel","data_standardized":["XI) Physiological"],"sensor":["XI) EEG"],"metric":["1) brain synchrony"],"metric_larger_category":["1) Physiological"],"metric_smaller_category":["1) Brain"],"data_per_metric":["1) XI"],"outcome":["A) engagement","B) social dynamics"],"outcome_instrument":["A) questionnaire","B) questionnaire"],"outcome_smaller_category":["A) cognitive engagement","B) interpersonal relationship \/ perception"],"outcome_larger_category":["A) process","B) process"],"analysis_and_results mm-oo:analysis:resultsig":["1-A: regression: sig","1-B: regression: sig"]},"65":{"paper_id_new":66,"title":"Multi-modal Social Signal Analysis for Predicting Agreement in Conversation Settings","year":2013,"authors":"V\u00edctor Ponce-L\u00f3pez, Sergio  Escalera, Xavier Bar\u00f3","data_standardized":["III) Audio","II) Video","IV) Body language"],"sensor":["III) microphone","II) kinect RGB","IV) kinect "],"metric":["1) upper body agitation","2) hand agitation","3) head orientation","4) speaking time\/turns"],"metric_larger_category":["1) Body","2) Body","3) Head","4) Verbal"],"metric_smaller_category":["1) Gross Body Motion","2) Hand Motion","3) Head Motion","4) Speech Participation"],"data_per_metric":["1) IV","2) IV","3) II","4) III"],"outcome":["A) agreement"],"outcome_instrument":["A) expert rating"],"outcome_smaller_category":["A) coordination"],"outcome_larger_category":["A) process"],"analysis_and_results mm-oo:analysis:resultsig":["1,2,3,4-A: sup. machine learning: sig (75% accuracy)"]},"66":{"paper_id_new":67,"title":"Predicting Collaborative Learning Quality through Physiological Synchrony Recorded by Wearable Biosensors","year":2020,"authors":"Yang Liu , Tingting Wang , Kun Wang , Yu Zhang","data_standardized":["VI) Physiological","VII) Physiological"],"sensor":["VI) wearable sensor","VII) wearable sensor"],"metric":["1) EDA synchrony","2) heart rate sychrony"],"metric_larger_category":["1) Physiological","2) Physiological"],"metric_smaller_category":["1) EDA","2) Heart"],"data_per_metric":["1) VI","2) VII"],"outcome":["A) collaboration quality"],"outcome_instrument":["A) researcher codes"],"outcome_smaller_category":["A) coordination, cognitive engagement"],"outcome_larger_category":["A) process"],"analysis_and_results mm-oo:analysis:resultsig":["1-A: sup. machine learning: sig","2-A: sup. machine learning: nonsig"]},"67":{"paper_id_new":68,"title":"Effects of Shared Gaze on Audio- Versus Text-Based Remote Collaborations","year":2020,"authors":"Grete Helena K\u00dctt, Teerapaun Tanprasert, Jay Rodolitz, Bernardo Moyza, Samuel So, Georgia Kenderova, Alexandra Papoutsaki","data_standardized":["I) Eye gaze"],"sensor":["I) Tobii 4C"],"metric":["1) shared gaze"],"metric_larger_category":["1) Gaze"],"metric_smaller_category":["1) Visual Attention"],"data_per_metric":["1) I"],"outcome":["A) cognitive load","B) collaboration quality","C) task performance","D) gaze overlap"],"outcome_instrument":["A) questionnaire","B) self-report","C) Completion time, correctness","D) calculation"],"outcome_smaller_category":["A) cognitive engagement","B) coordination, communication","C) performance","D) coordination"],"outcome_larger_category":["A) process","B) process","C) product","D) process"],"analysis_and_results mm-oo:analysis:resultsig":["1-A: anova: nonsig","1-B: anova: sig","1-C: anova: sig","1-D: anova: sig"]},"68":{"paper_id_new":69,"title":"Body synchrony in triadic interaction","year":2020,"authors":"Rick Dale, Gregory A. Bryant, Joseph H. Manson, and Matthew M. Gervais","data_standardized":["II) Video"],"sensor":["II) camera"],"metric":["1) body synchronization"],"metric_larger_category":["1) Body"],"metric_smaller_category":["1) Gross Body Motion"],"data_per_metric":["1) II"],"outcome":["A) cooperation","B) cultural style matching","C) language style matching","D) colaughter"],"outcome_instrument":["A) task outcome (prisoner's dilemna)","B) video coding","C) video coding","D) video coding"],"outcome_smaller_category":["A) interpersonal relationship \/ perception","B) group composition","C) group composition","D) interpersonal relationship \/ perception"],"outcome_larger_category":["A) process","B) condition","C) condition","D) process"],"analysis_and_results mm-oo:analysis:resultsig":["1-A: regression: nonsig","1-B: regression: sig (neg)","1-C: regression: sig (neg)","1-D: regression: sig"]},"69":{"paper_id_new":70,"title":"Using the Tablet Gestures and Speech of Pairs of Students to Classify Their Collaboration","year":2017,"authors":"Sree Aurovindh Viswanathan and Kurt VanLehn","data_standardized":["III) Audio","V) Log data"],"sensor":["III) Microphone","V) Log files"],"metric":["1) speech time","2) prosodic speech features","3) movement of objects"],"metric_larger_category":["1) Verbal","2) Verbal","3) Log Data"],"metric_smaller_category":["1) Speech Participation","2) Speech Features","3) Task-related"],"data_per_metric":["1) III","2) III","3) V"],"outcome":["A) collaboration quality"],"outcome_instrument":["A) self made researcher coding scheme (4-scale ranging between collaboration and cooperation)"],"outcome_smaller_category":["A) coordination"],"outcome_larger_category":["A) process"],"analysis_and_results mm-oo:analysis:resultsig":["1,2,3-A:sup. machine learning: sig"]},"70":{"paper_id_new":71,"title":"Improving Visibility of Remote Gestures in Distributed Tabletop Collaboration","year":2011,"authors":"Naomi Yamashita, Katsuhiko Kaji, Hideaki Kuzuoka, Keiji Hirata","data_standardized":["IV) Body language"],"sensor":["IV) video camera array"],"metric":["1) gesture type and location"],"metric_larger_category":["1) Body"],"metric_smaller_category":["1) Hand Motion"],"data_per_metric":["1) IV"],"outcome":["A) frequency of utterances","B) subjective workload"],"outcome_instrument":["A) calculation","B) survey"],"outcome_smaller_category":["A) communication","B) cognitive engagement"],"outcome_larger_category":["A) process","B) process"],"analysis_and_results mm-oo:analysis:resultsig":["1-A: ANOVA: sig","1-B: ANOVA: sig"]},"71":{"paper_id_new":72,"title":"Employing Social Gaze and Speaking Activity for Automatic Determination of the Extraversion Trait","year":2010,"authors":"Lepri, Bruno, Ramanathan Subramanian, Kyriaki Kalimeri, Jacopo Staiano, Fabio Pianesi, Nicu Sebe","data_standardized":["II) Video","III) Audio"],"sensor":["II) camera","III) microphone"],"metric":["1) speaking time","2) attention received per person","3) attention given by person"],"metric_larger_category":["1) Verbal","2) Gaze","3) Gaze"],"metric_smaller_category":["1) Speech Participation","2) Visual Attention","3) Visual Attention"],"data_per_metric":["1) III","2) II","3) II"],"outcome":["A) extraversion rating"],"outcome_instrument":["A) Big Marker Five Scale (? researcher coded ?)"],"outcome_smaller_category":["A) group composition"],"outcome_larger_category":["A) condition"],"analysis_and_results mm-oo:analysis:resultsig":["1,2,3-A: sup. machine learning: sig"]},"72":{"paper_id_new":73,"title":"See What I\u2019m Saying? Using Dyadic Mobile Eye Tracking to Study Collaborative Reference","year":2011,"authors":"Darren Gergle, Alan T. Clark","data_standardized":["I) Eye gaze"],"sensor":["I) eye-tracker"],"metric":["1) gaze overlap"],"metric_larger_category":["1) Gaze"],"metric_smaller_category":["1) Visual Attention"],"data_per_metric":["1) I "],"outcome":["A) referential form"],"outcome_instrument":["A) self-made researcher codes"],"outcome_smaller_category":["A) communication"],"outcome_larger_category":["A) process"],"analysis_and_results mm-oo:analysis:resultsig":["1-A:regression: nonsig"]},"73":{"paper_id_new":74,"title":"Privacy-Preserving Speech Analytics for Automatic Assessment of Student Collaboration","year":2016,"authors":"Nikoletta Bassiou, Andreas Tsiartas, Jennifer Smith, Harry Bratt, Colleen Richey, Elizabeth Shriberg, Cynthia D\u2019Angelo, Nonye Alozie","data_standardized":["III) Audio"],"sensor":["III) microphones"],"metric":["1) duration of speech by each student","2) duration in which each student was only speaker","3) duration of overlapping speech from pairs of students","4) duration of overlapping speech from all people","5) duration of silence for all people","6) prosodic and tone features"],"metric_larger_category":["1) Verbal","2) Verbal","3) Verbal","4) Verbal","5) Verbal","6) Verbal"],"metric_smaller_category":["1) Speech Participation","2) Speech Participation","3) Speech Participation","4) Speech Participation","5) Speech Participation","6) Speech Features"],"data_per_metric":["1) III","2) III","3) III","4) III","5) III","6) III"],"outcome":["A) collaboration quality"],"outcome_instrument":["A) video coded scale of collaboration"],"outcome_smaller_category":["A) coordination"],"outcome_larger_category":["A) process"],"analysis_and_results mm-oo:analysis:resultsig":["1,2,3,4,5,6-A:sup. machine learning: nonsig"]},"74":{"paper_id_new":75,"title":"(Dis)Engagement Matters: Identifying Efficacious Learning Practices with Multimodal Learning Analytics","year":2018,"authors":"Marcelo Worsley","data_standardized":["None"],"sensor":["IV) Kinect","II) camera"],"metric":["1) clustered hand\/wrist movement","2) object manipulation"],"metric_larger_category":["1) Body","2) Body"],"metric_smaller_category":["1) Hand Motion","2) Task-related"],"data_per_metric":["1) IV","2) II"],"outcome":["A) Learning gains"],"outcome_instrument":["A) pre-post test (references to  principles or mechanisms that confer stability to three example structures) "],"outcome_smaller_category":["A) learning"],"outcome_larger_category":["A) Product"],"analysis_and_results mm-oo:analysis:resultsig":["1-A:sup. machine learning: sig (ACC: 76%)"]},"75":{"paper_id_new":76,"title":"Unraveling Students\u2019 Interaction Around a Tangible Interface Using Multimodal Learning Analytics","year":2015,"authors":"Bertrand Schneider, Paulo Blikstein","data_standardized":["None"],"sensor":["IV) Kinect","V) Own application"],"metric":["1) amount of exploration","2) types of exploration","3) amount of movement","4) type of movement","5) Body synchronization","6) Body distance"],"metric_larger_category":["1) Body","2) Body","3) Body"],"metric_smaller_category":["1) Task-related","2)  Task-related","3) Gross Body Motion"],"data_per_metric":["1) V","2) V","3) IV","4) IV","5) IV","6) IV"],"outcome":["A) learning gains","B) learning gain group (high or low)","C) leadership (driver\/passenger)"],"outcome_instrument":["A) pre-post test","B) pre-post test","C) coding"],"outcome_smaller_category":["A) Learning","B) Learning","C) Interpersonal relationship"],"outcome_larger_category":["A) Product","B) Product","C) Process"],"analysis_and_results mm-oo:analysis:resultsig":["1-A: correlation: nonsig","2-A: correlation: mixed","3-A: correlation: nonsig","4-A: correlation: sig","4-C: ANOVA: sig","5-A: ANOVA: nonsig","6-A: correlation: nonsig","1,2,3,4,5,6-B: sup. machine learning: sig (ACC: 100%)"]},"76":{"paper_id_new":77,"title":"Leveraging mobile eye-trackers to capture joint visual attention in co-located collaborative learning groups","year":2018,"authors":"Bertrand Schneider, Kshitij Sharma, Sebastien Cuendet, Guillaume Zufferey, Pierre Dillenbourg & Roy Pea ","data_standardized":["None"],"sensor":["I) eyetracker"],"metric":["1) Joint visual Attention","2) Cycles of collaborative \/ individual work"],"metric_larger_category":["1) Gaze","2)"],"metric_smaller_category":["1) Visual Attention","2) Task-related"],"data_per_metric":["1) I","2) I"],"outcome":["A) learning gains","B) collaboration quality","C) task performance"],"outcome_instrument":["A) pre-post test","B) Meier Spada Rummel coding scheme","C) number of mazes solved"],"outcome_smaller_category":["A) Learning","B) Coordination","C) Performance"],"outcome_larger_category":["A) Product","B) Process","C) Product"],"analysis_and_results mm-oo:analysis:resultsig":["1-B: correlation: sig ","2-B: correlation: sig ","2-A: correlation: sig ","2-C: correlation: sig "]},"77":{"paper_id_new":78,"title":"Does Seeing One Another\u2019s Gaze Affect Group Dialogue? A Computational Approach","year":2015,"authors":"Bertrand Schneider,Roy Pea","data_standardized":["None"],"sensor":["I) eye-tracker","III) microphone"],"metric":["1) Joint visual attention","2) simple linguistic features","3) convergence (of linguistic styles)","4) coherence"],"metric_larger_category":["1) Gaze","2)Verbal"],"metric_smaller_category":["1) Visual Attention","2) Speech Participation"],"data_per_metric":["1) I","2) III","3) III","4) III"],"outcome":["A) Learning ","B) Collaboration",""],"outcome_instrument":["A) pre-post test","B) Coding scheme",""],"outcome_smaller_category":["A) Learning","B) Coordination",""],"outcome_larger_category":["A) Product","B) Process",""],"analysis_and_results mm-oo:analysis:resultsig":["","4-A: correlation: sig","4-C: ANOVA: sig","2-A: sup. machine learning: 75%"]},"78":{"paper_id_new":79,"title":"Real-time mutual gaze perception enhances collaborative 4 learning and collaboration quality","year":2013,"authors":"Bertrand Schneider,Roy Pea","data_standardized":["None"],"sensor":["I) eye tracker","II) eye tracker"],"metric":["1) joint visual attention","2) cognitive load (from pupil size)"],"metric_larger_category":["1) Gaze","2) Physiological"],"metric_smaller_category":["1) Visual Attention","2\uff09"],"data_per_metric":["1) I","2) II"],"outcome":["A) learning gains"],"outcome_instrument":["A) pre\/post test"],"outcome_smaller_category":["A) Learning"],"outcome_larger_category":["A) product"],"analysis_and_results mm-oo:analysis:resultsig":["1-A :mediation : sig","2-A:mediation: nonsig"]},"79":{"paper_id_new":80,"title":"Challenging Joint Visual Attention as a Proxy for Collaborative Performance","year":2021,"authors":"Sharma, Kshitij\uff0cOlsen, Jennifer K.Verma, Himanshu\uff0c Caballero, Daniela\uff0cJermann, Patrick","data_standardized":["None"],"sensor":["I) eye-tracker","III) computer system audio","V) NS"],"metric":["1) Joint Visual Attention","2) Joint Mental Effort ","3) Dialogue",""],"metric_larger_category":["1) Gaze","2) Gaze","3) Speech"],"metric_smaller_category":["1) visual attention","2) visual attention","3) Speech Features "],"data_per_metric":["1) I","2) I","3) III","4) V"],"outcome":["A) learning performance"],"outcome_instrument":["A) task outcome"],"outcome_smaller_category":["A)product"],"outcome_larger_category":["A)product"],"analysis_and_results mm-oo:analysis:resultsig":["1-A: ANOVA: sig ","2-A: ANOVA: sig ","",""]},"80":{"paper_id_new":81,"title":"Deep neural networks for collaborative learning analytics: Evaluating team collaborations using student gaze point prediction","year":2020,"authors":"Zhang Guo, Roghayeh Barmaki","data_standardized":["None"],"sensor":["I) video camera, ","gaze-tracker (deep neural networks)"],"metric":["1) Joint Visual Attention"],"metric_larger_category":["1) Gaze"],"metric_smaller_category":["1) Visual Attention"],"data_per_metric":["1) I"],"outcome":["A) learning gains"],"outcome_instrument":["A) post test"],"outcome_smaller_category":["A)product"],"outcome_larger_category":["A)product"],"analysis_and_results mm-oo:analysis:resultsig":["1-A: correlation: sig"]},"81":{"paper_id_new":82,"title":"Utilizing Interactive Surfaces to Enhance Learning, Collaboration and Engagement: Insights from Learners\u2019 Gaze and Speech","year":2020,"authors":"Kshitij Sharma, Ioannis Leftheriotis, Michail Giannakos\n","data_standardized":["None"],"sensor":["I) eye-tracker","III) NS","V) application"],"metric":["1) individual gaze (transition from image to text)","2) gaze similarity (collaborative gaze)","3) gaze similarity (gaze transition similarity)","4) speech"],"metric_larger_category":["1) Gaze","2) Gaze","3) Gaze","4) Verbal"],"metric_smaller_category":["1) Visual Attention","2) Visual Attention","3) Visual Attention","4) Speech Participation"],"data_per_metric":["1) I","2) I","3) I","4) III"],"outcome":["A) learning gains","(five posters, game on interactive display, gamified quiz - collaborative\/competitive)","B) game performance"],"outcome_instrument":["A1) pre test","A2) post test","A3) final post test","B) experience points"],"outcome_smaller_category":["A) Learning"],"outcome_larger_category":["A) product "],"analysis_and_results mm-oo:analysis:resultsig":["1-A1: correlation: sig ","1-A2: correlation: sig ","2-A2: correlation: sig ","2,4-A2: ANCOVA: sig "]},"82":{"paper_id_new":83,"title":"A Multimodal Exploration of Engineering Students Emotions and Electrodermal Activity in Design Activities","year":2018,"authors":"Idalis Villanueva, Brett D. Campbell, Adam C. Raikes, Suzanne H. Jones, LeAnn G. Putney","data_standardized":["None"],"sensor":["VI) wristband"],"metric":["1) mean range-corrected EDA responses"],"metric_larger_category":["1) Physiological"],"metric_smaller_category":["1) EDA"],"data_per_metric":["1) VI"],"outcome":["A) topic emotions - negative","B) topic emotions - positive"],"outcome_instrument":["A) pre post self report survey (Topic Emotions Scale)","B) pre post self report survey (Topic Emotions Scale)"],"outcome_smaller_category":["A) cognitive?"],"outcome_larger_category":["A) process?"],"analysis_and_results mm-oo:analysis:resultsig":["1-A: corrleation: sig"]},"83":{"paper_id_new":84,"title":"Multimodal, Multiparty Modeling of Collaborative Problem Solving Performance","year":2020,"authors":"Shree Krishna Subburaj, Angela E.B. Stewart, Arjun Ramesh Rao, Sidney K. D'Mello\n\n ","data_standardized":["None"],"sensor":["I) eye-tracker","II) web camera","II) Emotient","III) headset"],"metric":["1) gaze features (fixation dispersion, number of fixations and mean fixation duration, mean saccade amplitude, joint attention)","2) acoustic-prosodic information (fundamental frequency (pitch), loudness (energy), center frequency and amplitude of the first through third formants, harmonics to noise ratio, jitter, and shimmer)","3) facial features (face area, positive and negative valence, expressivity, face\/upper body motion)","4) task content feature"],"metric_larger_category":["1) Gaze","2) Verbal","3) Head","4) "],"metric_smaller_category":["1) Visual Attention","2) Speech Features","3) Facial Expressions","4) Task Related"],"data_per_metric":["1) I","2) III","3) III"],"outcome":["A) task performance"],"outcome_instrument":["A) AUROC"],"outcome_smaller_category":["A) performance"],"outcome_larger_category":["A) product"],"analysis_and_results mm-oo:analysis:resultsig":["1,2,3,4-A: AUROC=0.71","1-A: AUROC=0.62","2-A: AUROC=0.54","3-A: AUROC=0.61","4-A: AUROC=0.67"]},"84":{"paper_id_new":85,"title":"Temporal analysis of multimodal data to predict collaborative learning outcomes","year":2020,"authors":"Jennifer K. Olsen, Kshitij Sharma, Nikol Rummel, Vincent Aleven\n","data_standardized":["None"],"sensor":["I) eye-tracker ","III) microphone","V) NS"],"metric":["1) Gaze","2) Entropy","3) Similarity","4) Cognitive load","5) Auto-correlation coefficient","6) Energy","7) Shape of envelope","8) Linear predictive coding","9) Dialog","10)  correct\/incorrect\/hint feedbacks"],"metric_larger_category":["1) Gaze","2) Gaze","3) Gaze","4) Gaze","5) Verbal","6) Verbal","7) Verbal","8) Verbal","9) Verbal","10) Log Data",""],"metric_smaller_category":["1) Visual Attention","2) Visua Attention","3) Visual Attention","4) Eye Physiology","5) Speech Features","6) Speech Features","7) Speech Features","8) Speech Features","9) Speech Content","10) Task Related"],"data_per_metric":["1) I ","2) I","3) I","4) I","5) III","6) III","7) III","8) III","9) III","10) V"],"outcome":["A) Learning gains"],"outcome_instrument":["A)  pre -post test "],"outcome_smaller_category":["A) Learning "],"outcome_larger_category":["A) product"],"analysis_and_results mm-oo:analysis:resultsig":["9 - A: correlation: sig","10 - A: correlation: sig","1- A: correlation: nonsig","2- A: correlation: nonsig","3-A: correlation: nonsig","4-A : correlation: nonsig","5-A: correlation: nonsig","6-A: correlation: nonsig","7-A: correlation: nonsig","8-A: correlation: nonsig",""]},"85":{"paper_id_new":86,"title":"Collaboration on Procedural Problems May Support Conceptual Knowledge More than You May Think","year":2014,"authors":"Jennifer Olsen, Daniel Belenky,  Vincent Aleven, Nikol Rummel","data_standardized":["None"],"sensor":["I) eye-tracker","III) computer audio chat","V) computer"],"metric":["1) hint behavior","2) joint visual attention - collaboration quality","3) moments of good collaboration "],"metric_larger_category":["1) Log Data","2) Gaze","3) Log Data"],"metric_smaller_category":["1) Task Related","2) Visual Attention","3) Task Related"],"data_per_metric":["1) V","2) I","3) III"],"outcome":["A) learning gain",""],"outcome_instrument":["A) computer-based isomorphic test forms"],"outcome_smaller_category":["A) Learning "],"outcome_larger_category":["A) product"],"analysis_and_results mm-oo:analysis:resultsig":["2-A: correlation: sig","2-A: correlation: nosig"]},"86":{"paper_id_new":87,"title":"An Application of Extreme Value Theory to Learning Analytics: Predicting Collaboration Outcome from Eye-Tracking Data","year":2016,"authors":"Kshitij Sharma, Val\u00e9rie Chavez-Demoulin, Pierre Dillenbourg","data_standardized":["None"],"sensor":["I) eye-tracker "],"metric":["1) Gaze Visual Agitation","2) Gaze Spatial Entropy","3) Return Levels - EVT "],"metric_larger_category":["1) Gaze","2) Gaze","3) Gaze"],"metric_smaller_category":["1) Visual Attention","2) Visual Attention","3) Visual Attention"],"data_per_metric":["1) I","2) I","3) I"],"outcome":["A) quality of collaboration"],"outcome_instrument":["A) scores based on comparing concept map with the map created by two experts"],"outcome_smaller_category":["A) coordination"],"outcome_larger_category":["A) process"],"analysis_and_results mm-oo:analysis:resultsig":["1-A: ANOVA: nonsig","2-A: ANOVA: nonsig","2-A: correlation: nonsig"]},"87":{"paper_id_new":88,"title":"Using Dual Eye-Tracking to Evaluate Students' Collaboration with an Intelligent Tutoring System for Elementary-Level Fractions","year":2014,"authors":"Daniel Belenky, Michael Ringenberg, Jennifer Olsen, Vincent Aleven, Nikol Rummel","data_standardized":["None"],"sensor":["I) eye-tracker"],"metric":["1) joint visual attention ","2) gaze recurrence "],"metric_larger_category":["1) Gaze","2) Gaze"],"metric_smaller_category":["1) Visual Attention","2) Visual Attention"],"data_per_metric":["1) I ","2) I "],"outcome":["A) learning gain - procedural test","B) learning gain - conceptual test"],"outcome_instrument":["A) pre post test ","B) pre post test "],"outcome_smaller_category":["A) Learning "],"outcome_larger_category":["A) product"],"analysis_and_results mm-oo:analysis:resultsig":["1-A: correlation: nonsig","1-B: correlation: sig"]},"88":{"paper_id_new":89,"title":"Understanding Collaborative Program Comprehension: Interlacing Gaze and Dialogues\n","year":2013,"authors":"Kshitij Sharma, Patrick Jermann, Marc-Antoine N\u00fcssli, Pierre Dillenbourg","data_standardized":["None"],"sensor":["I) Eye-tracker ","III) NS",""],"metric":["1) Gaze Episodes","2) Dialogue Episodes","3) Gaze Transitions"],"metric_larger_category":["1) Gaze","2) Speech","3) Gaze"],"metric_smaller_category":["1) Visual Attention","2) Speech Participation","3) Visual Attention"],"data_per_metric":["1) I","2) III","3) I"],"outcome":["A) Task performance - level of understanding "],"outcome_instrument":["A) researcher coded"],"outcome_smaller_category":["A) performance"],"outcome_larger_category":["A) product"],"analysis_and_results mm-oo:analysis:resultsig":["1- A: ANOVA:sig",""]},"89":{"paper_id_new":90,"title":"Many are the ways to learn identifying multi-modal behavioral profiles of collaborative learning in constructivist activities","year":2021,"authors":"Jauwairia Nasir, Aditi Kothiyal, Barbara Bruno, Pierre Dillenbourg","data_standardized":["None"],"sensor":["II) Camera","III) Microphone","V) Log data"],"metric":["1) Affective States","2) Speech","3) Task-context Features",""],"metric_larger_category":["1) Gaze","2) Speech","3) "],"metric_smaller_category":["1) Affective State","2) Speech","3) Text Features"],"data_per_metric":["1) II","2) II","3) III","4) V",""],"outcome":["A) Last error","B) Learning gain"],"outcome_instrument":["A) performance score","B) pre-post test"],"outcome_smaller_category":["A) performance","B) learning"],"outcome_larger_category":["A) product","B) product"],"analysis_and_results mm-oo:analysis:resultsig":["1-A: Kruskal-Wallis: nonsig","2-B: Kruskal-Wallis: sig (expressive explorers, silent wanderers)"]},"90":{"paper_id_new":91,"title":"Looking To Understand: The Coupling Between Speakers\u2019 and Listeners\u2019 Eye Movements and Its Relationship to Discourse Comprehension","year":2005,"authors":"Daniel C. Richardsona, Rick Daleb","data_standardized":["None"],"sensor":["I) Eye Tracker"],"metric":["1) Eye Movement"],"metric_larger_category":["1) Gaze"],"metric_smaller_category":["1) Eye Motion"],"data_per_metric":["1) I"],"outcome":["A) Task performance"],"outcome_instrument":["A) performance score"],"outcome_smaller_category":["A) performance"],"outcome_larger_category":["A) product",""],"analysis_and_results mm-oo:analysis:resultsig":["1-A: ANOVA: sig"]},"91":{"paper_id_new":92,"title":"On the Same Wavelength: Exploring Team Neurosynchrony in Undergraduate Dyads Solving a Cyberlearning Problem With Collaborative Script","year":2019,"authors":"Robert Davis, Jiahui Wang, Mehmet Celepkolu","data_standardized":["None"],"sensor":["VIII) EEG Sensor"],"metric":["1) Brain synchrony"],"metric_larger_category":["1) Physiological"],"metric_smaller_category":["1) Brain"],"data_per_metric":["1) VIII"],"outcome":["A) collaborative problem-solving performance","B) collaborative problem-solving efficiency",""],"outcome_instrument":["A) researcher coded","B) log data",""],"outcome_smaller_category":["A) performance","B) coordination",""],"outcome_larger_category":["A) product","B) process"],"analysis_and_results mm-oo:analysis:resultsig":["1-A:positive correlation:sig","2-B:negative correlation: sig","",""]},"92":{"paper_id_new":93,"title":"Automatically Recognizing Facial Indicators of Frustration: A Learning-centric Analysis","year":2013,"authors":"Joseph F. Grafsgaard, Joseph B. Wiggins, Kristy Elizabeth Boyer, Eric N. Wiebe, James C. Lester ","data_standardized":["None"],"sensor":["IV) Kinect"],"metric":["1) Posture Estimation","2) Hand-to-Face Gesture"],"metric_larger_category":["1) Body","2) Body"],"metric_smaller_category":["1) Gross Body Motion","2) Gross Body Motion"],"data_per_metric":["1) IV","2) IV"],"outcome":["A) learning gain",""],"outcome_instrument":["A) pre post test",""],"outcome_smaller_category":["A) cognitive load",""],"outcome_larger_category":["A) product",""],"analysis_and_results mm-oo:analysis:resultsig":["1-A: negative correlation: sig (frustration level)"]},"93":{"paper_id_new":94,"title":"The relationships between learner variables, tool-usage behaviour and performance","year":2009,"authors":"Lai Jiang, Jan Elen, Geraldine Clarebout","data_standardized":["None"],"sensor":["V) Log Data"],"metric":["1) Task-Context Features"],"metric_larger_category":["1) Log Data"],"metric_smaller_category":["1) Task Related"],"data_per_metric":["1) V"],"outcome":["A) Task performance"],"outcome_instrument":["A) researcher coded"],"outcome_smaller_category":["A) perspective","B) argument"],"outcome_larger_category":["A) product","B) product"],"analysis_and_results mm-oo:analysis:resultsig":["1-A:Wilcoxon Signed Ranks test: sig","1-B: Wilcoxon Signed Ranks test: sig"]},"94":{"paper_id_new":95,"title":"Effects of Knowledge Interdependence with the Partner on Visual and Action Transactivity in Collaborative Concept Mapping","year":2008,"authors":"Ga\u00eblle Molinari, Mirweis Sangin, Marc-Antoine N\u00fcssli, & Pierre Dillenbourg","data_standardized":["None"],"sensor":["I) Eye Tracker"],"metric":["1) Concept map fixation time ratio","2) Number of concept-map eye-gaze transitions"],"metric_larger_category":["1) Gaze"],"metric_smaller_category":["1) Visual Attention"],"data_per_metric":["1) I"],"outcome":["A) Learning Measures",""],"outcome_instrument":["A) questionnaire (individual level), research coded (dyad level)",""],"outcome_smaller_category":["A) performance"],"outcome_larger_category":["A) product"],"analysis_and_results mm-oo:analysis:resultsig":["1,2-A: negative correlation: sig (individual learning, knowledge equivalence)"]},"95":{"paper_id_new":96,"title":"On the Same Wavelength: Exploring Team Neurosynchrony in Undergraduate Dyads Solving a Cyberlearning Problem With Collaborative Script","year":2009,"authors":null,"data_standardized":["None"],"sensor":["VIII) EEG Sensor"],"metric":["1) Physiological Synchrony"],"metric_larger_category":["1\uff09 physiology"],"metric_smaller_category":["1\uff09brain"],"data_per_metric":["1) VIII"],"outcome":["A) Collaborative problem-solving performance","B) Collaborative problem-solving efficiency"],"outcome_instrument":["A) Researcher coded ","B) Log data"],"outcome_smaller_category":["A) Performance ","B) performance",""],"outcome_larger_category":["A) product        ","A) product"],"analysis_and_results mm-oo:analysis:resultsig":["1-A: Positive Correlation: sig","1-B: Negative Correlation: sig"]},"96":{"paper_id_new":97,"title":"The NISPI framework: Analysing collaborative problem-solving from students' physical interactions","year":2008,"authors":null,"data_standardized":["None"],"sensor":["II) camera"],"metric":["1)physical interactivity"],"metric_larger_category":["1\uff09Body"],"metric_smaller_category":["1\uff09Gross Body motion"],"data_per_metric":["1) II",""],"outcome":["A)CPS competencies"],"outcome_instrument":["A) researcher coded "],"outcome_smaller_category":["A) performance"],"outcome_larger_category":["A) product"],"analysis_and_results mm-oo:analysis:resultsig":["1-A: anova:sig"]},"97":{"paper_id_new":98,"title":"Many are the ways to learn identifying multi-modal behavioral profiles of collaborative learning in constructivist activities","year":2021,"authors":null,"data_standardized":["None"],"sensor":["II) camera","III) Microphone","V) Log data"],"metric":["1) Affective States","2) Speech","3) Task- context","4) Gaze"],"metric_larger_category":["1)Physiological","2)Verbal","3)Log","4) Gaze"],"metric_smaller_category":["1) Affective","2) Speech","3) Task- related ","4) Gaze"],"data_per_metric":["1) II","2) III","3) V","4) II"],"outcome":["A) Learning gains ",""],"outcome_instrument":["A) pre-post test ",""],"outcome_smaller_category":["A) Learning "],"outcome_larger_category":["A) product"],"analysis_and_results mm-oo:analysis:resultsig":["2 - A: Kruskal-Wallis: sig ","1,2,3- A: sup. machine learning(0.89)"]},"98":{"paper_id_new":99,"title":"Supervised machine learning in multimodal learning analytics\nfor estimating success in project-based learning","year":2018,"authors":null,"data_standardized":["None"],"sensor":["II)Camera","III) microphone","V)log"],"metric":["II) NS III) NS V) Arduino IDE"],"metric_larger_category":["1\uff09Total number of faces looking toward the screen (FLLS)","2\uff09 Total number of connected Arduino components (IDEVHW) ","7\uff09 Mean distance between learners (DBL)","9)    measure of complexity (IDEX) "],"metric_smaller_category":["None"],"data_per_metric":["1) II ","2) V","3) II ","4) III","5\uff09II","6\uff09II","7\uff09V","8\uff09II","9)V"],"outcome":["A\uff09task performance"],"outcome_instrument":["A) researcher coded "],"outcome_smaller_category":["A) performance"],"outcome_larger_category":["A) product"],"analysis_and_results mm-oo:analysis:resultsig":["7,2,9,10-A: regression:sig(best result)"]},"99":{"paper_id_new":100,"title":"Modelling Collaborative Problem-solving Competence with Transparent Learning Analytics: Is Video Data Enough?","year":2020,"authors":null,"data_standardized":["None"],"sensor":["II) Camera"],"metric":["1) Metrics of frequency","2) Metrics of hands distance"],"metric_larger_category":["1) Body","2) Body"],"metric_smaller_category":["1) Hand 2) Hand"],"data_per_metric":["1) II","2) II"],"outcome":["A) CPS competence"],"outcome_instrument":["A) Researcher coded "],"outcome_smaller_category":["A) Collaboration"],"outcome_larger_category":["A) Process"],"analysis_and_results mm-oo:analysis:resultsig":["1,2 - A: machine learning(0.75)"]},"100":{"paper_id_new":101,"title":"An Integrated Observing Technic for Collaborative Learning: The Multimodal Learning Analytics Based on the Video Coding and EEG Data Mining","year":2021,"authors":null,"data_standardized":["None"],"sensor":["VIII) EEG"],"metric":["1)mediation- anxiety level"],"metric_larger_category":["1 ) physioloy"],"metric_smaller_category":["1) brain"],"data_per_metric":["1) VIII"],"outcome":["A) Learning gains ",""],"outcome_instrument":["A) researcher coded "],"outcome_smaller_category":["A) Learning "],"outcome_larger_category":["A) product"],"analysis_and_results mm-oo:analysis:resultsig":["1-A: Mann-Whitney U and Kruskal-Wllis tests:sig"]},"101":{"paper_id_new":102,"title":"Inter-brain synchrony in teams predicts collective performance","year":2021,"authors":null,"data_standardized":["None"],"sensor":["VIII) EEG sensor",""],"metric":["1) Inter-brain synchrony "],"metric_larger_category":["1\uff09 physiology"],"metric_smaller_category":["1\uff09brain"],"data_per_metric":["1) VIII"],"outcome":["A) tasks performance"],"outcome_instrument":["A) mixed - researcher coded & tests"],"outcome_smaller_category":["A)Performance"],"outcome_larger_category":["A) Product"],"analysis_and_results mm-oo:analysis:resultsig":["1-A: correlation: sig"]},"102":{"paper_id_new":103,"title":"Eye gaze patterns in conversations: there is more to conversational agents than meets the eyes","year":2001,"authors":"Roel Vertegaal, Robert Slagter, Gerrit van der Veer, Anton Nijholt\n\n ","data_standardized":["None"],"sensor":["I) Eye Tracker","III) Microphone"],"metric":["1) Fixation","2) Phonemic clause"],"metric_larger_category":["1) Gaze","2) Speech"],"metric_smaller_category":["1) Visual Attention","2) Speech"],"data_per_metric":["1) I","2) III"],"outcome":["A) Conversational attention"],"outcome_instrument":["A) researcher coded"],"outcome_smaller_category":["A) attention"],"outcome_larger_category":["A) process"],"analysis_and_results mm-oo:analysis:resultsig":["1,2-A: t-test: sig"]},"103":{"paper_id_new":104,"title":"Another person's eye gaze as a cue in solving programming problems","year":2004,"authors":"Randy Stein, Susan E. Brennan\n\n ","data_standardized":["None"],"sensor":["I) Eye Tracker"],"metric":["1) Eye gaze trace"],"metric_larger_category":["1) Gaze"],"metric_smaller_category":["1) Eye Motion"],"data_per_metric":["1) I"],"outcome":["A) Performance"],"outcome_instrument":["A) log data"],"outcome_smaller_category":["A) performance"],"outcome_larger_category":["B) product"],"analysis_and_results mm-oo:analysis:resultsig":["1-A: ANOVA: sig"]},"104":{"paper_id_new":105,"title":"Coordinating cognition: The costs and benefits of shared gaze during collaborative search","year":2008,"authors":"Susan E. Brennan, Xin Chen, Christopher A. Dickinson, Mark B. Neider, Gregory J. Zelinsky","data_standardized":["None"],"sensor":["I) Eye Tracker","III) Microphone"],"metric":["1) Shared gaze","2) Shared voice"],"metric_larger_category":["1) Gaze","2) Speech"],"metric_smaller_category":["1) Visual Attention","2) Speech"],"data_per_metric":["1) I","2) III"],"outcome":["A) Task performance"],"outcome_instrument":["A) log data"],"outcome_smaller_category":["A) performance"],"outcome_larger_category":["A) product"],"analysis_and_results mm-oo:analysis:resultsig":["1,2-A: t-test: sig"]},"105":{"paper_id_new":106,"title":"Analyzing and predicting focus of attention in remote collaborative tasks","year":2005,"authors":null,"data_standardized":["None"],"sensor":["V) Log Data"],"metric":["1) task properties","2) people's actions","3) message content"],"metric_larger_category":["1) log data","2) log data","3) log data"],"metric_smaller_category":["1) task-related","2) task-related","3) task-related"],"data_per_metric":["1) V","2) V","3) V"],"outcome":["A) focus attention"],"outcome_instrument":["A) log data"],"outcome_smaller_category":["A) coordination"],"outcome_larger_category":["A) process"],"analysis_and_results mm-oo:analysis:resultsig":["1, 2, 3-A: sup machine learning: 74.25%"]},"106":{"paper_id_new":107,"title":"Gestural Communication over Video Stream: Supporting Multimodal Interaction for Remote Collaborative Physical Tasks","year":2003,"authors":null,"data_standardized":["None"],"sensor":["X) Digital pen"],"metric":["1) gesture drawing on video feed"],"metric_larger_category":["1) Body"],"metric_smaller_category":["1) Gesture drawing"],"data_per_metric":["1) X"],"outcome":["A) Task performance"],"outcome_instrument":["A) log data"],"outcome_smaller_category":["A) collaboration"],"outcome_larger_category":["A) process"],"analysis_and_results mm-oo:analysis:resultsig":["1-A: ANOVA: sig"]},"107":{"paper_id_new":108,"title":"Influencing social dynamics in meetings through a peripheral display\n","year":2007,"authors":"Janienke Sturm,  Olga Houben-van Herwijnen,  Anke Eyck, Jacques Terken\n\n","data_standardized":["None"],"sensor":["None"],"metric":["1) attention","2) speech"],"metric_larger_category":["1) Gaze","2) Speech"],"metric_smaller_category":["1) Visual Attention","2) Speaking Time"],"data_per_metric":["1) I","2) III"],"outcome":["A) quality of collaboration"],"outcome_instrument":["A) log data"],"outcome_smaller_category":["A) collaboration"],"outcome_larger_category":["A) process"],"analysis_and_results mm-oo:analysis:resultsig":["1-A: t-test: nonsig","2-A: t-test: sig"]},"108":{"paper_id_new":109,"title":"A Look Is Worth a Thousand Words: Full Gaze Awareness in Video-Mediated Conversation","year":2002,"authors":"Andrew F. Monk , Caroline Gale","data_standardized":["None"],"sensor":["II) camera"],"metric":["1)  gaze awareness "],"metric_larger_category":["1) Gaze"],"metric_smaller_category":["1) Visual Attention"],"data_per_metric":["1) I"],"outcome":["A) Task Performance ","B) collaboration quality"],"outcome_instrument":["A) log data","B) log data "],"outcome_smaller_category":["A) performance ","B) collaboration",""],"outcome_larger_category":["A) product","B) process "],"analysis_and_results mm-oo:analysis:resultsig":["1- A: ANOVA: sig ","1- B: ANOVA: sig"]},"109":{"paper_id_new":110,"title":"Recognizing communicative facial expressions for discovering interpersonal emotions in group meetings","year":2009,"authors":null,"data_standardized":["None"],"sensor":["II) camera"],"metric":["1) facial expressions"],"metric_larger_category":["1) Physiological"],"metric_smaller_category":["1) Affective"],"data_per_metric":["1) I"],"outcome":["A) Emotion network"],"outcome_instrument":["A) researcher coded "],"outcome_smaller_category":["A) collaboration"],"outcome_larger_category":["A) process"],"analysis_and_results mm-oo:analysis:resultsig":["1-A unsup. machine learnig:sig "]},"110":{"paper_id_new":112,"title":"Deixis and Gaze in Collaborative Work at a Distance (over a shared map): a\nComputational Model to Detect Misunderstandings","year":2008,"authors":null,"data_standardized":["None"],"sensor":["None"],"metric":["None"],"metric_larger_category":["None"],"metric_smaller_category":["None"],"data_per_metric":["None"],"outcome":["None"],"outcome_instrument":["None"],"outcome_smaller_category":["None"],"outcome_larger_category":["None"],"analysis_and_results mm-oo:analysis:resultsig":["None"]},"111":{"paper_id_new":113,"title":"Coordinating spatial referencing using shared gaze","year":2010,"authors":null,"data_standardized":["None"],"sensor":["I) eyetracker","III) microphone"],"metric":["1) shared gaze","2) shared speech "],"metric_larger_category":["1) Gaze","2) Speech "],"metric_smaller_category":["1) visual attention","2) Speech Features"],"data_per_metric":["1) I","2) III"],"outcome":["A) task performance "],"outcome_instrument":["A) log data "],"outcome_smaller_category":["A) performance"],"outcome_larger_category":["A) Product "],"analysis_and_results mm-oo:analysis:resultsig":["1,2- A: t-test:sig "]},"112":{"paper_id_new":114,"title":"Affective e-Learning: Using \u201cEmotional\u201d Data to Improve Learning in Pervasive Learning Environment","year":2009,"authors":null,"data_standardized":["None"],"sensor":["VIII) EEG sensor"],"metric":["1) physiology","2) physiology","3) physiology","4) physiology",""],"metric_larger_category":["1) physiology","2) physiology","3) physiology","4) physiology",""],"metric_smaller_category":["1) brain","2) brain","3\uff09brain","4\uff09brain"],"data_per_metric":["1) VIII","2) VII","3) VI","4) X"],"outcome":["A) Affective states"],"outcome_instrument":["A) self-report "],"outcome_smaller_category":["A) Affective "],"outcome_larger_category":["A) Process"],"analysis_and_results mm-oo:analysis:resultsig":["1,2,3,4 - A: machine learning: sig (86.30%)"]},"113":{"paper_id_new":118,"title":"Multimodal Analysis of Group Attitudes Towards Meeting Management","year":2018,"authors":null,"data_standardized":["None"],"sensor":["III) Microphone"],"metric":["1) Speech Features","2) Linguisitc features "],"metric_larger_category":["1) Verbal","2) Verbal"],"metric_smaller_category":["1) Speech features ","2) Speech features"],"data_per_metric":["1) III","2) III"],"outcome":["A) social impressions"],"outcome_instrument":["A) self report"],"outcome_smaller_category":["A) Collaboration"],"outcome_larger_category":["A) Process"],"analysis_and_results mm-oo:analysis:resultsig":["1,2-A: sup.machine learning: sig(93%)"]},"114":{"paper_id_new":119,"title":"Toward Open-Microphone Engagement\rfor Multiparty Interactions","year":2006,"authors":null,"data_standardized":["None"],"sensor":["III) Microphone"],"metric":["1) speech features","2) speech features","3) speech style"],"metric_larger_category":["1) Verbal","2) Verbal","3) Verbal"],"metric_smaller_category":["1) Speech instructions","2) Adjacent utterances","3) Dialogue style"],"data_per_metric":["1) III","2) III","3) III"],"outcome":["A) amplitude difference","B) command usage"],"outcome_instrument":["A) log data","B) researcher coded"],"outcome_smaller_category":["A) Collaboration","B) Coordination"],"outcome_larger_category":["A) Process","B) Product"],"analysis_and_results mm-oo:analysis:resultsig":["1,2,3-A: t-test: sig","1,2,3-B: t-test: sig",""]},"115":{"paper_id_new":120,"title":"Evaluation of user gestures in multi-touch interaction: a case study in pair-programming","year":2011,"authors":null,"data_standardized":["None"],"sensor":["II) Camera"],"metric":["1) Gesture fluency"],"metric_larger_category":["1) Body"],"metric_smaller_category":["1) Gesture fluency"],"data_per_metric":["1) II"],"outcome":["A) communicative intent "],"outcome_instrument":["A) researcher coded"],"outcome_smaller_category":["A) Collaboration"],"outcome_larger_category":["A) Process "],"analysis_and_results mm-oo:analysis:resultsig":["1-A: ANOVA: sig",""]},"116":{"paper_id_new":121,"title":"Towards Adapting Fantasy, Curiosity and Challenge in\nMultimodal Dialogue Systems for Preschoolers","year":2009,"authors":null,"data_standardized":["None"],"sensor":["III) Microphone"],"metric":["1) speech features"],"metric_larger_category":["1) Verbal"],"metric_smaller_category":["1) speech usage"],"data_per_metric":["1) III"],"outcome":["A) task completion","B) fantasy levels"],"outcome_instrument":["A) researcher coded","B) researcher coded"],"outcome_smaller_category":["A) performance","B) affective"],"outcome_larger_category":["A) Product ","B) process "],"analysis_and_results mm-oo:analysis:resultsig":["1-A: correlation: sig (r=0.406)","1-B: correlation: sig (r=0.224)"]},"117":{"paper_id_new":122,"title":"Multimodal recognition of personality traits in social interactions\nAuthors\n","year":2008,"authors":null,"data_standardized":["None"],"sensor":["II) camera","III) Microphone",""],"metric":["1) Gross body movements ","2) Speech Features "],"metric_larger_category":["1) Body","2) Verbal"],"metric_smaller_category":["1) Gross body motions","2) Speech features "],"data_per_metric":["1)II","2)III"],"outcome":["A) personality traits (five personality)"],"outcome_instrument":["A) questionnaire"],"outcome_smaller_category":["A) group composition"],"outcome_larger_category":["A) Condition"],"analysis_and_results mm-oo:analysis:resultsig":["1,2- A:sup.machine learning:sig (90%)"]},"118":{"paper_id_new":123,"title":"Modeling the Personality of Participants During Group Interactions","year":2009,"authors":null,"data_standardized":["None"],"sensor":["II) camera","III) Microphone",""],"metric":["1) Gross body movements ","2) Speech Features "],"metric_larger_category":["1) Body","2) Verbal"],"metric_smaller_category":["1) Body","2) speech features "],"data_per_metric":["1)II","2)III"],"outcome":["A) personality traits (Extraversion and Locus of Control)"],"outcome_instrument":["A) questionnaire"],"outcome_smaller_category":["A) group composition"],"outcome_larger_category":["A) Condition"],"analysis_and_results mm-oo:analysis:resultsig":["1,2- A:sup.machine learning:sig (90%)"]},"119":{"paper_id_new":124,"title":"Automatic prediction of individual performance from\" thin slices\" of social behavior","year":2009,"authors":null,"data_standardized":["None"],"sensor":["II) camera","III) Microphone",""],"metric":["1) Gross body movements"," 2) Speech Features"],"metric_larger_category":["1) Body","2) Verbal"],"metric_smaller_category":["1) Body","2) Speech features "],"data_per_metric":["1)II","2)III"],"outcome":["A) task performance"],"outcome_instrument":["A) researcher coded"],"outcome_smaller_category":["A) Product "],"outcome_larger_category":["A) Product "],"analysis_and_results mm-oo:analysis:resultsig":["1,2-A:unsup:machine learning:50%"]},"120":{"paper_id_new":125,"title":"Combining audio and video to predict helpers' focus of attention in multiparty remote collaboration on physical tasks","year":2006,"authors":null,"data_standardized":["None"],"sensor":["II) Camera","III) Microphone"],"metric":["1) gross body movements","2) dialogue"],"metric_larger_category":["1) Body","2) Verbal"],"metric_smaller_category":["1) worker's actions","2) dialogue "],"data_per_metric":["1) II","2) III"],"outcome":["A) focus of attention"],"outcome_instrument":["A) researcher coded"],"outcome_smaller_category":["A) Collaboration"],"outcome_larger_category":["A) process"],"analysis_and_results mm-oo:analysis:resultsig":["1,2-A: sup.machine learning: 81.79%"]},"121":{"paper_id_new":126,"title":"Predicting remote versus collocated group interactions using nonverbal cues","year":2009,"authors":null,"data_standardized":["None"],"sensor":["III) Microphone"],"metric":["1) Speaking Length","2) Speaking Turns","3) Successful Interruptions","4) Backchannels","5) Fraction of overlapped speech","6) Fraction of silence"],"metric_larger_category":["1) Verbal","2) Verbal","3) Verbal","4) Verbal","5) Verbal","6) Verbal","7) Verbal"],"metric_smaller_category":["1) speech features","2) speech features","3) speech features","4) speech features","5) speech features","6) speech features "],"data_per_metric":["1) III","2) III","3) III","4) III","5) III","6) III"],"outcome":["A) type of meeting (collocated or remote)","B) collocated meeting inference","C) remote member participation"],"outcome_instrument":["A) log data","B) log data","C) log data"],"outcome_smaller_category":["A) Group dynamics","B) Group dynamics","C) Group dynamics"],"outcome_larger_category":["A) Condition","B) Condition","C) Condition"],"analysis_and_results mm-oo:analysis:resultsig":["2-A: unsup.machine.learning: 70% (compared to 66% random performance)","2-B: unsup.machine.learning: 81% (compared to 33% random performance)","1-C: unsup.machine.learning: 50% (compared to 25% random performance)"]},"122":{"paper_id_new":127,"title":"Reciprocal attentive communication in remote meeting with a humanoid robot","year":2007,"authors":null,"data_standardized":["None"],"sensor":["II) camera"],"metric":["1) gaze frequency"],"metric_larger_category":["1) Gaze"],"metric_smaller_category":["1) gaze frequency"],"data_per_metric":["1) II"],"outcome":["A) conversational attention"],"outcome_instrument":["A) questionnaire"],"outcome_smaller_category":["A) attention"],"outcome_larger_category":["A) Process"],"analysis_and_results mm-oo:analysis:resultsig":["1-A: Tukey\u2019s HSD test: sig"]},"123":{"paper_id_new":128,"title":"Estimating focus of attention based on gaze and sound","year":2001,"authors":null,"data_standardized":["None"],"sensor":["II) camera","III) Microphone"],"metric":["1) gaze focus ","2) sound focus"],"metric_larger_category":["1) Gaze","2) Verbal"],"metric_smaller_category":["1) Visual Attention ","2) speech features "],"data_per_metric":["1) II","2) III"],"outcome":["A) focus of attention"],"outcome_instrument":["A) researcher coded "],"outcome_smaller_category":["A) Group dynamics"],"outcome_larger_category":["A) Process"],"analysis_and_results mm-oo:analysis:resultsig":["1,2- A: sup. machine learning: 74.8%"]},"124":{"paper_id_new":129,"title":"Gaze quality assisted automatic recognition of social contexts in collaborative Tetris","year":2010,"authors":null,"data_standardized":["None"],"sensor":["I) Eye Tracker","V) application"],"metric":["1) gaze fixations","2) gaze saccades","3) task actions"],"metric_larger_category":["1)Gaze","2)Gaze","3) Log"],"metric_smaller_category":["1) Visual Attention","2) Eye Motion","3) task-related "],"data_per_metric":["1) I ","2) I","3) V"],"outcome":["A) group contexts "],"outcome_instrument":["A) researcher coded "],"outcome_smaller_category":["A) Group dynamics"],"outcome_larger_category":["A) Process"],"analysis_and_results mm-oo:analysis:resultsig":["1,2,3 - sup. machine learning: sig: 81.43%"]},"125":{"paper_id_new":131,"title":"Putting the Pieces Together: Multimodal Analysis of Social Attention in Meetings","year":2010,"authors":null,"data_standardized":["None"],"sensor":["II) Camera","III) Microphone"],"metric":["1) eye gaze directions","2) head movements","3) speech features"],"metric_larger_category":["1) Gaze","2) Body","3) Verbal"],"metric_smaller_category":["1) visual attention","2) hand motions","3) speech features",""],"data_per_metric":["1) I","2) II","3) III"],"outcome":["A) focus of attention "],"outcome_instrument":["A) researcher coded "],"outcome_smaller_category":["A) Coordination"],"outcome_larger_category":["A) Process"],"analysis_and_results mm-oo:analysis:resultsig":["1,2,3 -A: sup.machine learning:sig: 94.6%"]},"126":{"paper_id_new":132,"title":"Modeling focus of attention for meeting indexing","year":1999,"authors":null,"data_standardized":["None"],"sensor":["II) Camera","XI) Motion detecting sensor "],"metric":["1) eye gaze directions ","2) head movements"],"metric_larger_category":["1) Gaze","2) Body"],"metric_smaller_category":["1) visual attention ","2)head motions "],"data_per_metric":["1) I ","2) IV"],"outcome":["A) focus of attention "],"outcome_instrument":["A) researcher coded",""],"outcome_smaller_category":["A) coordination"],"outcome_larger_category":["A) process"],"analysis_and_results mm-oo:analysis:resultsig":["1,2- A: sup. machine learning: 93%"]},"127":{"paper_id_new":133,"title":"Modeling focus of attention for meeting indexing based on multiple cues","year":2002,"authors":null,"data_standardized":["None"],"sensor":["II) Camera","III) Microphone","XI) Motion detecting sensor "],"metric":["1) eye gaze directions ","2) head movements","3) Speech features"],"metric_larger_category":["1) Gaze","2) Body","3) Verbal"],"metric_smaller_category":["1) visual attention ","2)head motions ",""],"data_per_metric":["1) I ","2) IV"],"outcome":["A) focus of attention "],"outcome_instrument":["A) researcher coded",""],"outcome_smaller_category":["A) coordination"],"outcome_larger_category":["A) process"],"analysis_and_results mm-oo:analysis:resultsig":["1,2,3- A: sup.machine learning: 76%"]},"128":{"paper_id_new":134,"title":"Mediated attention with multimodal augmented reality","year":2009,"authors":null,"data_standardized":["None"],"sensor":["I) Eye Tracker"],"metric":["1) reaction time"],"metric_larger_category":["1) Gaze"],"metric_smaller_category":["1) visual attention ","2) head motions ","3) speech features "],"data_per_metric":["1) I"],"outcome":["A) search time","B) error rate"],"outcome_instrument":["A) log data","B) researcher coded"],"outcome_smaller_category":["A) attention","B) performance"],"outcome_larger_category":["A) Process","B) Product"],"analysis_and_results mm-oo:analysis:resultsig":["1-A: t-test: sig","1-B: t-test: sig"]},"129":{"paper_id_new":135,"title":"Implicit user-adaptive system engagement in speech and pen interfaces","year":2008,"authors":null,"data_standardized":["None"],"sensor":["III) Microphone","V) Digital Pen"],"metric":["1) speech amplitude","2) pen pressure"],"metric_larger_category":["1) Verbal","2) Log Data"],"metric_smaller_category":["1) speech features","2) body motions"],"data_per_metric":["1) III","2) IV"],"outcome":["A) task performance"],"outcome_instrument":["A) research coded"],"outcome_smaller_category":["A) performance"],"outcome_larger_category":["A) Product"],"analysis_and_results mm-oo:analysis:resultsig":["1-A: t-test: sig","2-A: t-test: nonsig"]},"130":{"paper_id_new":136,"title":"Gaze-communicative behavior of stuffed-toy robot with joint attention and eye contact based on ambient gaze-tracking","year":2007,"authors":null,"data_standardized":["None"],"sensor":["I) Eye Tracker"],"metric":["1) reaction to eye contact","2) user-initiative joint attention"],"metric_larger_category":["1) Gaze","2) Gaze"],"metric_smaller_category":["1) visual attention","2) visual attention"],"data_per_metric":["1) I","2) I"],"outcome":["A) subconscious interest","B) favorable feeling"],"outcome_instrument":["A) self report","B) self report"],"outcome_smaller_category":["A) engagement","B) interpersonal relationship",""],"outcome_larger_category":["A) Process","B) Process"],"analysis_and_results mm-oo:analysis:resultsig":["1-A: t-test: sig","2-B: t-test: sig"]},"131":{"paper_id_new":137,"title":"Multi-party focus of attention recognition in meetings from head pose and multimodal contextual cues","year":2008,"authors":"Sileye O. Ba; Jean-Marc Odobez","data_standardized":["None"],"sensor":["III)Microphone","II) Camera","V) Log "],"metric":["1) speech features ","2) head rotations ","3) slide data "],"metric_larger_category":["1) Verbal","2) Body ","3) Log Data"],"metric_smaller_category":["1) speech features","2) head motions ","3) Task- related "],"data_per_metric":["1) III","2) IV","3) V"],"outcome":["A) focus of attention "],"outcome_instrument":["A) researcher coded",""],"outcome_smaller_category":["A) coordination"],"outcome_larger_category":["A) process"],"analysis_and_results mm-oo:analysis:resultsig":["1,2,3 - A: unsup.machine learning: 46.7%"]},"132":{"paper_id_new":138,"title":"Multimodal Real-Time Focus of Attention Estimation in SmartRooms","year":2008,"authors":"C. Canton-Ferrer, C. Segura, M. Pardas, J.R. Casas, J. Hernando","data_standardized":["None"],"sensor":["IV) Camera"],"metric":["1) head rotation"],"metric_larger_category":["1) Body"],"metric_smaller_category":["1) head motions"],"data_per_metric":["1) IV"],"outcome":["A) focus of attention "],"outcome_instrument":["A) researcher coded",""],"outcome_smaller_category":["A) coordination"],"outcome_larger_category":["A) process"],"analysis_and_results mm-oo:analysis:resultsig":["1-A: unsup.machine learning: 85%"]},"133":{"paper_id_new":139,"title":"Coordination of Communication: Effects of Shared Visual\nContext on Collaborative Work","year":2000,"authors":null,"data_standardized":["None"],"sensor":["II) Camera","III) Audio"],"metric":["1) gesture","2) communication quality"],"metric_larger_category":["1) Body","2) Verbal"],"metric_smaller_category":["1) gross body motion","2) speech participation"],"data_per_metric":["1) II","2) III"],"outcome":["A) task performance"],"outcome_instrument":["A) researcher coded"],"outcome_smaller_category":["A) performance"],"outcome_larger_category":["A) Product"],"analysis_and_results mm-oo:analysis:resultsig":["1-A: ANOVA: nonsig","2-A: ANOVA: nonsig"]},"134":{"paper_id_new":140,"title":"Effects\rof head-mounted and scene-oriented video systems on\rremote collaboration on physical tasks","year":2003,"authors":null,"data_standardized":["None"],"sensor":["I) Eye Tracker"],"metric":["1) focus of attention"],"metric_larger_category":["1) Gaze"],"metric_smaller_category":["1) visual attention"],"data_per_metric":["1) I"],"outcome":["A) task performance","B) quality of assistance","C) communication efficiency"],"outcome_instrument":["A) log data","B) questionnaire","C) "],"outcome_smaller_category":["A) performance","B) coordination","C) coordination"],"outcome_larger_category":["A) Product","B) Process","C) Process"],"analysis_and_results mm-oo:analysis:resultsig":["1-A: ANOVA: sig","1-B: ANOVA: sig","1-C: ANOVA: sig"]},"135":{"paper_id_new":141,"title":"RemoteCoDe: Robotic Embodiment for Enhancing Peripheral\nAwareness in Remote Collaboration Tasks","year":2022,"authors":null,"data_standardized":["None"],"sensor":["IV) Camera"],"metric":["1) head rotation"],"metric_larger_category":["1) Body"],"metric_smaller_category":["1) head motions"],"data_per_metric":["1) IV"],"outcome":["A) peripheral awareness",""],"outcome_instrument":["A) researcher coded",""],"outcome_smaller_category":["A) coordination",""],"outcome_larger_category":["A) Process",""],"analysis_and_results mm-oo:analysis:resultsig":["1-A: t-test: sig",""]},"136":{"paper_id_new":142,"title":"EEG in classroom: EMD features to detect situational interest of students during learning","year":2018,"authors":null,"data_standardized":["None"],"sensor":["VIII) EEG sensor",""],"metric":["1) brain waves patterns "],"metric_larger_category":["1) physiology"],"metric_smaller_category":["1\uff09brain"],"data_per_metric":["1) VIII"],"outcome":["A) Situational Interest "],"outcome_instrument":["A) questionnaire"],"outcome_smaller_category":["A) coordination"],"outcome_larger_category":["A) process"],"analysis_and_results mm-oo:analysis:resultsig":["1-A: sup.machine learning: 93.3%"]},"137":{"paper_id_new":143,"title":"Speakers\u2019 eye gaze disambiguates referring expressions early during face-to-face conversation","year":2007,"authors":null,"data_standardized":["None"],"sensor":["I) Eye Tracker"],"metric":["1) eye movements"],"metric_larger_category":["1) Gaze"],"metric_smaller_category":["1) visual attention"],"data_per_metric":["1) I"],"outcome":["A) early disambiguation","B) orienting effect"],"outcome_instrument":["A) log data","B) researcher coded"],"outcome_smaller_category":["A) communication","B) coordination "],"outcome_larger_category":["A) Process","B) Process"],"analysis_and_results mm-oo:analysis:resultsig":["1-A: ANOVA: sig","1-B: t-test: sig"]},"138":{"paper_id_new":144,"title":"Meeting mediator: enhancing group collaboration with sociometric feedback","year":2008,"authors":null,"data_standardized":["None"],"sensor":["III) Microphone"],"metric":["1) speech time"],"metric_larger_category":["1) Verbal"],"metric_smaller_category":["1) speech features"],"data_per_metric":["1) III"],"outcome":["A) speaking dynamics","B) group interactivity level","C) distraction level"],"outcome_instrument":["A) log data","B) researcher coded","C) researcher coded"],"outcome_smaller_category":["A) group dynamics","B) engagement","C) engagement"],"outcome_larger_category":["A) Process","B) Process","C) Process"],"analysis_and_results mm-oo:analysis:resultsig":["1-A: ANOVA: sig","1-B: ANOVA: sig","1-C: ANOVA: sig"]},"139":{"paper_id_new":145,"title":"Using Audio and Video Features to Classify the Most\nDominant Person in a Group Meeting","year":2007,"authors":null,"data_standardized":["None"],"sensor":["III) Microphone","IV) Camera"],"metric":["1) speaking length","2) speaking energy","3) motion activity"],"metric_larger_category":["1) Verbal","2) Verbal","3) Body"],"metric_smaller_category":["1) speech features","2) speech features","3) gross body motion"],"data_per_metric":["1) III","2) III","3) IV"],"outcome":["A) perceived dominance"],"outcome_instrument":["A) researcher coded"],"outcome_smaller_category":["A) group dynamics"],"outcome_larger_category":["A) Process"],"analysis_and_results mm-oo:analysis:resultsig":["1-A: sup. machine learning: 85%","1-B: sup. machine learning: 82%","1-C: sup. machine learning: 62%"]},"140":{"paper_id_new":146,"title":"Visual focus of attention estimation from head pose posterior probability distributions","year":2008,"authors":"Sileye O. Ba; Jean Marc Odobez","data_standardized":["None"],"sensor":["II) Camera"],"metric":["1) head pose"],"metric_larger_category":["1) Body"],"metric_smaller_category":["1) head motions"],"data_per_metric":["1) IV"],"outcome":["A) focus of attention "],"outcome_instrument":["A) researcher coded",""],"outcome_smaller_category":["A) coordination"],"outcome_larger_category":["A) process"],"analysis_and_results mm-oo:analysis:resultsig":["1-A: unsup.machine learning:53.6%"]},"141":{"paper_id_new":147,"title":"Towards High-Level Human Activity Recognition through Computer Vision and Temporal Logic","year":2010,"authors":"Joris Ijsselmuiden, Rainer Stiefelhagen","data_standardized":["None"],"sensor":["II) Camera","III) Microphone","IV) Camera"],"metric":["1) focus of attention","2) gestures","3) speech features "],"metric_larger_category":["1) Gaze","2) Body","3) Verbal"],"metric_smaller_category":["1) visual attention","2) gross body motion","3) speech features "],"data_per_metric":["1) II","2) IV","3) III"],"outcome":["A) group activity"],"outcome_instrument":["A)researcher coded"],"outcome_smaller_category":["A) group dynamics"],"outcome_larger_category":["A) process"],"analysis_and_results mm-oo:analysis:resultsig":["1,2,3- A: unsup.machine learning: 74.4%"]},"142":{"paper_id_new":148,"title":"Biosignals reflect pair-dynamics in collaborative work: EDA and ECG study of pair-programming in a classroom environmen","year":2018,"authors":null,"data_standardized":["None"],"sensor":["VI) wearable sensor","VII) wearable sensor"],"metric":["1) Social Physiological Compliance (SPC)"],"metric_larger_category":["1) Physiology",""],"metric_smaller_category":["1) combined"],"data_per_metric":["1) VI& VII"],"outcome":["A) task performance"],"outcome_instrument":["A) log data "],"outcome_smaller_category":["A) performance"],"outcome_larger_category":["A) product"],"analysis_and_results mm-oo:analysis:resultsig":["1-A:minimum-width envelope: sig",""]},"143":{"paper_id_new":149,"title":"Toward Automated Detection of Phase Changes in Team Collaboration\n","year":2022,"authors":null,"data_standardized":["None"],"sensor":["III) Microphone"],"metric":["1) speech features"],"metric_larger_category":["1) Verbal"],"metric_smaller_category":["1) speech features"],"data_per_metric":["1) III"],"outcome":["A) team coordination"],"outcome_instrument":["A) researcuer coded"],"outcome_smaller_category":["A)coordination"],"outcome_larger_category":["A) process"],"analysis_and_results mm-oo:analysis:resultsig":["1- A:sup.machine learning: 40%"]},"144":{"paper_id_new":150,"title":"Modeling people's focus of attention","year":1999,"authors":"R. Stiefelhagen; Jie Yang; A. Waibel","data_standardized":["None"],"sensor":["IV) Camera"],"metric":["1) head rotation"],"metric_larger_category":["1) Body"],"metric_smaller_category":["1) head motions"],"data_per_metric":["1) IV"],"outcome":["A) focus of attention "],"outcome_instrument":["A) researcher coded",""],"outcome_smaller_category":["A) coordination"],"outcome_larger_category":["A) process"],"analysis_and_results mm-oo:analysis:resultsig":["1-A: unsup.machine learning: 93%"]},"145":{"paper_id_new":151,"title":"Social\u0001physiological compliance as a determinant of team performance","year":2001,"authors":null,"data_standardized":["None"],"sensor":["VI) Wearable sensor, Wristband","VII) ECG","?) "],"metric":["1) Physiological Compliance "],"metric_larger_category":["1) Physiology"],"metric_smaller_category":["1) Physiological Compliance"],"data_per_metric":["1) VI, VII, ?"],"outcome":["A) task completion time","B) collision damage","C) team coordination"],"outcome_instrument":["A) log data","B) researcher coded","C) researcher coded"],"outcome_smaller_category":["A) performance","B) performance","C) coordination"],"outcome_larger_category":["A) product","B) product","C) process"],"analysis_and_results mm-oo:analysis:resultsig":["1-A: regression: sig","1-B: regression: nonsig","1-C: regression: nonsig"]},"146":{"paper_id_new":152,"title":"Modelling Symmetry of Activity as an Indicator of Collocated Group Collaboration","year":2011,"authors":null,"data_standardized":["None"],"sensor":["III) Microphone","V) Log Data"],"metric":["1) verbal participation","2) physical participation"],"metric_larger_category":["1) Verbal","2) Log"],"metric_smaller_category":["1) speech features","2) physical participation"],"data_per_metric":["1) III","2) V"],"outcome":["A) group collaboration level"],"outcome_instrument":["A) researcher coded",""],"outcome_smaller_category":["A) collaboration"],"outcome_larger_category":["A) process"],"analysis_and_results mm-oo:analysis:resultsig":["1,2-A: unsup.machine learning: 60%"]},"147":{"paper_id_new":153,"title":"Estimating conversational dominance in multiparty interaction\n","year":2012,"authors":null,"data_standardized":["None"],"sensor":["I) Eye Tracker","III) Microphone"],"metric":["1) amount of eye gaze at others","2) amount of mutual gaze","3) amount of speech","4) breaking a silence"],"metric_larger_category":["1) Gaze","2) Gaze","3) Verbal","4) Verbal"],"metric_smaller_category":["1) visual attention","2) visual attention","3) speech features","4) speech particpation"],"data_per_metric":["1) I","2) I","3) III","4) III"],"outcome":["A) Conversational dominance"],"outcome_instrument":["A) questionnaire"],"outcome_smaller_category":["A) group dynamics"],"outcome_larger_category":["A) process"],"analysis_and_results mm-oo:analysis:resultsig":["1,2,3,4: regression: sig"]}}